<!DOCTYPE html>
<html>
<head>
	<title>Auto-Encoding Variational Bayes [arXiv:1312.6114] – ご注文は機械学習ですか？</title>

	    <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="自身の勉強用メモです">
    <meta property="og:description" content="自身の勉強用メモです" />
    
    <meta name="author" content="ご注文は機械学習ですか？" />

    
    <meta property="og:title" content="Auto-Encoding Variational Bayes [arXiv:1312.6114]" />
    <meta property="twitter:title" content="Auto-Encoding Variational Bayes [arXiv:1312.6114]" />
    

	<!--[if lt IE 9]>
		<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
		<![endif]-->

		<link rel="stylesheet" type="text/css" href="/style.css" />
		<link rel="alternate" type="application/rss+xml" title="ご注文は機械学習ですか？ - 自身の勉強用メモです" href="/feed.xml" />

		
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		TeX: { 
			equationNumbers: { 
				autoNumber: "AMS" 
			},
			Macros: {
				bold: ["\\boldsymbol{#1}", 1],
				double: ["\\mathbb{#1}", 1],
				argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
				argmin: ["\\mathop{\\rm arg\\,min}\\limits"],
			}
		},
		tex2jax: {
			inlineMath: [ ['$','$'] ],
			displayMath: [ ['$$','$$'] ],
			processEscapes: true,
		}
	});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
		<!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
	</head>

	<body>
		<div class="wrapper-masthead">
			<div class="container">
				<header class="masthead clearfix">
					<a href="/" class="site-avatar"><img src="http://static.beluga.fm/profile_images/225/feKPO6j83pM0/grande.jpg" /></a>

					<div class="site-info">
						<h1 class="site-name"><a href="/">ご注文は機械学習ですか？</a></h1>
						<p class="site-description">自身の勉強用メモです</p>
					</div>

					<nav>
						<a href="/about/">概要</a>
					</nav>
				</header>
			</div>
		</div>

		<div id="main" role="main" class="container">
			<article class="post">
	<h1>Auto-Encoding Variational Bayes [arXiv:1312.6114]</h1>
	<div class="date">
		2016年04月29日
	</div>

	<div class="entry">
		<h2 id="section">概要</h2>

<ul>
  <li><a href="http://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a> を読んだ</li>
</ul>

<!--more-->

<h2 id="section-1">はじめに</h2>

<p>最近名前をよく聞くようになった変分オートエンコーダ（Variational AutoEncoder）の基礎となる<strong>確率的勾配変分ベイズ（SGVB）</strong>について自分なりにまとめます。</p>

<h2 id="section-2">参考</h2>

<ul>
  <li><a href="http://deeplearning.jp/wp-content/uploads/2014/04/20150717-suzuki.pdf">20150717-suzuki.pdf</a>
    <ul>
      <li>SGVB</li>
    </ul>
  </li>
  <li><a href="http://chasen.org/~daiti-m/paper/vb-nlp-tutorial.pdf">vb-nlp-tutorial.pdf</a>
    <ul>
      <li>変分下限</li>
    </ul>
  </li>
</ul>

<h2 id="section-3">問題設定</h2>

<p>データを$\boldsymbol x$、隠れ変数を$\boldsymbol z$、パラメータを$\boldsymbol \theta$とし、同時確率分布$p<em>{\boldsymbol \theta}(\boldsymbol x, \boldsymbol z) = p</em>{\boldsymbol \theta}(\boldsymbol x\mid\boldsymbol z)p_{\boldsymbol \theta}(\boldsymbol z)$を推定します。</p>

<p>周辺尤度$p<em>{\boldsymbol \theta}(\boldsymbol x) = \int p</em>{\boldsymbol \theta}(\boldsymbol x\mid\boldsymbol z)p<em>{\boldsymbol \theta}(\boldsymbol z)d\boldsymbol z$が計算困難な場合、$p</em>{\boldsymbol \theta}(\boldsymbol x)$は$\boldsymbol \theta$で微分できないため、直接$\boldsymbol \theta$を最適化することはできません。</p>

<p>また事後分布$p<em>{\boldsymbol \theta}(\boldsymbol z\mid\boldsymbol x) = p</em>{\boldsymbol \theta}(\boldsymbol x\mid\boldsymbol z)p<em>{\boldsymbol \theta}(\boldsymbol z)/p</em>{\boldsymbol \theta}(\boldsymbol x)$も困難であり、EMアルゴリズムを使えません。</p>

<p>さらにデータが大量にあるので、時間のかかるサンプリングベースな手法は使いたくありません。</p>

<p>本論文ではこれらの問題を解決するための手法を提案します。</p>

<h2 id="section-4">認識モデル</h2>

<p>上記の問題の解決に関して、認識モデル$q_{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x)$を導入します。</p>

<p>これは真の事後分布$p_{\boldsymbol \theta}(\boldsymbol z\mid\boldsymbol x)$の近似であり、パラメータ$\boldsymbol \phi$は$\boldsymbol \theta$と同時に学習させます。</p>

<h2 id="section-5">変分下限（変分下界）</h2>

<p>$\boldsymbol z$の対数周辺尤度は以下のように変形できます。</p>

<script type="math/tex; mode=display">% <![CDATA[

	\begin{align}
		{\rm log}p_{\boldsymbol \theta}(\boldsymbol x) &= log\int p_{\boldsymbol \theta}(\boldsymbol x\mid \boldsymbol z)p_{\boldsymbol \theta}(\boldsymbol z)d\boldsymbol z\\
		&= log\int q_{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x)\frac{p_{\boldsymbol \theta}(\boldsymbol x\mid \boldsymbol z)p_{\boldsymbol \theta}(\boldsymbol z)}{q_{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x)}d\boldsymbol z\\
		&\geq\int q_{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x){\rm log}\frac{p_{\boldsymbol \theta}(\boldsymbol x\mid \boldsymbol z)p_{\boldsymbol \theta}(\boldsymbol z)}{q_{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x)}d\boldsymbol z\\
		&=\int q_{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x)
		\biggl\{
		{\rm log}\frac{p_{\boldsymbol \theta}(\boldsymbol z)}{q_{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x)}+{\rm log}p_{\boldsymbol \theta}(\boldsymbol x\mid \boldsymbol z)
		\biggr\}d\boldsymbol z\\
		&=\int q_{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x){\rm log}p_{\boldsymbol \theta}(\boldsymbol x\mid\boldsymbol z)d\boldsymbol z-\int q_{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x){\rm log}\frac{q_{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x)}{p_{\boldsymbol \theta}(\boldsymbol z)}d\boldsymbol z\\
		&=\double E_{\boldsymbol z \sim q_{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x)}[{\rm log}p_{\boldsymbol \theta}(\boldsymbol x\mid\boldsymbol z)] - D_{KL}(q_{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x)||p_{\boldsymbol \theta}(\boldsymbol z))
	\end{align}
 %]]></script>

<p>式(2)から(3)への変形にはイェンゼンの不等式を用います。</p>

<p>$\double E[\cdot]$は期待値、$D_{KL}$はKLダイバージェンスを表します。</p>

<p>式(6)は${\rm log}p_{\boldsymbol \theta}(\boldsymbol x)$の下限値を表しているため、これを増加させる$\boldsymbol \theta$を探せばよいことになります。</p>

<h3 id="section-6">第一項について</h3>

<p>$\double E<em>{\boldsymbol z \sim q</em>{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x)}[{\rm log}p<em>{\boldsymbol \theta}(\boldsymbol x\mid\boldsymbol z)]$は、あるデータ$\boldsymbol x^{(i)}$があるときに、$q</em>{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x)$からサンプリングした$\boldsymbol z^{(i)}$を用いて、$p_{\boldsymbol \theta}(\boldsymbol x\mid\boldsymbol z)$からサンプリングして得られた$\boldsymbol x’$が、もとの$\boldsymbol x^{(i)}$である度合いを表しています。</p>

<p>つまり、$\boldsymbol x \to \boldsymbol z \to \boldsymbol x$のようなオートエンコーダとしてうまく機能する度合いを表しているため、この項は復号誤差と呼ばれたりします。</p>

<h3 id="section-7">第二項について</h3>

<p>$D<em>{KL}(q</em>{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x)||p_{\boldsymbol \theta}(\boldsymbol z))$は常に0以上の値を取ります。</p>

<p>これは$q<em>{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x)$が事前分布$p</em>{\boldsymbol \theta}(\boldsymbol z)$からどれだけ離れているかを表しています。つまり、この項が0になれば、$q<em>{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x)$は$p</em>{\boldsymbol \theta}(\boldsymbol z)$に一致します。</p>

<p>$q<em>{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x)$を、$\boldsymbol x$に無関係な$\boldsymbol z$の事前分布$p</em>{\boldsymbol \theta}(\boldsymbol z)$に近づけることで過学習を防ぐことができると考えられるので、この項は正則化項と呼ばれています。</p>

<h3 id="section-8">誤差関数</h3>

<p>式(6)は尤度なので、値が大きくなることが望ましいです。</p>

<p>そのため誤差関数は負の対数周辺尤度となります。</p>

<script type="math/tex; mode=display">
	\begin{align}
		{\cal L}(\boldsymbol \theta, \boldsymbol \phi, \boldsymbol x) = -{\rm log}p_{\boldsymbol \theta}(\boldsymbol x)
	\end{align}
</script>

<h2 id="stochastic-gradient-variational-bayessgvb">Stochastic Gradient Variational Bayes(SGVB)</h2>

<p>式(7)の誤差関数を、それぞれのパラメータ$\boldsymbol \theta$、$\boldsymbol  \phi$で微分することを考えます。</p>

<h3 id="reparameterization-trick">reparameterization trick</h3>

<p>確率変数$\tilde {\boldsymbol z} \sim q<em>{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x)$を、微分可能な関数$q</em>{\boldsymbol \phi}(\boldsymbol \epsilon, \boldsymbol x)$を用いて以下のように表します。</p>

<script type="math/tex; mode=display">% <![CDATA[

	\begin{align}
		\boldsymbol \epsilon &\sim p(\boldsymbol \epsilon)\nonumber\\
		\tilde {\boldsymbol z} &= q_{\boldsymbol \phi}(\boldsymbol \epsilon, \boldsymbol x)\\
	\end{align}
 %]]></script>

<p>つまり、$\boldsymbol x$を入力として取る関数に確率的なノイズ$\boldsymbol \epsilon$を乗せることで、サンプリングを決定論的に求めることができます。</p>

<p>たとえばある隠れ変数$\tilde z$が、データ$x$によって決まる平均$\mu(x)$、分散$\sigma(x)^2$の正規分布に従っているとします。</p>

<p>この時、$\tilde z$のサンプリングは以下のように行います。</p>

<script type="math/tex; mode=display">% <![CDATA[

	\begin{align}
		\epsilon &\sim {\cal N}(0, 1)\nonumber \\
		\tilde z &= \mu(x) + \epsilon\cdot\sigma(x)\\
	\end{align}
 %]]></script>

<p>このようにすれば、$\mu(x)$や$\sigma(x)$のそれぞれのパラメータで$\tilde z$を微分することができます。</p>

<h3 id="monte-carlo-estimates">Monte Carlo estimates</h3>

<p>ここでは簡単のため${\rm log}p_{\boldsymbol \theta}(\boldsymbol x\mid\boldsymbol z)$を単に$f(\boldsymbol z)$と表記します。</p>

<p>式(6)の期待値部分は、データ$\boldsymbol x^{(i)}$に対して、reparameterization trickを用いて以下のように近似できます。</p>

<script type="math/tex; mode=display">% <![CDATA[

	\begin{align}
		\double E_{\boldsymbol z \sim q_{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x^{(i)})}[f(\boldsymbol z)] &= \double E_{p(\boldsymbol \epsilon)}\bigl[f\bigl(g_{\boldsymbol \phi}(\boldsymbol \epsilon, \boldsymbol x^{(i)})\bigr)\bigr]\\
		&\simeq \frac{1}{L}\sum_{l=1}^{L}f\bigl(g_{\boldsymbol \phi}(\boldsymbol \epsilon^{(l)}, \boldsymbol x^{(i)})\bigr)\\
		{\rm where}\hspace{10pt}\boldsymbol \epsilon^{(l)} &\sim p(\boldsymbol \epsilon)\nonumber
	\end{align}
 %]]></script>

<p>これは、$L$個の$\boldsymbol z^{(l)}$（$l=0,1,…,L$）を$q<em>{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x^{(i)})$からサンプリングし、それぞれの$\boldsymbol z^{(l)}$とデータ$\boldsymbol x^{(i)}$から求めた${\rm log}p</em>{\boldsymbol \theta}(\boldsymbol x^{(i)}\mid\boldsymbol z^{(l)})$の平均を期待値$\double E$の推定として用いていることを表しています。</p>

<p>そもそも期待値は本質的には平均のことですので、この推定は当然のことです。</p>

<p>サイコロで例えると、$L=1$の場合、1回だけ振って期待値を推定することに相当します。</p>

<p>6の目が出れば期待値の推定は6となります。</p>

<p>$L=10000$もあれば、出た目の平均は真の期待値$3.5$に近づくでしょう。</p>

<h3 id="sgvb">SGVB</h3>

<p>上記のモンテカルロサンプリングを用いることによって、データ$\boldsymbol x^{(i)}$に対する誤差関数は最終的に以下のようになります。</p>

<script type="math/tex; mode=display">% <![CDATA[

	\begin{align}
		\boldsymbol \epsilon^{(l)} &\sim p(\boldsymbol \epsilon)\nonumber\\
		\boldsymbol z^{(i, l)} &= g_{\boldsymbol \phi}(\boldsymbol \epsilon^{(i, l)}, \boldsymbol x^{(i)})\\
		{\cal L}(\boldsymbol \theta, \boldsymbol \phi, \boldsymbol x^{(i)}) &= -{\rm log}p_{\boldsymbol \theta}(\boldsymbol x^{(i)})\\
		&\simeq D_{KL}(q_{\boldsymbol \phi}(\boldsymbol z\mid\boldsymbol x^{(i)})||p_{\boldsymbol \theta}(\boldsymbol z)) - \frac{1}{L}\sum_{l=1}^{L}\bigl({\rm log}p_{\boldsymbol \theta}(\boldsymbol x^{(i)} \mid \boldsymbol z^{(i, l)})\bigr)\\
	\end{align}
 %]]></script>

<h2 id="section-9">応用例</h2>

<ul>
  <li><a href="/2016/07/02/semi-supervised-learning-with-deep-generative-models/">変分オートエンコーダ</a></li>
  <li>変分RNN（開発中）</li>
</ul>

	</div>

 	
<div id="disqus_thread"></div>
<script>
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//musyoku.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


</article>

		</div>

		<div class="wrapper-footer">
			<div class="container">
				<footer class="footer">
					



<a href="https://github.com/musyoku"><i class="svg-icon github"></i></a>








<a href="http://beluga.fm/stark"><i class="svg-icon beluga"></i></a>
<a href="https://qiita.com/jarvis"><i class="svg-icon qiita"></i></a>

				</footer>
			</div>
		</div>

		

	</body>
</html>
