<!DOCTYPE html>
<html>
<head>
	<title>Deep Reinforcement Learning with Double Q-learning [arXiv:1509.06461] – ご注文は機械学習ですか？</title>

	    <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="自身の勉強用メモです">
    <meta property="og:description" content="自身の勉強用メモです" />
    
    <meta name="author" content="ご注文は機械学習ですか？" />

    
    <meta property="og:title" content="Deep Reinforcement Learning with Double Q-learning [arXiv:1509.06461]" />
    <meta property="twitter:title" content="Deep Reinforcement Learning with Double Q-learning [arXiv:1509.06461]" />
    

	<!--[if lt IE 9]>
		<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
		<![endif]-->

		<link rel="stylesheet" type="text/css" href="/style.css" />
		<link rel="alternate" type="application/rss+xml" title="ご注文は機械学習ですか？ - 自身の勉強用メモです" href="/feed.xml" />

		
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		TeX: { 
			equationNumbers: { 
				autoNumber: "AMS" 
			},
			Macros: {
				bold: ["\\boldsymbol{#1}", 1],
				double: ["\\mathbb{#1}", 1],
				argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
				argmin: ["\\mathop{\\rm arg\\,min}\\limits"],
			}
		},
		tex2jax: {
			inlineMath: [ ['$','$'] ],
			displayMath: [ ['$$','$$'] ],
			processEscapes: true,
		}
	});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
		<!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
	</head>

	<body>
		<div class="wrapper-masthead">
			<div class="container">
				<header class="masthead clearfix">
					<a href="/" class="site-avatar"><img src="http://static.beluga.fm/profile_images/225/feKPO6j83pM0/grande.jpg" /></a>

					<div class="site-info">
						<h1 class="site-name"><a href="/">ご注文は機械学習ですか？</a></h1>
						<p class="site-description">自身の勉強用メモです</p>
					</div>

					<nav>
						<a href="/about/">概要</a>
					</nav>
				</header>
			</div>
		</div>

		<div id="main" role="main" class="container">
			<article class="post">
	<h1>Deep Reinforcement Learning with Double Q-learning [arXiv:1509.06461]</h1>
	<div class="date">
		2016年03月16日
	</div>

	<div class="entry">
		<h2 id="section">概要</h2>

<ul>
  <li><a href="http://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double Q-learning</a> を読んだ</li>
  <li>Double DQNをChainerで実装した</li>
  <li>DQNと比較した</li>
</ul>

<!--more-->

<h2 id="section-1">はじめに</h2>

<p>今回の論文は <a href="http://papers.nips.cc/paper/3964-double-q-learning.pdf">Double Q-learning</a> をもとに <a href="http://arxiv.org/abs/1312.5602">DQN</a> を改良したものになっています。</p>

<p>Double Q-learningの方は時間がなくてあまり読めていません。</p>

<h2 id="section-2">実装</h2>

<p>今回も実装にはChainerを使います。</p>

<p>実装と言ってもDQNのコードを数行書き換えるだけなので、前回実装したものをベースにしました。</p>

<p>前処理などは <a href="/2016/03/06/human-level-control-through-deep-reinforcement-learning/">前回の記事</a> と全く変わっていません。</p>

<p>具体的な変更点ですが、まずQ学習では以下の更新式により状態行動関数Qを更新します。</p>

<script type="math/tex; mode=display">
	\begin{align}
		Q_{\theta}(s,a)\gets Q_{\theta}(s,a)+\alpha(r+\gamma \max_{a'}Q_{\pi}(s',a')-Q_{\theta}(s,a))
	\end{align}
</script>

<p>ここでは状態$s$で行動$s$を取り、報酬$r$と次の状態$s’$を得たとしています。</p>

<p>$Q_{\theta}(s,a)$はパラメータ$\theta$を持つニューラルネット（DQNでは畳み込みニューラルネット＋全結合層）です。</p>

<p>$Q<em>{\pi}(s,a)$は教師信号出力用のニューラルネットで、$Q</em>{\theta}(s,a)$のコピーになっています。（詳細は<a href="/2016/03/06/human-level-control-through-deep-reinforcement-learning/">前回の記事</a>）</p>

<p>式((1))は$Q_{\theta}(s,a)$を<script type="math/tex">r+\gamma \max_{a'}Q_{\pi}(s',a')</script>に近づける働きがあるため、教師あり学習とみなすことができます。</p>

<p>そこでDQNでは教師信号$target$を以下のように定義します。</p>

<script type="math/tex; mode=display">
	\begin{align}
		target\equiv r+\gamma \max_{a'}Q_{\pi}(s',a')
	\end{align}
</script>

<p>Double DQNではこの$target$を以下のように変更します。</p>

<script type="math/tex; mode=display">
	\begin{align}
		target\equiv r+\gamma Q_{\pi}(s',\argmax_aQ_{\theta}(s',a))
	\end{align}
</script>

<p>DQN（式((2))）では、次状態$s’$のもとで取るべき最善の行動の評価値<script type="math/tex">\max_{a'}Q_{\pi}(s',a')</script>を用いて$Q_{\theta}$を更新していました。</p>

<p>つまり、次に取るべき行動の選択とその評価を同じ$Q_{\pi}$を用いて行っています。</p>

<p>一方Double DQN（式((3))）では、まず次状態$s’$で取るべき行動$a$を<script type="math/tex">argmax_aQ_{\theta}(s',a)</script>により決定し、その$a$の評価値$Q<em>{\pi}(s’,a)$を用いて$Q</em>{\theta}$を更新します。</p>

<p>こうすることで、次に取るべき行動の選択を$Q<em>{\theta}$で行い、その評価を$Q</em>{\pi}$で行うことになります。</p>

<p>この手法はDouble Q-learningの応用ですが、どうやらパラメータの違う２種類のQ関数を用いることで性能が上がるそうです。（まだ読めていないのでなんとも言えませんが）</p>

<h2 id="section-3">実際に動かしてみる</h2>

<h3 id="section-4">必要なもの</h3>

<ul>
  <li><a href="http://www.arcadelearningenvironment.org/">Arcade Learning Environment（ALE）</a>
    <ul>
      <li>エミュレータであり、ゲームを起動し後述のRL-Glueと接続してくれます。強化学習で言うところの<strong>環境</strong>と<strong>エージェント</strong>です。</li>
    </ul>
  </li>
  <li><a href="https://code.google.com/archive/p/rl-glue-ext/wikis/RLGlueCore.wiki">RL-Glue</a>
    <ul>
      <li>ALEで起動したゲームの操作をプログラムから行えるようにするものです。</li>
      <li>どうやら.debの方をインストールすると失敗するみたいなのでソース（3.04.tar.gz）を落としてコンパイルする方が良いみたいです。</li>
    </ul>
  </li>
  <li><a href="https://sites.google.com/a/rl-community.org/rl-glue/Home/Extensions/python-codec">PL-Glue Python codec</a>
    <ul>
      <li>RL-GlueをPythonで使えるようにするものです。</li>
    </ul>
  </li>
  <li><a href="http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html">Atari 2600 VCS ROM Collection</a>
    <ul>
      <li>ブロック崩しやインベーダーなどのROMです。</li>
    </ul>
  </li>
  <li><a href="https://github.com/musyoku/double-dqn">double-dqn</a>
    <ul>
      <li>今回実装したDouble DQNのコードです。</li>
    </ul>
  </li>
  <li><strong>Chainer 1.6</strong>
    <ul>
      <li>古いバージョンのChainerで動くかどうかはわかりません。</li>
    </ul>
  </li>
</ul>

<p>環境構築に関しては <a href="http://vaaaaaanquish.hatenablog.com/entry/2015/12/11/215417">DQN-chainerリポジトリを動かすだけ</a> が参考になります。</p>

<h3 id="section-5">実験</h3>

<p>今回も例によってDouble DQNにAtari Breakoutをプレイさせます。</p>

<p>ダウンロードしたROMにはBreakoutが2つ入っており、片方は画面サイズがALE非対応のため起動できなくなっています。</p>

<p>起動できる方をbreakout.binにリネームしておいてください。</p>

<p>ターミナルを4つ起動し、以下をそれぞれのターミナルで実行します。</p>

<p><code>
rl_glue
</code></p>

<p><code>
cd path_to_double-dqn
python experiment.py --csv_dir breakout/csv --plot_dir breakout/plot
</code></p>

<p><code>
cd path_to_double-dqn/breakout
python train.py
</code></p>

<p><code>
cd /home/your_name/ALE
./ale -game_controller rlglue -use_starting_actions true -random_seed time -display_screen true -frame_skip 4 -send_rgb true /path_to_rom/breakout.bin
</code></p>

<p>ALEの–send_rgb はtrueで構いません。falseにするとグレースケールのスクリーンを取得できますが、なぜかALEネイティブのグレースケール変換は不自然だったのでDouble DQN側で変換するようになっています。</p>

<p>実験に用いたコンピュータのスペックは以下の通りです。</p>

<table>
  <tbody>
    <tr>
      <td>OS</td>
      <td>Ubuntu 14.04 LTS</td>
    </tr>
    <tr>
      <td>CPU</td>
      <td>Core i7</td>
    </tr>
    <tr>
      <td>RAM</td>
      <td>16GB</td>
    </tr>
    <tr>
      <td>GPU</td>
      <td>GTX 970M 6GB</td>
    </tr>
  </tbody>
</table>

<p>残念ながらメモリが足りず論文通りのReplay Memory Sizeでは動かないので、サイズを10分の1にしました。</p>

<h3 id="atari-breakout">Atari Breakout</h3>

<p><img src="/images/post/2016-03-06/breakout_result.gif" alt="Breakout" /></p>

<p>Breakoutはブロック崩しです。</p>

<p>合計46時間の学習（7600プレイ・95世代・479万フレーム）を行い、DQNとDouble DQNでどちらの性能が優れているかを調べました。</p>

<h4 id="section-6">プレイ回数とスコアの関係:</h4>

<p><img src="/images/post/2016-03-16/breakout_episode_reward.png" alt="Breakout episode-score" /></p>

<p>途中でとんでもないスコアを叩きだしていますが、おそらく偶然に背面を通す裏技を発見した可能性が高いです。（確認できませんでした）</p>

<p><img src="/images/post/2016-03-16/breakout_episode_reward_comparison.png" alt="Breakout episode-score" /></p>

<h4 id="section-7">プレイ回数とハイスコア:</h4>

<p><img src="/images/post/2016-03-16/breakout_training_episode_highscore.png" alt="Breakout episode-highscore" /></p>

<p>また、$\epsilon–greedy$手法の$\epsilon$を$0.05$に固定して評価を行いました。</p>

<p>学習100プレイごとに評価を20プレイ行い、スコアの平均を取りました。</p>

<h4 id="section-8">平均スコア:</h4>

<p><img src="/images/post/2016-03-16/breakout_evaluation_episode_reward.png" alt="Breakout episode-average" /></p>

<p>見た感じDouble DQNのほうが性能が良さそうですが、どの実験もすべて１回しか行っていないのでなんとも言えません。</p>

<p>１回実験を行うのに40時間くらいかかるのですが、DeepMindの発表によると数時間程度でスコア100を超えるそうなので、私の実装に何か不備があるとしか思えません。</p>

<h2 id="section-9">関連</h2>

<ul>
  <li><a href="/2016/03/06/human-level-control-through-deep-reinforcement-learning/">DQN</a></li>
</ul>

	</div>

 	
<div id="disqus_thread"></div>
<script>
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//musyoku.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


</article>

		</div>

		<div class="wrapper-footer">
			<div class="container">
				<footer class="footer">
					



<a href="https://github.com/musyoku"><i class="svg-icon github"></i></a>








<a href="http://beluga.fm/stark"><i class="svg-icon beluga"></i></a>
<a href="https://qiita.com/jarvis"><i class="svg-icon qiita"></i></a>

				</footer>
			</div>
		</div>

		

	</body>
</html>
