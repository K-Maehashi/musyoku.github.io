<!DOCTYPE html>
<html>
<head>
	<title>Dueling Network Architectures for Deep Reinforcement Learning [arXiv:1511.06581] – ご注文は機械学習ですか？</title>

	    <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="自身の勉強用メモです">
    <meta property="og:description" content="自身の勉強用メモです" />
    
    <meta name="author" content="ご注文は機械学習ですか？" />

    
    <meta property="og:title" content="Dueling Network Architectures for Deep Reinforcement Learning [arXiv:1511.06581]" />
    <meta property="twitter:title" content="Dueling Network Architectures for Deep Reinforcement Learning [arXiv:1511.06581]" />
    

	<!--[if lt IE 9]>
		<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
		<![endif]-->

		<link rel="stylesheet" type="text/css" href="/style.css" />
		<link rel="alternate" type="application/rss+xml" title="ご注文は機械学習ですか？ - 自身の勉強用メモです" href="/feed.xml" />

		
<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		TeX: { 
			equationNumbers: { 
				autoNumber: "AMS" 
			},
			Macros: {
				bold: ["\\boldsymbol{#1}", 1],
				double: ["\\mathbb{#1}", 1],
				argmax: ["\\mathop{\\rm arg\\,max}\\limits"],
				argmin: ["\\mathop{\\rm arg\\,min}\\limits"],
			}
		},
		tex2jax: {
			inlineMath: [ ['$','$'] ],
			displayMath: [ ['$$','$$'] ],
			processEscapes: true,
		}
	});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
		<!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
	</head>

	<body>
		<div class="wrapper-masthead">
			<div class="container">
				<header class="masthead clearfix">
					<a href="/" class="site-avatar"><img src="http://static.beluga.fm/profile_images/225/feKPO6j83pM0/grande.jpg" /></a>

					<div class="site-info">
						<h1 class="site-name"><a href="/">ご注文は機械学習ですか？</a></h1>
						<p class="site-description">自身の勉強用メモです</p>
					</div>

					<nav>
						<a href="/about/">概要</a>
					</nav>
				</header>
			</div>
		</div>

		<div id="main" role="main" class="container">
			<article class="post">
	<h1>Dueling Network Architectures for Deep Reinforcement Learning [arXiv:1511.06581]</h1>
	<div class="date">
		2016年03月27日
	</div>

	<div class="entry">
		<h2 id="section">概要</h2>

<ul>
  <li><a href="http://arxiv.org/abs/1511.06581">Dueling Network Architectures for Deep Reinforcement Learning</a> を読んだ</li>
  <li>Double DQNにDueling Networkを組み込んだ</li>
  <li>DQN・Double DQNと比較した</li>
</ul>

<!--more-->

<h2 id="section-1">はじめに</h2>

<p>この論文は新しい強化学習のアルゴリズムを提案するのではなく、Q関数の内部構造に変更を加えたDueling Architectureを提案しています。</p>

<p>そのためQ関数を用いた強化学習全般に適用でき、導入する際のコードの変更も少なくて済みます。</p>

<h2 id="dueling-architecture">Dueling Architecture</h2>

<p>まずQ関数を以下のように分解します。</p>

<script type="math/tex; mode=display">
	\begin{align}
		Q(s,a)=\hat{V}(s)+\hat{A}(s,a)
	\end{align}
</script>

<p><script type="math/tex">\hat{V}(s)</script>は状態価値関数を表し、$\hat{A}(s,a)$は行動優位関数を表します。</p>

<p>$Q$から$\hat{V}$を引くことで、各行動$a$の相対的な重要度を求めることができます。</p>

<h2 id="dueling-network">Dueling Network</h2>

<p>式((1))をニューラルネットで実装する場合、以下のような構成になります。</p>

<p><code>
        +--&gt; A(s,a) +--+
Conv +--+              +--&gt; Q(s,a)
        +--&gt;  V(s)  +--+
</code></p>

<p>$\hat{V}$、$\hat{A}$はそれぞれパラメータの異なる個別のニューラルネット（パラメータ$\alpha,\beta$）で表しますが、画面入力を受け持つ畳み込みニューラルネット（パラメータ$\theta$）は共有します。</p>

<p>Dueling Networkでは、$\hat{V}$と$\hat{A}$の出力ベクトルを合成し$Q$を出力します。この合成を行う部分を$aggregator$と著者らは呼んでいます。</p>

<p>しかし、$aggregator$を素直に実装すると問題が生じます。</p>

<p>まず以下の式を見てください。</p>

<script type="math/tex; mode=display">% <![CDATA[

	\begin{align}
		Q^*_{\theta}(s,a)&=\hat{V}^*_{\theta,\alpha}(s)+\hat{A}^*_{\theta,\beta}(s,a)\\
		&=\left\{\hat{V}^*_{\theta,\alpha}(s)+\epsilon\right\}+\left\{\hat{A}^*_{\theta,\beta}(s,a)-\epsilon\right\}\\
		&=\hat{V}_{\theta,\alpha}(s)+\hat{A}_{\theta,\alpha}(s,a)\\
	\end{align}
 %]]></script>

<p><script type="math/tex">Q^*_{\theta}(s,a)</script>は真の行動価値、<script type="math/tex">\hat{V}^*_{\theta,\alpha}(s)</script>は真の状態価値、<script type="math/tex">\hat{A}^*_{\theta,\beta}(s,a)</script>は真の行動優位値を表します。</p>

<p><script type="math/tex">\hat{V}_{\theta,\alpha}(s)</script>、<script type="math/tex">\hat{A}_{\theta,\alpha}(s,a)</script>は、それぞれ真の値にノイズ$\epsilon$を加算・減算したもので、真の値から外れた誤りを含む値であることを表しています。</p>

<p>式((4))は、その誤った$\hat{V}$と$\hat{A}$から、真の行動価値$Q^*$が得られてしまうことを意味します。</p>

<p>このように$aggregator$を単純な加算処理にしてしまうと、ニューラルネットが誤った関数を学習してしまうため、以下のように変更します。</p>

<script type="math/tex; mode=display">% <![CDATA[

	\begin{align}
		Q_{\theta}(s,a)&=\hat{V}_{\theta,\alpha}(s)+\hat{A}_{\theta,\beta}(s,a)-\frac{1}{|{\cal A}|}\sum_{a'}\hat{A}_{\theta,\beta}(s,a')
	\end{align}
 %]]></script>

<p><script type="math/tex">\mid{\cal A}\mid</script>は行動の総数です。</p>

<p>平均値を引くことで統計学的には自由度が1つ下がるのですが、私はあまり詳しくないので説明は控えます。</p>

<h2 id="section-2">実装</h2>

<p>Chainerによる実装は<a href="https://github.com/musyoku/dueling-network">こちら</a>です。</p>

<p>ここでは式((5))で表される$aggregator$の実装を抜き出して載せておきます。</p>

<p>```
class Aggregator(function.Function):
	def as_mat(self, x):
		if x.ndim == 2:
			return x
		return x.reshape(len(x), -1)</p>

<pre><code>def check_type_forward(self, in_types):
	n_in = in_types.size()
	type_check.expect(n_in == 3)
	value_type, advantage_type, mean_type = in_types

	type_check.expect(
		value_type.dtype == np.float32,
		advantage_type.dtype == np.float32,
		mean_type.dtype == np.float32,
		value_type.ndim == 2,
		advantage_type.ndim == 2,
		mean_type.ndim == 1,
	)

def forward(self, inputs):
	value, advantage, mean = inputs
	mean = self.as_mat(mean)
	sub = advantage - mean
	output = value + sub
	return output,

def backward(self, inputs, grad_outputs):
	xp = cuda.get_array_module(inputs[0])
	gx1 = xp.sum(grad_outputs[0], axis=1)
	gx2 = grad_outputs[0]
	return self.as_mat(gx1), gx2, -gx1
</code></pre>

<p>def aggregate(value, advantage, mean):
	return Aggregator()(value, advantage, mean)
```</p>

<p>forward時にvalueとmeanは総行動数<script type="math/tex">\mid{\cal A}\mid</script>の分だけコピーされるので、backwardするときに勾配のsumを取ります。</p>

<p>前回作った<a href="http://localhost:4000/2016/03/16/deep-reinforcement-learning-with-double-q-learning/">Double DQN</a>のQ関数の出力を得る部分を以下のように変更しました。</p>

<p><code>
	def compute_q_variable(self, state, test=False):
		output = self.conv(state, test=test)
		value = self.fc_value(output, test=test)
		advantage = self.fc_advantage(output, test=test)
		mean = F.sum(advantage, axis=1) / float(len(config.ale_actions))
		return aggregate(value, advantage, mean)
</code></p>

<h2 id="section-3">実験</h2>

<p>ALEを使った実験はとにかく時間がかかる（数日以上）のであまりやりたくないのですが、とりあえずAtari Breakoutで4000プレイほどさせました。</p>

<p>DeepMindの論文では全てのゲームで2000万フレームを２週間かけて学習させていますが、私の環境では100万フレームの学習に２５時間くらいかかります。</p>

<h4 id="section-4">プレイ回数とスコアの関係:</h4>

<p><img src="/images/post/2016-03-27/episode_reward.png" alt="Breakout episode-score" /></p>

<p>論文にも書いてありましたが、Dueling NetworkはBreakoutが苦手みたいです。</p>

<h4 id="section-5">プレイ回数とハイスコア:</h4>

<p><img src="/images/post/2016-03-27/training_episode_highscore.png" alt="Breakout episode-highscore" /></p>

<h4 id="section-6">平均スコア:</h4>

<p><img src="/images/post/2016-03-27/evaluation_episode_reward.png" alt="Breakout episode-average" /></p>

<p>そもそも訓練回数が圧倒的に足りていないので何ともいえません。</p>

<h2 id="section-7">関連</h2>

<ul>
  <li><a href="http://localhost:4000/2016/03/06/human-level-control-through-deep-reinforcement-learning/">DQN</a></li>
  <li><a href="http://localhost:4000/2016/03/16/deep-reinforcement-learning-with-double-q-learning/">Double DQN</a></li>
</ul>

	</div>

 	
<div id="disqus_thread"></div>
<script>
/**
* RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
* LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL; // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');

s.src = '//musyoku.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>


</article>

		</div>

		<div class="wrapper-footer">
			<div class="container">
				<footer class="footer">
					



<a href="https://github.com/musyoku"><i class="svg-icon github"></i></a>








<a href="http://beluga.fm/stark"><i class="svg-icon beluga"></i></a>
<a href="https://qiita.com/jarvis"><i class="svg-icon qiita"></i></a>

				</footer>
			</div>
		</div>

		

	</body>
</html>
