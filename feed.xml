<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.1.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2016-02-27T04:41:30+09:00</updated><id>/</id><title>ご注文は機械学習ですか？</title><subtitle>自身の勉強用メモです</subtitle><entry><title>強化学習（1）- 価値反復</title><link href="/2016/02/27/reinforcement-learning-1/" rel="alternate" type="text/html" title="強化学習（1）- 価値反復" /><published>2016-02-27T00:00:00+09:00</published><updated>2016-02-27T00:00:00+09:00</updated><id>/2016/02/27/reinforcement-learning-1</id><content type="html" xml:base="/2016/02/27/reinforcement-learning-1/">&lt;p&gt;最近（というか昨日）、Sutton本と呼ばれる「&lt;a href=&quot;http://www.amazon.co.jp/dp/4627826613&quot;&gt;強化学習&lt;/a&gt;」という本を使って勉強を始めました。&lt;/p&gt;

&lt;p&gt;今回は価値反復の章（p.107）の例4.3「ギャンブラーの問題」についてプログラムを作成しました。&lt;/p&gt;

&lt;p&gt;ちなみにこの本は英語版が全ページHTML化されておりWeb上で読むことができます。&lt;/p&gt;

&lt;p&gt;今回の範囲は&lt;a href=&quot;http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node44.html&quot;&gt;こちら&lt;/a&gt;です。&lt;/p&gt;

&lt;p&gt;この例題の設定は以下のようになります。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;プレイヤーはコイントスを行う&lt;/li&gt;
  &lt;li&gt;コインの表が出れば賭け金を所持金に加える&lt;/li&gt;
  &lt;li&gt;裏が出れば賭け金が所持金から引かれる&lt;/li&gt;
  &lt;li&gt;所持金が100ドルに到達すればプレイヤーの勝利&lt;/li&gt;
  &lt;li&gt;所持金がなくなればプレイヤーの敗北&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;このような状況で、所持金を100ドルにするためには何ドル賭ければ良いのか（これを最適方策と呼びます）を強化学習によって求めます。&lt;/p&gt;

&lt;p&gt;価値反復では動的計画法により以下のように価値関数を更新します。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
		V_{k+1}(s)=\max_a\sum_{s&#39;}{\cal P}_{ss&#39;}^a[{\cal R}_{ss&#39;}^a+\gamma V_k(s&#39;)]
	\end{align}&lt;/script&gt;

&lt;p&gt;エピソード的タスクなので割引率$\gamma=1$です。&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;実験&lt;/h2&gt;

&lt;p&gt;この本によれば学習結果は以下のようになるそうです。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://webdocs.cs.ualberta.ca/~sutton/book/ebook/figtmp17.png&quot; alt=&quot;実行結果&quot; /&gt;
&lt;a href=&quot;http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node44.html&quot;&gt;http://webdocs.cs.ualberta.ca/~sutton/book/ebook/node44.html&lt;/a&gt;より引用&lt;/p&gt;

&lt;p&gt;下段のグラフは学習完了後のもので、横軸が現在の所持金を表し、縦軸はその所持金のうちいくらを賭ければ勝つ確率が一番高いかを表しています。&lt;/p&gt;

&lt;p&gt;これはコインの表が出る確率$p=0.4$とした時の結果です。&lt;/p&gt;

&lt;p&gt;注目すべき所は、所持金が50ドルの時は50ドル全部賭けてしまうのに、所持金が51ドルだとたった1ドルしか賭けないということです。&lt;/p&gt;

&lt;p&gt;練習問題4.7はこの現象の理由を考察せよとなっていますので、実装して検証しました。&lt;/p&gt;

&lt;p&gt;コードは&lt;a href=&quot;https://github.com/musyoku/reinforcement-learning/tree/master/value_iteration&quot;&gt;GitHub&lt;/a&gt;にあります。&lt;/p&gt;

&lt;p&gt;まず価値関数の変化は以下のようになりました。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/musyoku/reinforcement-learning/master/value_iteration/value_estimates.png&quot; alt=&quot;価値関数&quot; /&gt;&lt;/p&gt;

&lt;p&gt;これは本に載っている結果と一致しました。&lt;/p&gt;

&lt;p&gt;しかし本の方は収束するのにsweep 32までかかっていますが、私の実装ではsweep 11で収束しました。&lt;/p&gt;

&lt;p&gt;次に最終的に得られた最適方策です。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/musyoku/reinforcement-learning/master/value_iteration/final_policy.png&quot; alt=&quot;最適方策&quot; /&gt;&lt;/p&gt;

&lt;p&gt;これは本の結果と違うものになりました。&lt;/p&gt;

&lt;p&gt;ものすごく実装ミスをしてそうな気がします。&lt;/p&gt;

&lt;p&gt;そして練習問題4.7の答えですが、今回はちょうど100ドルに達した時のみ報酬が与えられる（つまり101ドルでは何も貰えない）という設定のせいではないかと思います。&lt;/p&gt;

&lt;p&gt;50ドル全賭けして当たれば100ドルに達しますが、51ドルの場合は全部賭けると当たれば102ドル（無報酬）、外れれば50ドル（無報酬、ただし次に当てれば100ドル到達）になりますので、51ドルの時はわざと1ドル賭けて負けて50ドルにしないと100ドルに到達しないからだと思います。&lt;/p&gt;</content><category term="強化学習" /><category term="実装" /><category term="強化学習" /><summary>最近（というか昨日）、Sutton本と呼ばれる「強化学習」という本を使って勉強を始めました。</summary></entry><entry><title>Adversarial Autoencoders [arXiv:1511.05644]</title><link href="/2016/02/22/adversarial-autoencoder/" rel="alternate" type="text/html" title="Adversarial Autoencoders [arXiv:1511.05644]" /><published>2016-02-22T00:00:00+09:00</published><updated>2016-02-22T00:00:00+09:00</updated><id>/2016/02/22/adversarial-autoencoder</id><content type="html" xml:base="/2016/02/22/adversarial-autoencoder/">&lt;h2 id=&quot;section&quot;&gt;概要&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1511.05644&quot;&gt;Adversarial Autoencoders&lt;/a&gt; を読んだ&lt;/li&gt;
  &lt;li&gt;Chainerで実装した&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;はじめに&lt;/h2&gt;

&lt;p&gt;Adversarial Autoencoderは、通常のオートエンコーダの隠れ層の出力ベクトル$\boldsymbol z$を、任意の事前分布$p(\boldsymbol z)$に押し込むことで正則化するオートエンコーダです。&lt;/p&gt;

&lt;p&gt;パラメータの学習は &lt;a href=&quot;http://arxiv.org/abs/1406.2661&quot;&gt;Generative Adversarial Networks&lt;/a&gt; の枠組みで行うので、まずGANについて簡単に説明します。&lt;/p&gt;

&lt;h2 id=&quot;adversarial-networks&quot;&gt;Adversarial Networks&lt;/h2&gt;

&lt;p&gt;いま訓練データとして$\boldsymbol x$が与えられ、その生成過程を表す確率分布$p_{data}(\boldsymbol x)$を推定することを考えます。&lt;/p&gt;

&lt;p&gt;Adversarial Networksでは、まず任意のノイズ分布$p_z(\boldsymbol z)$を考え、そこからサンプリングしたノイズ$\boldsymbol z$を偽のデータ$\boldsymbol x_{gen}$へ変換する関数$G(\boldsymbol z)$をニューラルネットで定義します。&lt;/p&gt;

&lt;p&gt;次に任意のデータ$\boldsymbol x$に対し、それが訓練データ由来の本物のデータ$\boldsymbol x_{real}$なのか、それとも$G$が生成した偽のデータ$\boldsymbol x_{gen}$なのかを識別する$D(\boldsymbol x)$を同様にニューラルネットで定義します。&lt;/p&gt;

&lt;p&gt;このような設定のもとで、著者らは$G$と$D$にゲームをさせました。&lt;/p&gt;

&lt;p&gt;まず$G$は自らが生成する偽のデータ$\boldsymbol x_{gen}$をより本物のデータに近づけることで$D$を騙そうとします。&lt;/p&gt;

&lt;p&gt;一方で$D$は僅かな違いから本物と偽物を区別できるように訓練します。&lt;/p&gt;

&lt;p&gt;このゲームを式にすると以下のような目的関数になります。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
		\min_G\max_DV(G,D) = \mathbb E_{\boldsymbol x \sim p_{data}(\boldsymbol x)} [{\rm log}D(\boldsymbol x)]+\mathbb E_{\boldsymbol z \sim p_z(\boldsymbol z)}[{\rm log}(1-D(G(\boldsymbol z)))]
	\end{align}&lt;/script&gt;

&lt;p&gt;これを繰り返していくと、$G$は真のデータ分布$p_{data}(\boldsymbol x)$に近づくことができ、結果的に本来の目的であるデータ分布の推定が達成されます。&lt;/p&gt;

&lt;h2 id=&quot;adversarial-autoencoder&quot;&gt;Adversarial Autoencoder&lt;/h2&gt;

&lt;p&gt;Adversarial Autoencoderは基本的には通常のオートエンコーダで、入力$\boldsymbol x$を隠れ変数ベクトル$\boldsymbol z$に符号化したり、逆に$\boldsymbol z$から$\boldsymbol x$に復号化するものですが、以下の点が変更されています。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\boldsymbol x$を符号化した$\boldsymbol z_{gen}$を偽のデータと考える。&lt;/li&gt;
  &lt;li&gt;任意の隠れ変数分布$p_z(\boldsymbol z)$からサンプリングした$\boldsymbol z_{real}$を本物のデータと考える。&lt;/li&gt;
  &lt;li&gt;オートエンコーダの符号化部分を$G$とみなし、$G(\boldsymbol x)$と表す。&lt;/li&gt;
  &lt;li&gt;$D$は$\boldsymbol z$を入力とする新たなニューラルネット$D(\boldsymbol z)$で定義する。&lt;/li&gt;
  &lt;li&gt;$D$は与えられた$\boldsymbol z$が$p_z(\boldsymbol z)$由来なのか、それとも$G$が生成した$\boldsymbol z_{gen}$なのかを識別する。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上のような変更により、このオートエンコーダで$\boldsymbol x$を$\boldsymbol z$に符号化すると、その$\boldsymbol z$は$p_z(\boldsymbol z)$に従うようになります。&lt;/p&gt;

&lt;p&gt;パラメータの学習では、復号化誤差を最小化する通常のオートエンコーダとしての学習と、以下の目的関数を用いた2段階の学習を行います。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
		\min_G\max_DV(G,D) = \mathbb E_{\boldsymbol z \sim p_z(\boldsymbol z)} [{\rm log}D(\boldsymbol z)]+\mathbb E_{\boldsymbol x \sim p_{data}(\boldsymbol x)}[{\rm log}(1-D(G(\boldsymbol x)))]
	\end{align}&lt;/script&gt;

&lt;p&gt;GANの時とは$\boldsymbol x$と$\boldsymbol z$が逆になっていることに注意が必要です。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;実装&lt;/h2&gt;

&lt;p&gt;$D$を実装する際、初めは出力ユニットを1つにして確率を出力するようにしていましたがうまくいきませんでした。&lt;/p&gt;

&lt;p&gt;他の方の実装例を見ていると、ユニットを2つにし片方は真のデータであることを表し、もう片方は偽のデータを表すようにしてソフトマックス層として確率を求めているようでしたので、今回はそのように実装しました。&lt;/p&gt;

&lt;p&gt;コードはGitHubにあります。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/musyoku/adversarial-autoencoder&quot;&gt;adversarial-autoencoder&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ルートにimagesディレクトリを作成しそこに訓練用データを入れてください。&lt;/p&gt;

&lt;p&gt;もしくは実行時にimage_dir引数で指定することもできます。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;実験&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1511.05644&quot;&gt;論文&lt;/a&gt;では、教師あり学習でガウス混合分布とSwiss Roll分布（と著者が呼んでいる分布）に隠れ変数をマッチさせていたので、同様の実験を行いました。&lt;/p&gt;

&lt;p&gt;隠れ変数$\boldsymbol z$は今回2次元とします。&lt;/p&gt;

&lt;p&gt;まず$p_z(\boldsymbol z)$に従わせるガウス混合分布を以下のようにします。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/musyoku/adversarial-autoencoder/blob/master/example/10_2d-gaussian_train_labeled_z.png?raw=true&quot; alt=&quot;10 2D-Gaussian&quot; /&gt;&lt;/p&gt;

&lt;p&gt;それぞれの羽に各数字の隠れ変数$\boldsymbol z$を押し込むことが目的です。&lt;/p&gt;

&lt;p&gt;各数字100枚で学習を行い、9,000枚のテストデータの隠れ変数$\boldsymbol z$を可視化したものが以下になります。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/musyoku/adversarial-autoencoder/blob/master/example/10_2d-gaussian_test_labeled_z.png?raw=true&quot; alt=&quot;10 2D-Gaussian&quot; /&gt;&lt;/p&gt;

&lt;p&gt;次にSwiss Roll分布は以下の様な分布になっています。&lt;/p&gt;

&lt;p&gt;Swiss Roll（ロールケーキ）の名前通り渦を巻いた分布で、中心に近い部分から順に数字の0, 1, 2, …を押し込みます。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/musyoku/adversarial-autoencoder/blob/master/example/swiss_roll_train_labeled_z.png?raw=true&quot; alt=&quot;Swiss Roll&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ガウス混合分布の時と同様に各数字100枚で学習を行い、9,000枚のテストデータの隠れ変数$\boldsymbol z$を可視化したものが以下になります。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/musyoku/adversarial-autoencoder/blob/master/example/swiss_roll_test_labeled_z.png?raw=true&quot; alt=&quot;Swiss Roll&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;おわりに&lt;/h2&gt;

&lt;p&gt;画像のクラス分類を行う場合、オートエンコーダが出力する隠れ変数$\boldsymbol z$をソフトマックス層に繋いで分類を行いますが、上記の実験結果のようにオートエンコーダ側でそれぞれの数字の$\boldsymbol z$を分離させておくと分類精度を上げやすいのではないかと思いました。&lt;/p&gt;

&lt;p&gt;またChainer初心者のためラベル情報の付加のやりかたがわからなかったため、隠れ変数ベクトルの次元を10拡張してone-hotなラベルで置き換えるやり方をしていますが、正しいやり方はどうなんでしょう。&lt;/p&gt;</content><category term="論文" /><category term="Deep Learning" /><category term="Python" /><category term="arXiv" /><category term="Chainer" /><category term="実装" /><category term="論文読み" /><category term="Adversarial Autoencoder" /><summary>概要</summary></entry></feed>
