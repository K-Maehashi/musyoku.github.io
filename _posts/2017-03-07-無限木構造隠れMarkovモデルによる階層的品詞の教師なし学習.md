---
layout: post
title: 無限木構造隠れMarkovモデルによる階層的品詞の教師なし学習
category: 実装
tags:
- HMM
- iTHMM
excerpt_separator: <!--more-->
---

## 概要

- [無限木構造隠れMarkovモデルによる階層的品詞の教師なし学習](http://chasen.org/~daiti-m/paper/nl226ithmm.pdf)を読んだ
- C++で実装した

## はじめに

無限木構造隠れMarkovモデル（Infinite Tree HMM、以下iTHMM）は、状態遷移確率を木構造棒折り過程により表す隠れマルコフモデルです。

以前に実装した[Infinite HMM](/2017/02/27/Infinite-Hidden-Markov-Modelによる教師なし品詞推定/)と同様、どちらも状態数がデータから決まりますが、iTHMMは階層的な状態を学習することができます。

たとえば自然言語の品詞を考えると、「名詞」という親には「名詞 - 人名」などの子がありますが、従来のIHMMではこのような階層的な状態を学習することはできません。

実装難易度は以前に実装した[NPYLM](/2016/12/14/%E3%83%98-%E3%82%A4%E3%82%B9-%E9%9A%8E%E5%B1%A4%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%86-%E3%83%AB%E3%81%AB%E3%82%88%E3%82%8B%E6%95%99%E5%B8%AB%E3%81%AA%E3%81%97%E5%BD%A2%E6%85%8B%E7%B4%A0%E8%A7%A3%E6%9E%90/)と同じくらいだと感じました。

最後まで諦めない強い精神力が必要です。

またこの論文は密度が濃いため、この記事では要点だけ解説します。

## 参考文献

実装にあたって読まなければならない論文です。

- [無限木構造隠れMarkovモデルによる階層的品詞の教師なし学習](http://chasen.org/~daiti-m/paper/nl226ithmm-slides.pdf)
	- 本論文の発表スライドです
- [Tree-Structured Stick Breaking for Hierarchical Data](https://hips.seas.harvard.edu/files/adams-tssb-nips-2010.pdf)
	- 木構造棒折り過程の論文です
- [Hierarchical Dirichlet Processes](http://people.eecs.berkeley.edu/~jordan/papers/hdp.pdf)
	- 階層ディリクレ過程の解説論文です
- [A Bayesian Interpretation of Interpolated Kneser-Ney](https://www.stats.ox.ac.uk/~teh/research/compling/hpylm.pdf)
	- 階層Pitman-Yor言語モデルの論文です

## 棒折り過程

iTHMMを実装するには棒折り過程（以下SBP）がどのような仕組みになっているかを理解する必要があります。

ここでは基底分布を$G_0$とし、$k$番目のデータ$\theta_k$が$\theta_k \sim G_0$のように生成されている場合を考えます。

まず長さ1の棒を用意します。

![image](/images/post/2017-03-07/ithmm_sbp_1.png)

この棒を、現在の長さに対する比率$\gamma_1 \sim {\rm Be}(1, \alpha)$で折り、折った棒を$\theta_1$の位置に立てます。

![image](/images/post/2017-03-07/ithmm_sbp_2.png)

比率$\gamma_k$は$k$の値によらず$0 < \gamma_k < 1$を満たします。

$\gamma_k$は棒の長さではなく比率であることをよく覚えておいてください。

次に棒を$\gamma_2 \sim {\rm Be}(1, \alpha)$の比率で折り、$\theta_2$の位置に立てます。

![image](/images/post/2017-03-07/ithmm_sbp_3.png)

同様に$\gamma_3 \sim {\rm Be}(1, \alpha)$の比率で折り、$\theta_3$の位置に立てます。

![image](/images/post/2017-03-07/ithmm_sbp_4.png)

これを無限回繰り返すと$G_0$から離散分布$G$を生成することができます。

最終的に$\theta_k$の位置に立っている棒の長さが$\theta_k$の確率になります。

また長さが1の棒から始めているため、確率の総和は必ず1になります。

このように生成される$G$は多項分布になっているため、これをHMMに用いれば、無限の状態についてそれぞれの事前確率を生成することができます。

次に、$k$回目に折った棒の長さ$\pi_k$を考えます。

これは$k-1$回までの棒折り全てで残った棒を選択し、最後に折った棒を選択することになるので、以下のように表すことができます。

$$
	\begin{align}
		\pi_k = \gamma_k \prod_{j=1}^{k-1}(1-\gamma_j)
	\end{align}\
$$

何度も言いますが$\pi_k$は棒の長さであり、$\gamma_k$は比率であることに注意しましょう。

このようにして得られる無限次元の多項分布を$\pi=\\{\pi_1, \pi_2, \ldots\\}$で表します。

この$\pi$はハイパーパラメータ$\alpha$を調整することでどのような形にもなりますが、データから$\gamma_k$や$\pi_k$の事後確率を求めることができます。

いま、$\pi$からの実現値${\cal D} = \\{x_1, x_2, \ldots\\}$が与えられたとします。

${\cal D}$のもとでの$\pi$の事後確率$p(\pi \mid \cal D)$は、SBPを構成するそれぞれの確率変数$\gamma_k$の事後確率の積で表現することができ、$\cal D$の中で値が$k$であるものの個数を$n_0(k)$、値が$k$よりも大きいものの個数を$n_1(k)$とすると、$\gamma_k$の事後確率は

$$
	\begin{align}
		\gamma_k \mid {\cal D} \sim {\rm Be}(1+n_0(k), \alpha+n_1(k))
	\end{align}\
$$

となります。

この$\gamma_k$の期待値は以下のように計算できます。

$$
	\begin{align}
		\double E[\gamma_k \mid {\cal D}] = \frac{1+n_0(k)}{1+\alpha+n_0(k)+n_1(k)}
	\end{align}\
$$

したがって、$\pi_k$の事後確率の期待値は、式(1)と式(3)から

$$
	\begin{align}
		\double E[\pi_k \mid {\cal D}] 
		&= \frac{1+n_0(k)}{1+\alpha+n_0(k)+n_1(k)}\prod_{j=1}^{k-1}\left\{ 1-\frac{1+n_0(j)}{1+\alpha+n_0(j)+n_1(j)} \right\}\\
		&= \frac{1+n_0(k)}{1+\alpha+n_0(k)+n_1(k)}\prod_{j=1}^{k-1}\frac{\alpha+n_1(j)}{1+\alpha+n_0(j)+n_1(j)}\\
	\end{align}\
$$

となります。

## 木構造棒折り過程

上の棒折り過程の例では折った棒に親子関係などは一切ありませんが、木構造棒折り過程（以下TSSB）は折った棒をノードとする木構造を作ることで親子関係を考えます。

またTSSBは階層クラスタリングのための事前分布であり、データを実際に観測する前に、データをクラスタリングするとしたらどういう分割になるか？ということをモデル化するためのものです。

ここで言うデータとは品詞列$s_1 \ldots s_{\infty}$の各品詞$s_t$のことだと考えるとわかりやすいです。

実際に品詞列を観測する前に、もし品詞列$s_1 \ldots s_{\infty}$をこれから観測するとしたら、それぞれの品詞$s_t$はどのようにクラスタリングされるのか？というのが「クラスタリングのための事前分布」です。

TSSBでは棒に対する再帰的な処理を行うことで各$s_t$が木構造のどの位置に配置されるかを決定しますが、ここではその処理の流れについて先に見ていきます。

（クラスタリングされるデータそれぞれを「客」と呼びます）

#### 1. 棒が与えられる

![image](/images/post/2017-03-07/ithmm_tssb_1.png)

#### 2. 折る

比率$\nu_1$で折ります。

![image](/images/post/2017-03-07/ithmm_tssb_2.png)

#### 3. 停止するかどうか決める

確率$\nu_1$で客は折った棒にとどまります。

そうでない場合は子の棒に降ります。

![image](/images/post/2017-03-07/ithmm_tssb_3.png)

親にとどまる場合はここで処理終了です。

#### 4. 折る

比率$\psi_1$で子の棒を折ります。

また確率$\psi_1$で折った棒に止まるかどうかを決定します。

![image](/images/post/2017-03-07/ithmm_tssb_4.png)

ここでは止まらずに次に進むことを考えます。

#### 5. 折る

比率$\psi_2$で残った子の棒を折ります。

また確率$\psi_2$で折った棒に止まるかどうかを決定します。

![image](/images/post/2017-03-07/ithmm_tssb_5.png)

今回はここで止まるとします。

#### 6. 1に戻る

子の棒に止まったら、その棒に対して処理1から順に同様の処理を再帰的に行います。

TSSBの処理はこの1~6の繰り返しですが、実際にこの後の処理がどのように行われるかを可視化すると、まず比率$\nu_2$で棒を折ります。

![image](/images/post/2017-03-07/ithmm_tssb_6.png)

確率$\nu_2$で親にとどまり、確率$1-\nu_2$で子に降ります。

![image](/images/post/2017-03-07/ithmm_tssb_7.png)

この作業を品詞列$s_1 \ldots s_{\infty}$のそれぞれの品詞について行うと、木は細分化され以下のようになります。

![image](/images/post/2017-03-07/ithmm_tssb_8.png)

木構造は以下のようになっています。

![image](/images/post/2017-03-07/ithmm_tssb_9.png)

このようにして得られるTSSBですが、上から押しつぶすと一本の棒になり、IHMMにおける品詞の事前分布と同じであることがわかります。

![image](/images/post/2017-03-07/ithmm_tssb_ihmm.png)

棒の長さが確率を表すのは同じですが、TSSBではそれに加えてそれぞれの棒の階層構造を考えることができます。

通常の棒折り過程でクラスタリングを行えば、それぞれのクラスタは整数値$k=1,2,\ldots$で表すことができますが、TSSBの各クラスタ（つまりそれぞれの棒）は可変長の整数列$\boldsymbol s$で表されます。

まずルートノードは空の配列[]になります。

ルートノードの子は左から順に[1]、[2]、[3]のように表します。

同様にノード[1]の子は[1,1]、[1,2]、[1,3]のように表します。

後で出てきますが、あるノードの左側にあるノードのインデックスは辞書順で先に来る必要があります。

たとえばノード[1,2]とノード[1,3]であれば[1,2]の方が左側に来ます。

## TSSBのCDP表現

TSSBにおいて客がノード$\boldsymbol s$に止まる確率$\pi_{\boldsymbol s}$は以下のように定義されます。

$$
	\begin{align}
		\pi_{\boldsymbol s} &= \nu_{\boldsymbol s}\phi_{\boldsymbol s}\prod_{\boldsymbol s' \prec \boldsymbol s} \phi_{\boldsymbol s}(1 - \nu_{\boldsymbol s'})\\
		&= \nu_{\boldsymbol s}\prod_{\boldsymbol s' \prec \boldsymbol s}(1 - \nu_{\boldsymbol s'})\cdot \prod_{\boldsymbol s'\preceq \boldsymbol s}\phi_{\boldsymbol s'}\\
		\nu_{\boldsymbol s} &\sim {\rm Be}(1, \alpha)\\
		\psi_{\boldsymbol sk} &\sim {\rm Be}(1, \gamma)\\
		\phi_{\boldsymbol sk} &= \psi_{\boldsymbol sk}\prod_{j=1}^{k-1}(1-\psi_{sj})
	\end{align}\
$$

$\psi_{\boldsymbol sk}$はノード$\boldsymbol s$の$k$番目の子ノードを折る時の比率です。

この式からTSSBには$\nu$で定義される縦方向のSBPと$\psi$で定義される横方向のSBPの2つが存在することがわかります。

ここでは下図のようにノード$\boldsymbol s_{[2,3,2]}$に客が止まった状態を考えます。

![image](/images/post/2017-03-07/ithmm_cdp_1.png)

まず縦方向ですが、これは深さのみに注目すると以下のようなSBPになっています。

![image](/images/post/2017-03-07/ithmm_cdp_2.png)

次に横方向ですが、これはそれぞれの深さについてSBPが存在します。

![image](/images/post/2017-03-07/ithmm_cdp_3.png)
![image](/images/post/2017-03-07/ithmm_cdp_4.png)
![image](/images/post/2017-03-07/ithmm_cdp_5.png)

客を追加したら式(3)の事後分布を更新するために$n_0(\boldsymbol s)$、$n_1(\boldsymbol s)$、$m_0(\boldsymbol s)$、$m_1(\boldsymbol s)$を更新する必要がありますが、上の図の例だと以下のようになります。

![image](/images/post/2017-03-07/ithmm_cdp_6.png)

たった1人の客を追加するだけでもこれだけのパラメータを更新する必要があります。

## 無限木構造Markovモデル

TSSBを用いれば階層化された状態とそれぞれの確率が計算でき、無限木構造を持つモデルを作れることがわかりました。

しかし、このままではTSSBは状態の分布$p(s_j)$を表しているだけであり、HMMに用いるためには状態遷移確率$p(s_j \mid s_i)$を計算する必要があります。

つまり、TSSB上の各ノードについて、同じTSSB上の他の全ノードへの遷移確率が必要になります。

いま、客をクラスタリングした結果以下のようなTSSBが得られたとします。

左の木構造は右のTSSBの構造だけを抜き出したものです。

![image](/images/post/2017-03-07/ithmm_htssb_1.png)

このとき、それぞれの状態から別の状態への遷移確率はどのように定義すればよいでしょうか？

![image](/images/post/2017-03-07/ithmm_htssb_2.png)

iTHMMでは以下のように各ノードにTSSBを持たせることで遷移確率を表します。

![image](/images/post/2017-03-07/ithmm_htssb_3.png)

これらのTSSBはもとの木構造と同じ形をしていますが、それぞれの棒の長さは違います。

これらのTSSBでは、例えば$\boldsymbol s_{[1]}$への遷移確率は以下の場所に存在します。

![image](/images/post/2017-03-07/ithmm_htssb_4.png)

元の木構造の位置に対応する棒の長さがそのまま遷移確率になります。

iTHMMでは、それぞれのノードが持つ遷移確率用のTSSBを$\boldsymbol \pi^{\boldsymbol s}$で表します。

上付きか下付きかで意味が違うので注意が必要です。

下付きの$\pi_{\boldsymbol s}$は棒の長さですが、上付きの$\boldsymbol \pi^{\boldsymbol s}$は、木構造上のノード$\boldsymbol s$が持つTSSBのことを差しています。

つまり、$$\boldsymbol \pi^{\boldsymbol s} = \{\pi^{\boldsymbol s}_{[]},\pi^{\boldsymbol s}_{[1]},\pi^{\boldsymbol s}_{[2]},\ldots\}$$です。

## 階層的TSSB

TSSBにおけるノードは木構造になっているので、各ノードからの遷移確率を表すTSSB$\boldsymbol \pi^{\boldsymbol s}$は独立ではなく、親子間の依存関係を持っていると考えるのが自然です。

例えばノード[2,3]が「名詞 - 固有名詞」を表していたとすると、[2,3]からの遷移確率$\boldsymbol \pi^{[2,3]}$は親ノードの[2]（つまり「名詞」を表す状態）の遷移確率$\boldsymbol \pi^{[2]}$の影響を受けているはずです。

![image](/images/post/2017-03-07/ithmm_htssb_5.png)

TSSBは縦と横の2つのSBPから成り立っているので、親の影響を子に与えるためには階層的なSBPを考える必要があります。

これは[参考文献](http://people.eecs.berkeley.edu/~jordan/papers/hdp.pdf)の式(20)以降に載っていますがここにも載せておきます。

まず親のSBPでは棒を折る比率$\beta'_k$をベータ分布から以下のように生成します。

$$
	\begin{align}
		\beta'_k \sim {\rm Be}(1, \gamma)
	\end{align}\
$$

これらの比率で棒を折っていった時、$k$番目に折った棒の長さ$\beta_k$は

$$
	\begin{align}
		\beta_k = \beta'_k\prod_{j=1}^{k-1}(1-\beta'_j)
	\end{align}\
$$

となります。

ここで使用した文字は参考文献に合わせてありますが、$\beta'_k$は折る比率、$\beta_k$は棒の長さであることに注意してください。

つぎに、この$$\boldsymbol \beta = \{\beta_1,\beta_2,\ldots\}$$からディリクレ過程によって$\boldsymbol \beta$の影響を受けた$$\boldsymbol \pi=\{\pi_1,\pi_2,\ldots\}$$が

$$
	\begin{align}
		\boldsymbol \pi \sim {\rm DP}(\alpha, \boldsymbol \beta)
	\end{align}\
$$

のように生成されるとします。

このとき、$\boldsymbol \pi$を構成する棒を折るそれぞれの比率$\gamma_k$は

$$
	\begin{align}
		\gamma_k \sim {\rm Be}\left(\alpha\beta_k,\alpha\left(1-\sum_{j=1}^k\beta_j\right)\right)
	\end{align}\
$$

のように生成されます。

またデータ$\cal D$が与えられたもとでの$\gamma_k$の事後分布の期待値は

$$
	\begin{align}
		\double E[\gamma_k \mid {\cal D}] = \frac
		{
			\alpha\beta_k+n_0(k)
		}{
			\alpha(1-\sum_{j=1}^{k-1}\beta_j)+n_0(k)+n_1(k)
		}
	\end{align}\
$$

となります。

あとはこの式(15)がTSSBではどのようになるかを考えれば良いのですが、ここで1つ問題があります。

iTHMM論文の式(20)は以下のようになっていますが、

$$
	\begin{align}
		\nu_{\boldsymbol s} \sim {\rm Be}\left(\alpha\nu'_{\boldsymbol s},\alpha\left(1-\sum_{\boldsymbol u \preceq \boldsymbol s}\nu'_{\boldsymbol u}\right)\right)
	\end{align}\
$$

これは縦のSBPに式(14)を当てはめたもので、$$\nu'_{\boldsymbol s}$$は親のTSSBにおける$$\nu_{\boldsymbol s}$$を表しています。

このiTHMM論文の式(20)とこの記事の式(14)は記号こそ違えど本質は同じはずですが、比較してみると以下のように食い違いがあります。

![image](/images/post/2017-03-07/ithmm_wrong.png)

これはiTHMM論文のほうが誤っていると私は考えています。

理由を説明するために、まずiTHMM論文の式(22)と式(23)を載せます。

$$
	\begin{align}
		\double E[\nu_{\boldsymbol s} \mid {\cal D}] &= \frac
		{
			\alpha\nu'_{\boldsymbol s}+n_0(\boldsymbol s)
		}{
			\alpha(1-\sum_{\boldsymbol u \prec \boldsymbol s}\nu'_{\boldsymbol u})+n_0(\boldsymbol s)+n_1(\boldsymbol s)
		}\\
		\double E[\psi_{\boldsymbol sk} \mid {\cal D}] &= \frac
		{
			\alpha\psi'_{\boldsymbol sk}+m_0(\boldsymbol sk)
		}{
			\alpha(1-\sum_{j=1}^{k-1}\psi'_{\boldsymbol sj})+m_0(\boldsymbol sk)+m_1(\boldsymbol sk)
		}\\
	\end{align}\
$$

これらの式は上の式(15)のTSSB版なのですが、分母の$$1-\sum_{\boldsymbol u \preceq \boldsymbol s}\nu'_{\boldsymbol u}$$に着目すると、$$\sum_{\boldsymbol u \preceq \boldsymbol s}\nu'_{\boldsymbol u}$$は比率の総和であるため1を超える可能性が十分にあります。

もしこれが1を超えてしまうと、分母の$$\alpha(1-\sum_{\boldsymbol u \preceq \boldsymbol s}\nu'_{\boldsymbol u})$$が負の値になり、$$\double E[\nu_{\boldsymbol s} \mid {\cal D}]$$が負の値になります。

$\nu_{\boldsymbol s}$は$0 < \nu_{\boldsymbol s} < 1$を満たす必要があるため、負の値を取ってはいけません。

しかし、$$\sum_{\boldsymbol u \preceq \boldsymbol s}\nu'_{\boldsymbol u}$$の$$\nu'_{\boldsymbol u}$$が比率ではなく棒の長さだったとしたら、棒の総和は必ず1以下になるため分母が負の値になることはありえません。

したがってこれはiTHMM論文の単純な記号の表記ミスだと考えられます。

実はiTHMM論文では縦横それぞれのSBPにおける棒の長さは横のSBPのみ上の式(10)で$$\phi_{\boldsymbol sk} = \psi_{\boldsymbol sk}\prod_{j=1}^{k-1}(1-\psi_{sj})$$と記号に割り当てられているだけであり、縦のSBPにおける棒の長さがどの記号にも割り当てられていません。

さらにiTHMM論文では$\alpha$が多重定義されており混乱を招きます。

そこでこれ以降の説明のためにも以下のように定義しておきます。

$$
	\begin{align}
		\nu_{\boldsymbol s} &\sim {\rm Be}(1, \alpha)\nonumber\\
		\psi_{\boldsymbol sk} &\sim {\rm Be}(1, \gamma)\nonumber\\
		\phi_{\boldsymbol sk} &= \psi_{\boldsymbol sk}\prod_{j=1}^{k-1}(1-\psi_{sj})\nonumber\\
		\mu_{\boldsymbol s} &= \nu_{\boldsymbol s}\prod_{\boldsymbol s' \prec \boldsymbol s}(1 - \nu_{\boldsymbol s'})\\
		\pi_{\boldsymbol s} &= \nu_{\boldsymbol s}\phi_{\boldsymbol s}\prod_{\boldsymbol s' \prec \boldsymbol s} \phi_{\boldsymbol s}(1 - \nu_{\boldsymbol s'})\nonumber\\
		&= \nu_{\boldsymbol s}\prod_{\boldsymbol s' \prec \boldsymbol s}(1 - \nu_{\boldsymbol s'})\cdot \prod_{\boldsymbol s'\preceq \boldsymbol s}\phi_{\boldsymbol s'}\nonumber\\
		&= \mu_{\boldsymbol s} \cdot \prod_{\boldsymbol s'\preceq \boldsymbol s}\phi_{\boldsymbol s'}\\
		\boldsymbol \pi^{\boldsymbol s} &\sim {\rm HTSSB}(\sigma, \boldsymbol \pi^{\boldsymbol s'})
	\end{align}\
$$

このように表記すれば、TSSBは縦のSBPと、深さの数だけ存在する横のSBPの積になっていることがよくわかると思います。

これらの記号を用いてTSSBにおけるそれぞれのSBPの棒を折る比率の事後分布を正しく書くと以下のようになります。

$$
	\begin{align}
		\double E[\nu_{\boldsymbol s} \mid {\cal D}] &= \frac
		{
			\sigma\mu'_{\boldsymbol s}+n_0(\boldsymbol s)
		}{
			\sigma(1-\sum_{\boldsymbol u \prec \boldsymbol s}\mu'_{\boldsymbol u})+n_0(\boldsymbol s)+n_1(\boldsymbol s)
		}\\
		\double E[\psi_{\boldsymbol sk} \mid {\cal D}] &= \frac
		{
			\sigma\phi'_{\boldsymbol sk}+m_0(\boldsymbol sk)
		}{
			\sigma(1-\sum_{j=1}^{k-1}\phi'_{\boldsymbol sj})+m_0(\boldsymbol sk)+m_1(\boldsymbol sk)
		}\\
	\end{align}\
$$

この式(21)と式(22)が何を意味しているかを正確に把握しなければ実装できません。

図で示すと以下のようになります。

![image](/images/post/2017-03-07/ithmm_htssb_6.png)

![image](/images/post/2017-03-07/ithmm_htssb_7.png)

さらに$$\mu'_{\boldsymbol s}$$や$$\phi'_{\boldsymbol sk}$$自体も親のTSSBから生成されているため、ルートノードの遷移確率TSSBに到達するまで再帰処理を行ない計算する必要があります。

## iTHMMとHCDP

階層ディリクレ過程といえば、その実現例としてよく使われる中華料理店過程やPitman-Yor過程があります。

たとえば基底分布$G_0$からディリクレ過程によって分布$G$が生成され、k番目の観測値$\theta_k$が$G(\theta)$から生成される場合を考えます。

$$
	\begin{align}
		G &\sim {\rm DP}(\alpha, G_0)\\
		\theta_k &\sim G(\theta)
	\end{align}\
$$

この$\theta_k$を中華料理店過程でクラスタリングしている場合に、1番目から$n-1$番目までのデータを観測しクラスタリングし終わったとします。

このとき、$n$番目の観測値$\theta_{n}$は以下の分布から生成されます。

$$
	\begin{align}
		p(\theta_n \mid \theta_1,\ldots,\theta_{n-1}) = \frac{\alpha}{\alpha + n - 1}G_0(\theta)+\frac{n-1}{\alpha+n-1}\left(\frac{1}{n-1}\sum_{k=1}^{n-1}\delta_{\theta_k}\right)
	\end{align}\
$$

$$\left(\frac{1}{n-1}\sum_{k=1}^{n-1}\delta_{\theta_k}\right)$$は今まで観測したデータからなる経験分布（この場合は多項分布）を表しています。

この式(25)が意味することは、データ$\theta_1,\ldots,\theta_{n-1}$を観測した後では、$\theta_n$は$\frac{\alpha}{\alpha + n - 1}$の比で$G_0(\theta)$から生成され、$\frac{n-1}{\alpha+n-1}$の比で経験分布から生成されます。

$\theta_n$が経験分布から生成された場合は客を経験分布に追加して分布を更新しますが、$G_0(\theta)$から生成された場合、経験分布に追加するとともに代理客を$G_0(\theta)$に追加することで$G_0(\theta)$も更新します。

このように客を用いた階層ディリクレ過程のクラスタリングでは「代理客」を適切に追加しなければなりません。

iTHMMの話に戻りますが、iTHMMの遷移確率を表すTSSBは縦と横のSBPからなりますが、それぞれのSBPが親のTSSBから影響を受けている階層ディリクレ過程になっています。

$$
	\begin{align}
		\boldsymbol \pi^{\boldsymbol s} \sim {\rm HTSSB}(\sigma, \boldsymbol \pi^{\boldsymbol s'})
	\end{align}\
$$

階層ディリクレ過程は中華料理店過程として表すこともできるため、式(25)はHTSSBでも成り立たなければなりません。

したがって、子のTSSBから新しいデータが生成された時、代理客を親のTSSBに追加する必要がありますが、これはどのようにして行えばよいでしょうか？

ここでHTSSBを中華料理店過程で表した時に、何が何から生成されているかを確認しておきましょう。

![image](/images/post/2017-03-07/ithmm_crp_1.png)

CRPで考えた場合、$\boldsymbol \pi^{\boldsymbol s}$を構成するSBPの棒を折る比率が親のTSSBから生成されていると考えます。

図では垂直方向のSBPのみ示していますが水平方向も同様です。

TSSBにおいて$\nu_{\boldsymbol s}$が意味するものは、「その位置で止まる確率」です。

したがって、$\nu_{\boldsymbol s}$から生成される観測値$\theta_k$は「通過」か「停止」のどちらかを表します。

（正確には確率$\nu_{\boldsymbol s}$で「停止」し、確率$1-\nu_{\boldsymbol s}$で「通過」します）

ここでノード$\boldsymbol s$を通過した客数を$n_1(\boldsymbol s)$、停止した客数を$n_0(\boldsymbol s)$、総和を$n(\boldsymbol s)=n_0(\boldsymbol s) + n_1(\boldsymbol s)$とすると、式(25)をHTSSBに当てはめたものは以下のようになります。

$$
	\begin{align}
		p(\theta_n=停止 \mid \theta_1,\ldots,\theta_{n-1}) &= \frac{\sigma}{\sigma + n(\boldsymbol s)}\nu'_{\boldsymbol s}+\frac{n(\boldsymbol s)}{\sigma+n(\boldsymbol s)}\left(\frac{n_0(\boldsymbol s)}{n(\boldsymbol s)}\right)\\
		p(\theta_n=通過 \mid \theta_1,\ldots,\theta_{n-1}) &= \frac{\sigma}{\sigma + n(\boldsymbol s)}(1-\nu'_{\boldsymbol s})+\frac{n(\boldsymbol s)}{\sigma+n(\boldsymbol s)}\left(\frac{n_1(\boldsymbol s)}{n(\boldsymbol s)}\right)
	\end{align}\
$$

$\theta_k$は「停止」か「通過」の2つのクラスタのどちらかに割り当てられます。

![image](/images/post/2017-03-07/ithmm_crp_2.png)

CRP的に描くと上の図のようになり、「通過」か「停止」のテーブルが生成され、客はいずれかのテーブルに着席します。

$n_1(\boldsymbol s)$はテーブル「通過」に座っている総客数、$n_0(\boldsymbol s)$はテーブル「停止」に座っている総客数を表しています。

しかし今重要なのは、「停止」や「通過」の判定の際、どのくらいの比で親の$\nu'_{\boldsymbol s}$が使われたかということです。

式(28)と式(29)より、子のTSSBで客が$\boldsymbol s$に止まる時、$\frac{\sigma}{\sigma + n(\boldsymbol s)}$の比で親TSSBの$\nu_{\boldsymbol s}$を用いて停止判定を行い、$\frac{n(\boldsymbol s)}{\sigma+n(\boldsymbol s)}$の比で自分のもつ経験分布（つまり$\nu_{\boldsymbol s}$）を用いて停止判定を行っていることがわかります。。

したがって以下のベルヌーイ試行を行ない、後者が出た場合親のTSSBのノード$\boldsymbol s$に客を追加します。

$$
	\begin{align}
		\left[\frac{n(\boldsymbol s)}{n(\boldsymbol s)+\sigma}, \frac{\sigma}{n(\boldsymbol s) + \sigma}\right]
	\end{align}\
$$

ところが、この式に対応するiTHMM論文の式(30)は以下のようになっています。

$$
	\begin{align}
		\left[\frac{n(\boldsymbol s)}{n(\boldsymbol s)+\sigma}, \frac{\sigma}{n(\boldsymbol s) + \sigma}\nu'_{\boldsymbol s}\right]
	\end{align}\
$$

一体どちらが正しいのか私は自信を持てませんが、iTHMM論文の方は$\frac{n(\boldsymbol s)}{n(\boldsymbol s)+\sigma}$が比率を表しているのに対し$\frac{\sigma}{n(\boldsymbol s) + \sigma}\nu'_{\boldsymbol s}$が確率を表しているため不自然な感じがします。

念のため私の実装では式(30)と式(31)両方に対応させています。

上のベルヌーイ試行で後者が出た場合、垂直なCDPの例だけ載せますが、以下のように客を追加します。

![image](/images/post/2017-03-07/ithmm_crp_3.png)

ただし、客を削除する際に代理客を適切に削除しなければならないため、実際はベルヌーイ試行ではなく専用のCRPを作ります。

親から生成されたと判定された場合にテーブルを作成し客をそこに追加しますが、子から生成された場合はすでに存在するテーブルの客数に比例した確率でテーブルを選択肢そこに客を追加します。

このCRPは以下のようにテーブル番号がない特殊なものになっており、これを用いるとテーブル数+1の多項分布を考えることができます。

![image](/images/post/2017-03-07/ithmm_crp_4.png)

この多項分布からサンプリングし、新しいテーブルだった場合に親TSSBに客を追加します。

逆にノード$\boldsymbol s$から客が削除された時、このCRPにおいては客数に比例した確率でテーブルを選択し、そこから客を１人削除します。

テーブルが空になったら親のTSSBのノード$\boldsymbol s$から客を削除します。

## 出力確率

HMMの出力確率$p(w_t \mid s_t)$ですが、木構造の階層を考慮し、親の品詞の出力確率が子の品詞の出力確率に影響を与えていると考えます。

![image](/images/post/2017-03-07/ithmm_implementation_2.png)

そこでPitman-Yor過程を用いて親の出力確率から生成します。

$$
	\begin{align}
		p(\cdot \mid \boldsymbol s) \sim {\rm PY}(p(\cdot \mid \boldsymbol s'), d_{\mid \boldsymbol s \mid}, \theta_{\mid \boldsymbol s \mid})
	\end{align}\
$$

ハイパーパラメータの$d_{\mid \boldsymbol s \mid}$と$\theta_{\mid \boldsymbol s \mid}$は深さごとに共通のものを使用します。

ルートノードには親がないため一様分布$H$から$p(\cdot \mid \boldsymbol s_{[]})$を生成します。

## 無限木構造について

iTHMMには「無限木構造」「TSSB」「HTSSB」「HPYLM」の4つの要素があります。

![image](/images/post/2017-03-07/ithmm_implementation_1.png)

この無限木構造ですが、これは上でTSSBを用いて作ると書きましたが、

![image](/images/post/2017-03-07/ithmm_htssb_1.png)

実際は$\boldsymbol \pi^{\boldsymbol s}$へのポインタを持っていればどのように実装しても構いません。

というのも、iTHMMではこの図にあるような$p(s_j)$を実際に用いることはなく、$p(s_j \mid s_i)$だけ考えています。

## 深さの固定

iTHMMは本来無限の深さまでノードが存在しますが、論文によると深さを固定したほうが良い結果が得られるようです。

これは$\boldsymbol s$が一定の深さ以上のときに$\nu_{\boldsymbol s}$の値を常に1にすれば実現できます。

## 実装する際の注意点

iTHMMでは**親ノード**、**親TSSB**、**親ノードが持つTSSBの自分と同じ位置のノード**を頻繁に扱います。

さらに再帰処理が非常に多いため、どのノードに対して何を計算すべきかを正しく把握しておかないと途中で実装を諦めてしまうことになります。

ここでは私が実装していて気づいた注意点やデバッグすべき項目を紹介します。

### 客

この記事でも頻繁に「客」という言葉を使いましたが、iTHMMの学習における「客」とは、品詞$\boldsymbol s_i$に続く品詞$\boldsymbol s_j$のカウントのことを指しています。

たとえば$\boldsymbol s_i=[1]$に続いて品詞$\boldsymbol s_j=[1,2]$を観測したとします。

このとき客をTSSB$\boldsymbol \pi^{[1]}$のノード$\boldsymbol s_{[1,2]}$に追加します。

![image](/images/post/2017-03-07/ithmm_implementation_3.png)

縦横それぞれのCDPに追加すると以下のようになります。

![image](/images/post/2017-03-07/ithmm_implementation_4.png)

この時、この客が親のTSSBから生成されていると判定された時は親のTSSBのノード$\boldsymbol s_{[1,2]}$に代理客を追加します。

![image](/images/post/2017-03-07/ithmm_implementation_5.png)

ただし、親から生成されたかどうかの判定は縦と横のCDPそれぞれ独立して行ないます。

縦だけ親から生成されたと判定された場合の代理客の配置は以下のようになります。

![image](/images/post/2017-03-07/ithmm_implementation_6.png)

横だけ親から生成されたと判定された場合の代理客の配置は以下のようになります。

![image](/images/post/2017-03-07/ithmm_implementation_7.png)

縦も横も親から生成されたと判定された場合の代理客の配置は以下のようになります。

![image](/images/post/2017-03-07/ithmm_implementation_8.png)

これを親TSSBが無くなるまで上に再帰的に処理を行っていきます。

つまり代理客に対しても、それがさらに親から生成されたかどうかを判定します。

### HPYLM

Pitman-Yor過程ではパラメータ$\theta_{\mid \boldsymbol s \mid}$が集中度を表しており、これを$\infty$にすると生成された分布は基底分布に一致します。

そこでわざと$\theta_{\mid \boldsymbol s \mid}$を巨大な値にし、全てのノード$\boldsymbol s$の出力分布が一様分布に近くなることを確認しましょう。

### HTSSB

式(21)よりHTSSBの集中度は$\sigma$です。

そこでわざと$\sigma$を巨大な値にし、全てのノード$\boldsymbol s$のもつ$\boldsymbol \pi^{\boldsymbol s}$が、$\boldsymbol \pi^{[]}$と同一の形（構造が同じなのは当然ですが、さらにそれぞれの棒の長さも同じ）になることを確認しましょう。

### 客の削除

一旦訓練データのすべての客を追加てからすべての客を削除したときに、代理客も含めてモデルから客が全員いなくなるかを確認しましょう。

## 実験

不思議の国のアリスの原作を用いて実験を行いました。

iTHMMの実装および実験のコードはGitHubにあります。

[https://github.com/musyoku/unsupervised-pos-tagging/tree/master/infinite-tree-hmm](https://github.com/musyoku/unsupervised-pos-tagging/tree/master/infinite-tree-hmm)

今回はTreeTaggerを用いて正確に形態素解析を行った以外には前処理をしていません。

