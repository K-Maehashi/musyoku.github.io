---
layout: post
title: 無限木構造隠れMarkovモデルによる階層的品詞の教師なし学習
category: 実装
tags:
- HMM
- iTHMM
excerpt_separator: <!--more-->
---

## 概要

- [無限木構造隠れMarkovモデルによる階層的品詞の教師なし学習](http://chasen.org/~daiti-m/paper/nl226ithmm.pdf)を読んだ
- C++で実装した

## はじめに

無限木構造隠れMarkovモデル（Infinite Tree HMM、以下iTHMM）は、状態遷移確率を木構造棒折り過程により表す隠れマルコフモデルです。

以前に実装した[Infinite HMM](/2017/02/27/Infinite-Hidden-Markov-Modelによる教師なし品詞推定/)と同様、どちらも状態数がデータから決まりますが、iTHMMは階層的な状態を学習することができます。

たとえば自然言語の品詞を考えると、「名詞」という親には「名詞 - 人名」などの子がありますが、従来のIHMMではこのような階層的な状態を学習することはできません。

実装難易度は以前に実装した[NPYLM](/2016/12/14/%E3%83%98-%E3%82%A4%E3%82%B9-%E9%9A%8E%E5%B1%A4%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%86-%E3%83%AB%E3%81%AB%E3%82%88%E3%82%8B%E6%95%99%E5%B8%AB%E3%81%AA%E3%81%97%E5%BD%A2%E6%85%8B%E7%B4%A0%E8%A7%A3%E6%9E%90/)と同じくらいです。

最後まで諦めない強い精神力が必要です。

## 参考文献

実装にあたって読まなければならない論文です。

- [無限木構造隠れMarkovモデルによる階層的品詞の教師なし学習](http://chasen.org/~daiti-m/paper/nl226ithmm-slides.pdf)
	- 本論文の発表スライドです
- [Tree-Structured Stick Breaking for Hierarchical Data](https://hips.seas.harvard.edu/files/adams-tssb-nips-2010.pdf)
	- 木構造棒折り過程の論文です
- [Hierarchical Dirichlet Processes](http://people.eecs.berkeley.edu/~jordan/papers/hdp.pdf)
	- 階層ディリクレ過程の解説論文です
- [A Bayesian Interpretation of Interpolated Kneser-Ney](https://www.stats.ox.ac.uk/~teh/research/compling/hpylm.pdf)
	- 階層Pitman-Yor言語モデルの論文です

## 棒折り過程

iTHMMを実装するには棒折り過程がどのような仕組みになっているかを理解する必要があります。

ここでは基底分布を$G_0$とし、$k$番目のデータ$\theta_k$が$\theta_k \sim G_0$のように生成されている場合を考えます。

まず長さ1の棒を用意します。

![image](/images/post/2017-03-07/ithmm_sbp_1.png)

この棒を、現在の長さに対する比率$\gamma_1$で折り、折った棒を$\theta_1$の位置に立てます。

![image](/images/post/2017-03-07/ithmm_sbp_2.png)

比率$\gamma_k$は$k$の値によらず$0 < \gamma_k < 1$を満たします。

$\gamma_k$は棒の長さではなく比率であることをよく覚えておいてください。

次に棒を$\gamma_2$の比率で折り、$\theta_2$の位置に立てます。

![image](/images/post/2017-03-07/ithmm_sbp_3.png)

同様に$\gamma_3$の比率で折り、$\theta_3$の位置に立てます。

![image](/images/post/2017-03-07/ithmm_sbp_4.png)

これを無限回繰り返すと$G_0$から離散分布$G$を生成することができます。

最終的に$\theta_k$の位置に立っている棒の長さが$\theta_k$の確率になります。

また長さが1の棒から始めているため、確率の総和は必ず1になります。

このように生成される$G$は多項分布になっているため、これをHMMに用いれば、無限の状態への遷移確率を生成することができます。

次に、$k$回目に折った棒の長さ$\pi_k$を考えます。

これは$k-1$回までの棒折り全てで残った棒を選択し、最後に折った棒を選択することになるので、以下のように表すことができます。


$$
	\begin{align}
		\pi_k = \gamma_k \prod_{j=1}^{k-1}(1-\gamma_j)
	\end{align}\
$$

何度も言いますが$\pi_k$は棒の長さであり、$\gamma_k$は比率であることに注意しましょう。

## 木構造棒折り過程

上の棒折り過程の例では折った棒に親子関係などは一切ありませんが、木構造棒折り過程（以下TSSB）は折った棒をノードとする木構造を作ることで親子関係を考えます。

またTSSBは階層クラスタリングのための事前分布なので、データを実際に観測する前に、データをクラスタリングするとしたらどういう分割になるか？ということをモデル化したものです。

ここで言うデータとは品詞列$s_1 \ldots s_{\infty}$の各品詞$s_t$のことだと考えるとわかりやすいです。

実際に品詞列を観測する前に、もし品詞列$s_1 \ldots s_{\infty}$をこれから観測するとしたら、それぞれの品詞$s_t$はどのようにクラスタリングされるのか？というのが「クラスタリングのための事前分布」です。

TSSBでは棒に対する再帰的な処理を行うことで各$s_t$が木構造のどの位置に配置されるかを決定しますが、ここではその処理の流れについて先に見ていきます。

#### 1. 棒が与えられる

![image](/images/post/2017-03-07/ithmm_tssb_1.png)

#### 2. 折る

比率$\nu_1$で折ります。

![image](/images/post/2017-03-07/ithmm_tssb_2.png)

#### 3. 停止するかどうか決める

確率$\nu_1$で$s_t$は折った棒にとどまります。

そうでない場合は子の棒に降ります。

![image](/images/post/2017-03-07/ithmm_tssb_3.png)

親にとどまる場合はここで処理終了です。

#### 4. 折る

比率$\psi_1$で子の棒を折ります。

また確率$\psi_1$で折った棒に止まるかどうかを決定します。

![image](/images/post/2017-03-07/ithmm_tssb_4.png)

ここでは止まらずに次に進むことを考えます。

#### 5. 折る

比率$\psi_2$で残った子の棒を折ります。

また確率$\psi_2$で折った棒に止まるかどうかを決定します。

![image](/images/post/2017-03-07/ithmm_tssb_5.png)

今回はここで止まるとします。

#### 6. 1に戻る

子の棒に止まったら、その棒に対して処理1から順に同様の処理を再帰的に行います。

TSSBの処理はこの1~6の繰り返しですが、実際にこの後の処理がどのように行われるかを可視化すると、まず比率$\nu_2$で棒を折ります。

![image](/images/post/2017-03-07/ithmm_tssb_6.png)

確率$\nu_2$で親にとどまり、確率$1-\nu_2$で子に降ります。

![image](/images/post/2017-03-07/ithmm_tssb_7.png)

この作業を品詞列$s_1 \ldots s_{\infty}$のそれぞれの品詞について行うと、木は細分化され以下のようになります。

![image](/images/post/2017-03-07/ithmm_tssb_8.png)

木構造は以下のようになっています。

![image](/images/post/2017-03-07/ithmm_tssb_9.png)

このようにして得られるTSSBですが、上から押しつぶすと一本の棒になり、通常のIHMMと同等になります。

![image](/images/post/2017-03-07/ithmm_tssb_ihmm.png)
