---
layout: post
title: 無限木構造隠れMarkovモデルによる階層的品詞の教師なし学習
category: 実装
tags:
- HMM
- iTHMM
excerpt_separator: <!--more-->
---

## 概要

- [無限木構造隠れMarkovモデルによる階層的品詞の教師なし学習](http://chasen.org/~daiti-m/paper/nl226ithmm.pdf)を読んだ
- C++で実装した

## はじめに

無限木構造隠れMarkovモデル（Infinite Tree HMM、以下iTHMM）は、状態遷移確率を木構造棒折り過程により表す隠れマルコフモデルです。

以前に実装した[Infinite HMM](/2017/02/27/Infinite-Hidden-Markov-Modelによる教師なし品詞推定/)と同様、どちらも状態数がデータから決まりますが、iTHMMは階層的な状態を学習することができます。

たとえば自然言語の品詞を考えると、「名詞」という親には「名詞 - 人名」などの子がありますが、従来のIHMMではこのような階層的な状態を学習することはできません。

実装難易度は以前に実装した[NPYLM](/2016/12/14/%E3%83%98-%E3%82%A4%E3%82%B9-%E9%9A%8E%E5%B1%A4%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%86-%E3%83%AB%E3%81%AB%E3%82%88%E3%82%8B%E6%95%99%E5%B8%AB%E3%81%AA%E3%81%97%E5%BD%A2%E6%85%8B%E7%B4%A0%E8%A7%A3%E6%9E%90/)と同じくらいだと感じました。

最後まで諦めない強い精神力が必要です。

## 参考文献

実装にあたって読まなければならない論文です。

- [無限木構造隠れMarkovモデルによる階層的品詞の教師なし学習](http://chasen.org/~daiti-m/paper/nl226ithmm-slides.pdf)
	- 本論文の発表スライドです
- [Tree-Structured Stick Breaking for Hierarchical Data](https://hips.seas.harvard.edu/files/adams-tssb-nips-2010.pdf)
	- 木構造棒折り過程の論文です
- [Hierarchical Dirichlet Processes](http://people.eecs.berkeley.edu/~jordan/papers/hdp.pdf)
	- 階層ディリクレ過程の解説論文です
- [A Bayesian Interpretation of Interpolated Kneser-Ney](https://www.stats.ox.ac.uk/~teh/research/compling/hpylm.pdf)
	- 階層Pitman-Yor言語モデルの論文です

## 棒折り過程

iTHMMを実装するには棒折り過程（以下SBP）がどのような仕組みになっているかを理解する必要があります。

ここでは基底分布を$G_0$とし、$k$番目のデータ$\theta_k$が$\theta_k \sim G_0$のように生成されている場合を考えます。

まず長さ1の棒を用意します。

![image](/images/post/2017-03-07/ithmm_sbp_1.png)

この棒を、現在の長さに対する比率$\gamma_1 \sim {\rm Be}(1, \alpha)$で折り、折った棒を$\theta_1$の位置に立てます。

![image](/images/post/2017-03-07/ithmm_sbp_2.png)

比率$\gamma_k$は$k$の値によらず$0 < \gamma_k < 1$を満たします。

$\gamma_k$は棒の長さではなく比率であることをよく覚えておいてください。

次に棒を$\gamma_2 \sim {\rm Be}(1, \alpha)$の比率で折り、$\theta_2$の位置に立てます。

![image](/images/post/2017-03-07/ithmm_sbp_3.png)

同様に$\gamma_3 \sim {\rm Be}(1, \alpha)$の比率で折り、$\theta_3$の位置に立てます。

![image](/images/post/2017-03-07/ithmm_sbp_4.png)

これを無限回繰り返すと$G_0$から離散分布$G$を生成することができます。

最終的に$\theta_k$の位置に立っている棒の長さが$\theta_k$の確率になります。

また長さが1の棒から始めているため、確率の総和は必ず1になります。

このように生成される$G$は多項分布になっているため、これをHMMに用いれば、無限の状態についてそれぞれの事前確率を生成することができます。

次に、$k$回目に折った棒の長さ$\pi_k$を考えます。

これは$k-1$回までの棒折り全てで残った棒を選択し、最後に折った棒を選択することになるので、以下のように表すことができます。

$$
	\begin{align}
		\pi_k = \gamma_k \prod_{j=1}^{k-1}(1-\gamma_j)
	\end{align}\
$$

何度も言いますが$\pi_k$は棒の長さであり、$\gamma_k$は比率であることに注意しましょう。

このようにして得られる無限次元の多項分布を$\pi=\\{\pi_1, \pi_2, \ldots\\}$で表します。

この$\pi$はハイパーパラメータ$\alpha$を調整することでどのような形にもなりますが、データから$\gamma_k$や$\pi_k$の事後分布を求めることができます。

いま、$\pi$からの実現値${\cal D} = \\{x_1, x_2, \ldots\\}$が与えられたとします。

${\cal D}$のもとでの$\pi$の事後分布$p(\pi \mid \cal D)$は、SBPを構成するそれぞれの確率変数$\gamma_k$の事後分布の積で表現することができ、$\cal D$の中で値が$k$であるものの個数を$n_0(k)$、値が$k$よりも大きいものの個数を$n_1(k)$とすると、$\gamma_k$の事後分布は

$$
	\begin{align}
		\gamma_k \mid {\cal D} \sim {\rm Be}(1+n_0(k), \alpha+n_1(k))
	\end{align}\
$$

となります。

この$\gamma_k$の期待値は以下のように計算できます。

$$
	\begin{align}
		\double E[\gamma_k \mid {\cal D}] = \frac{1+n_0(k)}{1+\alpha+n_0(k)+n_1(k)}
	\end{align}\
$$

したがって、$\pi_k$の事後確率の期待値は、式(1)と式(3)から

$$
	\begin{align}
		\double E[\pi_k \mid {\cal D}] 
		&= \frac{1+n_0(k)}{1+\alpha+n_0(k)+n_1(k)}\prod_{j=1}^{k-1}\left\{ 1-\frac{1+n_0(j)}{1+\alpha+n_0(j)+n_1(j)} \right\}\\
		&= \frac{1+n_0(k)}{1+\alpha+n_0(k)+n_1(k)}\prod_{j=1}^{k-1}\frac{\alpha+n_1(j)}{1+\alpha+n_0(j)+n_1(j)}\\
	\end{align}\
$$

となります。

## 木構造棒折り過程

上の棒折り過程の例では折った棒に親子関係などは一切ありませんが、木構造棒折り過程（以下TSSB）は折った棒をノードとする木構造を作ることで親子関係を考えます。

またTSSBは階層クラスタリングのための事前分布であり、データを実際に観測する前に、データをクラスタリングするとしたらどういう分割になるか？ということをモデル化するためのものです。

ここで言うデータとは品詞列$s_1 \ldots s_{\infty}$の各品詞$s_t$のことだと考えるとわかりやすいです。

実際に品詞列を観測する前に、もし品詞列$s_1 \ldots s_{\infty}$をこれから観測するとしたら、それぞれの品詞$s_t$はどのようにクラスタリングされるのか？というのが「クラスタリングのための事前分布」です。

TSSBでは棒に対する再帰的な処理を行うことで各$s_t$が木構造のどの位置に配置されるかを決定しますが、ここではその処理の流れについて先に見ていきます。

（クラスタリングされるデータそれぞれを「客」と呼びます）

#### 1. 棒が与えられる

![image](/images/post/2017-03-07/ithmm_tssb_1.png)

#### 2. 折る

比率$\nu_1$で折ります。

![image](/images/post/2017-03-07/ithmm_tssb_2.png)

#### 3. 停止するかどうか決める

確率$\nu_1$で客は折った棒にとどまります。

そうでない場合は子の棒に降ります。

![image](/images/post/2017-03-07/ithmm_tssb_3.png)

親にとどまる場合はここで処理終了です。

#### 4. 折る

比率$\psi_1$で子の棒を折ります。

また確率$\psi_1$で折った棒に止まるかどうかを決定します。

![image](/images/post/2017-03-07/ithmm_tssb_4.png)

ここでは止まらずに次に進むことを考えます。

#### 5. 折る

比率$\psi_2$で残った子の棒を折ります。

また確率$\psi_2$で折った棒に止まるかどうかを決定します。

![image](/images/post/2017-03-07/ithmm_tssb_5.png)

今回はここで止まるとします。

#### 6. 1に戻る

子の棒に止まったら、その棒に対して処理1から順に同様の処理を再帰的に行います。

TSSBの処理はこの1~6の繰り返しですが、実際にこの後の処理がどのように行われるかを可視化すると、まず比率$\nu_2$で棒を折ります。

![image](/images/post/2017-03-07/ithmm_tssb_6.png)

確率$\nu_2$で親にとどまり、確率$1-\nu_2$で子に降ります。

![image](/images/post/2017-03-07/ithmm_tssb_7.png)

この作業を品詞列$s_1 \ldots s_{\infty}$のそれぞれの品詞について行うと、木は細分化され以下のようになります。

![image](/images/post/2017-03-07/ithmm_tssb_8.png)

木構造は以下のようになっています。

![image](/images/post/2017-03-07/ithmm_tssb_9.png)

このようにして得られるTSSBですが、上から押しつぶすと一本の棒になり、IHMMにおける品詞の事前分布と同じであることがわかります。

![image](/images/post/2017-03-07/ithmm_tssb_ihmm.png)

棒の長さが確率を表すのは同じですが、TSSBではそれに加えてそれぞれの棒の階層構造を考えることができます。

通常の棒折り過程でクラスタリングを行えば、それぞれのクラスタは整数値$k=1,2,\ldots$で表すことができますが、TSSBの各クラスタ（つまりそれぞれの棒）は可変長の整数列$\boldsymbol s$で表されます。

まずルートノードは空の配列[]になります。

ルートノードの子は左から順に[1]、[2]、[3]のように表します。

同様にノード[1]の子は[1,1]、[1,2]、[1,3]のように表します。

後で出てきますが、あるノードの左側にあるノードのインデックスは辞書順で先に来る必要があります。

たとえばノード[1,2]とノード[1,3]であれば[1,2]の方が左側に来ます。

## TSSBのCDP表現

TSSBにおいて客がノード$\boldsymbol s$に止まる確率$\pi_{\boldsymbol s}$は以下のように定義されます。

$$
	\begin{align}
		\pi_{\boldsymbol s} &= \nu_{\boldsymbol s}\phi_{\boldsymbol s}\prod_{\boldsymbol s' \prec \boldsymbol s} \phi_{\boldsymbol s}(1 - \nu_{\boldsymbol s'})\\
		&= \nu_{\boldsymbol s}\prod_{\boldsymbol s' \prec \boldsymbol s}(1 - \nu_{\boldsymbol s'})\cdot \prod_{\boldsymbol s'\preceq \boldsymbol s}\phi_{\boldsymbol s'}\\
		\nu_{\boldsymbol s} &\sim {\rm Be}(1, \alpha)\\
		\psi_{\boldsymbol sk} &\sim {\rm Be}(1, \gamma)\\
		\phi_{\boldsymbol sk} &= \phi_{\boldsymbol sk}\prod_{j=1}^{k-1}(1-\phi_{sj})
	\end{align}\
$$

$\psi_{\boldsymbol sk}$はノード$\boldsymbol s$の$k$番目の子ノードを折る時の比率です。

この式からTSSBには$\nu$で定義される縦方向のSBPと$\psi$で定義される横方向のSBPの2つが存在することがわかります。

ここでは下図のようにノード$\boldsymbol s_{[2,3,2]}$に客が止まった状態を考えます。

![image](/images/post/2017-03-07/ithmm_cdp_1.png)

まず縦方向ですが、これは深さのみに注目すると以下のようなSBPになっています。

![image](/images/post/2017-03-07/ithmm_cdp_2.png)

次に横方向ですが、これはそれぞれの深さについてSBPが存在します。

![image](/images/post/2017-03-07/ithmm_cdp_3.png)
![image](/images/post/2017-03-07/ithmm_cdp_4.png)
![image](/images/post/2017-03-07/ithmm_cdp_5.png)

客を追加したら式(3)の事後分布を更新するために$n_0(\boldsymbol s)$、$n_1(\boldsymbol s)$、$m_0(\boldsymbol s)$、$m_1(\boldsymbol s)$を更新する必要がありますが、上の図の例だと以下のようになります。

![image](/images/post/2017-03-07/ithmm_cdp_6.png)

たった1人の客を追加するだけでもこれだけのパラメータを更新する必要があります。

## 無限木構造Markovモデル

TSSBを用いれば階層化された状態とそれぞれの確率が計算でき、無限木構造を持つモデルを作れることがわかりました。

しかし、このままではTSSBは状態の分布$p(s_j)$を表しているだけであり、HMMに用いるためには状態遷移確率$p(s_j \mid s_i)$を計算する必要があります。

つまり、TSSB上の各ノードについて、同じTSSB上の他の全ノードへの遷移確率が必要になります。

いま、客をクラスタリングした結果以下のようなTSSBが得られたとします。

左の木構造は右のTSSBの構造だけを抜き出したものです。

![image](/images/post/2017-03-07/ithmm_htssb_1.png)

このとき、それぞれの状態から別の状態への遷移確率はどのように定義すればよいでしょうか？

![image](/images/post/2017-03-07/ithmm_htssb_2.png)

iTHMMでは以下のように各ノードにTSSBを持たせることで遷移確率を表します。

![image](/images/post/2017-03-07/ithmm_htssb_3.png)

これらのTSSBはもとの木構造と同じ形をしていますが、それぞれの棒の長さは違います。

これらのTSSBでは、例えば$\boldsymbol s_{[1]}$への遷移確率は以下の場所に存在します。

![image](/images/post/2017-03-07/ithmm_htssb_4.png)

元の木構造の位置に対応する棒の長さがそのまま遷移確率になります。

iTHMMでは、それぞれのノードが持つ遷移確率用のTSSBを$\boldsymbol \pi^{\boldsymbol s}$で表します。

上付きか下付きかで意味が違ってきますので注意が必要です。

下付きの$\pi_{\boldsymbol s}$は棒の長さですが、上付きの$\boldsymbol \pi^{\boldsymbol s}$は、木構造上のノード$\boldsymbol s$が持つTSSBのことを差しています。

つまり、$$\boldsymbol \pi^{\boldsymbol s} = \{\pi^{\boldsymbol s}_{[]},\pi^{\boldsymbol s}_{[1]},\pi^{\boldsymbol s}_{[2]},\ldots\}$$です。

## 階層的TSSB

TSSBにおけるノードは木構造になっているので、各ノードからの遷移確率を表すTSSB$\boldsymbol \pi^{\boldsymbol s}$は独立ではなく、親子間の依存関係を持っていると考えるのが自然です。

例えばノード[2,3]が「名詞 - 固有名詞」を表していたとすると、[2,3]からの遷移確率$\boldsymbol \pi^{[2,3]}$は親ノードの[2]（つまり「名詞」を表す状態）の遷移確率$\boldsymbol \pi^{[2]}$の影響を受けているはずです。

