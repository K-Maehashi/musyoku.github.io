---
layout: post
title: ベイズ階層言語モデルによる教師なし形態素解析 [NPYLM]
category: 論文
tags:
- 自然言語処理
- VPYLM
- HPYLM
excerpt_separator: <!--more-->
---

## 概要

- [ベイズ階層言語モデルによる教師なし形態素解析](http://chasen.org/~daiti-m/paper/nl190segment.pdf)を読んだ
- NPYLMの実装方法の解説

## はじめに

NPYLMは文字nグラムモデルと単語nグラムモデルを教師なし学習によって同時に学習を行うモデルです。

文字nグラムモデルは文字列の並びに適切な確率を与えるモデルです。

例えばbrowに続いてnが出現する確率は文字nグラムモデルによって与えられます。

$$
	\begin{align}
		p(n \mid b, r, o, w)
	\end{align}\
$$

単語nグラムモデルは単語の並びに適切な確率を与えるモデルです。

the quick brownに続いてfoxが出現する確率は単語nグラムモデルによって与えられます。

$$
	\begin{align}
		p(fox \mid the, quick, brown)
	\end{align}\
$$

NPYLMではこれらのモデルを、分かち書きされていない文字列データから教師なしで学習します。

単語nグラムの学習の際に文字列を単語列に分かち書きする必要があり、その操作が「教師なし形態素解析」にあたります。

NPYLMは言語モデルであり、形態素解析手法ではありませんので注意が必要です。

## 参考文献

読んでおいたほうがいい文献です。

- [A Bayesian Interpretation of Interpolated Kneser-Ney](https://www.stats.ox.ac.uk/~teh/research/compling/hpylm.pdf)
- [Pitman-Yor過程に基づく可変長n-gram 言語モデル](http://chasen.org/~daiti-m/paper/nl178vpylm.pdf)
- [ベイズ階層言語モデルによる教師なし形態素解析](http://chasen.org/~daiti-m/paper/nl190segment-slides.pdf)
- [教師なし形態素解析とその周辺](http://chasen.org/~daiti-m/paper/Robotics-20130327.pdf)
- [理論はどうでもいいから作ってみたい人のためのNPYLM](http://www.slideshare.net/uchumik/dsirnlp06-nested-pitmanyor-language-model)

## おことわり

私は自然言語処理を勉強したことがないため、この分野の慣例などは全く知りません。

論文の理解が間違っている可能性があるためご了承下さい。

## ベイズ階層nグラム言語モデル

NPYLMを実装するにあたって、その根幹をなすベイズ階層言語モデルを理解する必要があります。

### nグラム分布の階層的な生成

ユニグラム（1グラム）分布$G_1 = p(\cdot)$があるとき、ある単語$w$を文脈とするバイグラム（2グラム）分布$G_2 = p(\cdot \mid w)$は、$G_1$とは異なるものの高頻度語などについて$G_1$を反映していると考えられます。

同様にしてトライグラム（3グラム）分布$G_3 = P(\cdot \mid w_1, w_2)$は$G_2$を反映し、4グラム分布$G_4 = P(\cdot \mid w_1, w_2, w_3)$は$G_3$を反映し、・・・

つまり、nグラム分布はn-1グラム分布を反映したものになっていると仮定します。

このような親の分布を反映した新しい分布を生成する仕組みとしてディリクレ過程やPitman-Yor過程があり、NPYLMではPitman-Yor過程に基づくnグラムモデルを用います。

（ディリクレ過程に基づくモデルも作ろうと思えば作れますが、自然言語がもつ"べき乗則"に従わないため、言語モデルにはPitman-Yor過程が使われます）

これらの確率過程については[続・わかりやすいパターン認識](http://amzn.asia/fauVKjZ)などの書籍に非常によくまとまっているため読むことをおすすめします。

### 中華料理店過程（CRP）

CRPはディリクレ過程の実現例の一つであり、クラスタリングの事前確率をモデル化する手法です。

これはどういうことかと言うと、まだデータを観測していない状態で、もしデータをクラスタリングするとしたらどのような分割が起こりうるか？ということをモデル化するものです。

自然言語処理においては、たとえばユニグラムの場合、ある1単語を観測する以前の状態のときに、これから観測する単語が何なのかをモデル化するということです。

CRPでは、テーブル、客、集中度パラメータ$\theta$を考えます。

これから観測するであろう単語を客と呼び、テーブル=単語、客数=その単語が生成された回数を表します。

$n$番目以降の客は、

- すでに$n_i$人着席しているテーブル$i$に確率$n_i/(n-1+\theta)$で着席する
- 新しいテーブルに確率$\theta/(n-1+\theta)$で着席する

のルールにもとづいてテーブルに着席します。

以下の図は7人目の客が座るテーブルを決定するそれぞれの確率を示しています。

![crp](/images/post/2016-12-11/crp.png)

この図には客は6人いますが、これは単語を観測する以前の状態で、単語を6個観測したと仮定すると、それら6個の単語はそれぞれ何なのかを表しています。

図より単語1を3回、単語2を2回、単語3を1回観測するであろうということがわかります。

ただし、あくまで分割の事前分布であるため、単語1が具体的にどの単語を表すかはまだ分かりません。

図では3種類の異なる単語が観測されるということが分かります。

次に、$n$番目までの客を観測した状態での$n+1$番目の客$x_{n+1}$の予測分布を考えます。

これは以下のように、

$$
	\begin{align}
		p(x_{n+1} \mid x_1,...,x_n) = \sum_{k=1}^{t_{\cdot}}\frac{c_k}{c_{\cdot}+\theta} + \frac{\theta}{c_{\cdot}+\theta}G_0
	\end{align}\
$$

各テーブルの客からなる経験分布と基底測度$G_0$の混合分布になります。

（$t_{\cdot}$は総テーブル数、$c_{\cdot}$は総客数、$c_k$は$k$番目のテーブルの客数）

ここで基底測度$G_0$が出てきましたが、これはテーブルを生成する親の連続分布です。一様分布を考えるとわかりやすいと思います。

この分布は、客$x_{n+1}$の座るテーブルが、$\frac{c_k}{c_{\cdot}+\theta}$の比で$k$番目のテーブルになり、$\frac{\theta}{c_{\cdot}+\theta}$の比で$G_0$から生成された新しいテーブルになることを表しています。

この分布から客$x_{n+1}$の座るテーブルが$w$になる確率を求めることができ、以下のようになります。

$$
	\begin{align}
		p(x_{n+1} = w \mid \Theta) = \frac{c_w}{c_{\cdot}+\theta} + \frac{\theta}{c_{\cdot}+\theta}G_0
	\end{align}\
$$

（$c_w$はテーブル$w$の客数、$\Theta$は$n$番目の客が着席した時点の客とテーブルの配置）

いま$G_0$が一様分布だと仮定します。

この時、$\theta$が大きければ$p(x_{n+1} = w \mid \Theta)$は$G_0$に近づくため、一様分布に近い分布になります。

逆に$\theta$が小さければ$p(x_{n+1} = w \mid \Theta)$は経験分布$\frac{c_w}{c_{\cdot}+\theta}$に近い分布になります。

このようにCRP（ディリクレ過程）は集中度パラメータ$\theta$と基底測度$G_0$から新しい分布$G$を生成することができ、$\theta$を調整することによって$G$を$G_0$に似せたり違う分布にしたりすることができます。

これは通常以下のように書きます。

$$
	\begin{align}
		G \sim DP(G_0, \theta)
	\end{align}\
$$

（$DP$はディリクレ過程を表します）

ちなみに$\theta \to \infty$では$G$は$G_0$に一致します。

ディリクレ過程では連続分布である基底測度と、生成済みのデータ点からなる経験分布との混合分布から次のデータを生成するのですが、CRPにおいて生成されるのはテーブル（単語）であり、客数はそのテーブル（単語）が生成された回数を表しています。

### Pitman-Yor過程

Pitman-Yor過程はCRPにディスカウント係数$d$を足したものになっており、基本はCRPと変わりません。

自然言語にはジップの法則と呼ばれる"べき乗則"が存在するのですが、Pitman-Yor過程はこのべき乗則に従う事前分布を作ることができるため、言語モデルの学習において良い事前分布となります。（詳しくは書籍をお読み下さい）

着席の際、$n$番目以降の客は、

- すでに$n_i$人着席しているテーブル$i$に確率$(n_i-d)/(n-1+\theta)$で着席する
- 新しいテーブルに確率$(\theta+dt_{\cdot})/(n-1+\theta)$で着席する

のルールにもとづいてテーブルに着席します。

ただし$t_{\cdot}$は$n-1$番目の客が着席した時点での総テーブル数を表します。

![pitman-yor](/images/post/2016-12-11/pitman-yor.png)

生成される分布$G$は

$$
	\begin{align}
		G \sim PY(G_0, \theta, d)
	\end{align}\
$$

のように書きます。

CRPと同様に予測分布は以下のようになります。

$$
	\begin{align}
		p(x_{n+1} \mid x_1,...,x_n) = \sum_{k=1}^{t_{\cdot}}\frac{c_k-d}{c_{\cdot}+\theta} + \frac{\theta+dt_{\cdot}}{c_{\cdot}+\theta}G_0\\
		p(x_{n+1} = w \mid \Theta) = \frac{c_w-dt_w}{c_{\cdot}+\theta} + \frac{\theta+dt_{\cdot}}{c_{\cdot}+\theta}G_0
	\end{align}\
$$

結局のところCRP（ディリクレ過程）やPitman-Yor過程がやっていることは、基底測度$G_0$に似た分布を作っているだけです。

そのことを理解しておけば問題はありません。

### 階層Pitman-Yor過程

Pitman-Yor過程の$G_0$をまた別のPitman-Yor過程にすることで階層Pitman-Yor過程を作ることができます。

先ほどnグラム分布はn-1グラム分布を反映していると仮定しましたが、階層Pitman-Yor仮定の枠組みでnグラムモデルを考えると以下の図のようになります。

![nested](/images/post/2016-12-11/nested.png)

ユニグラムの基底測度$G_0$は全語彙$V$の逆数の一様分布です。

この$G_0$をゼログラム分布と呼びます。

ユニグラム分布は$G_0$から生成され、バイグラム分布はユニグラム分布から生成され、トライグラム分布はバイグラムから生成されると仮定します。

具体的には以下のように生成されます。

$$
	\begin{align}
		p(\cdot) &\sim PY(\frac{1}{V}, d_1, \theta_1)\nonumber\\
		p(\cdot \mid the) &\sim PY(p(\cdot), d_2, \theta_2)\nonumber\\
		p(\cdot \mid the, quick) &\sim PY(p(\cdot \mid the), d_3, \theta_3)\nonumber\\
		p(\cdot \mid the, quick, brown) &\sim PY(p(\cdot \mid the, quick), d_4, \theta_4)\nonumber\\
	\end{align}\
$$

## 文脈木

階層Pitman-Yor過程の実装には文脈木を用います。

1つのPitman-Yor過程をレストランと呼び、文脈ごとにレストランが作られます。

たとえば以下のバイグラム分布はそれぞれ別のレストランにより管理されます。

$$
	\begin{align}
		p(\cdot \mid the) &\sim PY(p(\cdot), d_2, \theta_2)\nonumber\\
		p(\cdot \mid a) &\sim PY(p(\cdot), d_2, \theta_2)\nonumber\\
		p(\cdot \mid when) &\sim PY(p(\cdot), d_2, \theta_2)\nonumber\\
		p(\cdot \mid he) &\sim PY(p(\cdot), d_2, \theta_2)\nonumber\\
		p(\cdot \mid she) &\sim PY(p(\cdot), d_2, \theta_2)\nonumber\\
	\end{align}\
$$

集中度パラメータやディスカウント係数はnグラムオーダーごとに共通のものを使います。

ここで単語列the quick brown foxを観測したときに、レストラン$p(\cdot \mid the, quick, brown)$に単語foxを追加することを考えます。

![suffix_tree_1](/images/post/2016-12-11/suffix_tree_1.png)

文脈the quick brownを右から左に辿っていき、theに相当するノード（レストラン）に単語foxを追加します。

追加する際は以下の確率に従って追加するテーブルを決定します。

- $max(0, c_{wk}-d)$に比例する確率で、単語$w$のテーブルの中から$k$番目のテーブルに客を追加
- $(\theta+dt_{\cdot})p(w \mid quick, brown)$に比例する確率で単語$w$のテーブルを新しく生成しそこに客を追加

ここではレストランにテーブルfoxが1つもない場合を考えます。

![suffix_tree_2](/images/post/2016-12-11/suffix_tree_2.png)

この場合は$G_0 = p(\cdot \mid the, quick)$からテーブルfoxが生成され、客はそこに追加されます。

$G_0$から単語が生成された場合、代理客をレストラン$G_0$（つまり$p(\cdot \mid the, quick)$）に追加します。

これはスムージングのためと説明されていますが、Pitman-Yor過程では分布$G$は自らが生成した全ての客の配置に基づく経験分布と$G_0$によって決まるため、$p(\cdot \mid the, quick)$が単語を生成したのなら$p(\cdot \mid the, quick)$に客を追加するのは当然のことです。

この代理客の追加は再帰的に行われます。

つまり$p(\cdot \mid the, quick)$において、その親$p(\cdot \mid the)$からテーブルが生成された場合、$p(\cdot \mid the)$にも代理客が追加されます。

その結果、ルートノードのレストラン（ユニグラム分布$p(\cdot)$）にはすべての単語のテーブルが存在します。

またレストランには同じ単語のテーブルが複数存在してもかまいません。

テーブルの数=親から生成された回数となります。

この文脈木を用いて文脈$h$に続く単語$w$の確率を以下のように計算します。

$$
	\begin{align}
		p(w \mid h) = \frac{c_{hw}-dt_{hw}}{\theta+c_{h\cdot}}+\frac{\theta+dt_{h\cdot}}{\theta+c_{h\cdot}}p(w|\pi(h))
	\end{align}\
$$

$c_{h\cdot}$はレストラン$h$の総客数、$c_{hw}$は単語$w$のテーブルに着席している総客数、$t_{hw}$は単語$w$の総テーブル数、$t_{\cdot}$は総テーブル数を表します。

$\pi(h)$はオーダーを一つ落とした文脈で、具体的には

$$
	\begin{align}
		h &= the\ quick\ brown\nonumber\\
		\pi(h) &= quick\ brown\nonumber\\
		\pi(\pi(h)) &= brown\nonumber\\
	\end{align}\
$$

のようになります。

式(9)の$p(w \mid \pi(h))$の部分は式(9)を再帰的に用いて計算され、$p(w \mid \pi(h))$がゼログラム分布に到達するまで計算を行ないます。

## HPYLMとVPYLM

式(9)が単語ユニグラムの場合、$p(w \mid \pi(h))$が単語ゼログラム確率になりますが、NPYLMではこれを一様分布ではなく文字nグラムモデルによって

$$
	\begin{align}
		G_0(w) &= p(c_1c_2...c_k)\\
		p(c_1c_2...c_k) &= p(c_1)p(c_2 \mid c_1)...p(c_k \mid c_1c_2...c_{k-1})
	\end{align}\
$$

のように計算します。

よってNPYLMは図のように単語nグラムモデルの基底測度に文字nグラムモデルが埋め込まれた階層nグラムモデルになります。

![npylm](/images/post/2016-12-11/npylm.png)

（論文より引用）

NPYLMにおいて単語nグラムの文脈木の深さはあまり必要ではありません。

そこでNPYLMの単語nグラムはバイグラムかトライグラムとし、深さは2か3に固定します。

よって固定の深さの階層ベイズ言語モデルであるHYPLMを単語nグラムに用います。

これは[以前のHPYLMの記事](/2016/07/26/A_Hierarchical_Bayesian_Language_Model_based_on_Pitman-Yor_Processes/)で解説を行っています。

一方で文字nグラムモデルはあらゆる文字列に適切な確率を与える必要があり、単語の文字列長は$1 \sim \infty$が考えられるため、オーダーnに依存するHPYLMでは問題が生じます。

そこで文字nグラムモデルには可変長nグラムモデルであるVPYLMを用います。

これも[以前のVPYLMの記事](/2016/07/28/Pitman-Yor%E9%81%8E%E7%A8%8B%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E5%8F%AF%E5%A4%89%E9%95%B7n-gram%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB/)で解説しているため参考にして下さい。

余談ですが[VPYLMとHPYLMのPythonラッパー](/2016/11/14/VPYLM-HPYLM%E3%81%AEPython%E3%83%A9%E3%83%83%E3%83%91%E3%83%BC/)もあります。

VPYLMの基底測度$G_0$は可能な総文字種類数の逆数の一様分布とします。

## 学習

NPYLMにおいて学習すべきパラメータはHPYLMとVPYLMを含めたすべての客の配置$\Theta$です。

$\Theta$の推定にはギブスサンプリングを用います。

これはHPYLMやVPYLMと同じで、観測された単語列をモデルに追加・削除することで客の配置をギブスサンプリングし最適化します。

ただし今回は単語境界が未知であるため、まず文を単語に分解する必要があります。

NPYLMでは文ごとの単語分割$\boldsymbol {\rm w}$を動的計画法によりまとめてサンプリングし、得られた単語列をHPYLMに渡し、得られた各単語の文字列をVPYLMに渡して学習を行います。

### HPYLM

HPYLMのパラメータ（全ての客の配置）$\Theta_{hpylm}$のギブスサンプリングでは、まず単語列$\boldsymbol {\rm w}=w_1w_2...w_n$のそれぞれの単語について、対応するレストランから客を一人削除します。（この時確率的に代理客も削除されます）

こうすると新しいパラメータ$\lnot\Theta_{hpylm}$が自動的に決まりますが、これによりパラメータがギブスサンプリングされたことになります。

その後それぞれの単語を再追加することで新たな客の配置$\Theta_{hpylm}$が決定します。（ギブスサンプリングされたことになります）

### VPYLM

HPYLMのユニグラムのノードにおいて、テーブル$w$が新たに生成された場合を考えます。

テーブルは単語ゼログラム分布（=VPYLM）から生成されるため、客$w$を文字列に分解した$c_1c_2...c_k$のそれぞれの文字をVPYLMに追加することによってパラメータ$\Theta_{vpylm}$を更新します。

この操作は階層Pitman-Yor過程においてテーブルが親から生成された際に代理客を親に送るのと同じ操作です。

ここでは代理客$w$の文字列$c_1c_2...c_k$が親（VPYLM）に送られています。

またVPYLMではHPYLMとは異なり、客を追加する際にオーダー$n$をサンプリングします。

文字列$c_1c_2...c_k$を追加する際にはオーダー$n_1n_2...n_k$がサンプリングにより決定し、このオーダーは文字列$c_1c_2...c_k$を削除する際に必要になるため保存しておく必要があります。

さらに同一の単語$w$のテーブルは複数存在しうるため、テーブルの数だけオーダー$n_1n_2...n_k$が存在することになります。

実装する際は$m$番目の単語$w_m$をVPYLMに追加した際にサンプリングされたオーダー$n_{m1}n_{m2}...n_{mk}$を取れるようにします。

![vpylm-hpylm](/images/post/2016-12-11/vpylm-hpylm.png)

一方でユニグラムのノードからテーブル$w$が消えた場合、VPYLMから$w$の文字列$c_1c_2...c_k$を削除することで新たな配置$\Theta_{vpylm}$をギブスサンプリングします。

## Forward filtering-Backward sampling

具体的に文$s$から単語列$\boldsymbol {\rm w}$をサンプリングするには、Forward filtering-Backward sampling法を用います。

### Forward filtering

HPYLMがバイグラムの場合は前向き確率$\alpha[t][k]$を用います。

これは$s$の部分文字列$c_1c_2...c_t$の最後の$k$文字が単語である確率を表しており、以下の再帰式により周辺化されています。

$$
	\begin{align}
		\alpha[t][k]=\sum_{j=1}^{t-k}p(c^t_{t-k+1} \mid c^{t-k}_{t-k-j+1})\cdot \alpha[t-k][j]
	\end{align}\
$$

ただし$c^m_n=c_n...c_m$です。

![forward-filtering-2](/images/post/2016-12-11/forward-filtering-2.png)

$p(X \mid Y)$はHPYLMにより計算します。

次にトライグラムの場合は$\alpha[t][k][j]$を用います。

これは$s$の部分文字列$c_1c_2...c_t$が、最後の$k$文字を単語とし、さらにその$j$文字前が単語である確率を表しています。

$$
	\begin{align}
		\alpha[t][k][j]=\sum_{i=1}^{t-k-j}p(c^t_{t-k+1} \mid c^{t-k-j}_{t-k-j-i+1}c^{t-k}_{t-k-j+1})\cdot \alpha[t-k][j][i]
	\end{align}\
$$

![forward-filtering-3](/images/post/2016-12-11/forward-filtering-3.png)

### Backward sampling

前向き確率テーブル$\alpha$が求まれば、文末から後ろ向きに可能な単語分割をサンプリングします。

文$s$の文字数を$N$とすると、$\alpha[N][k]$は最後の$k$文字が単語である確率を表します。

これに加えて文末には必ず文末を表す特殊な単語$$<eos>$$が存在するため、$$p(<eos> \mid c^N_{N-k+1})\cdot\alpha[N][k]$$に比例する確率で$k$をサンプリングします。

$k$をサンプリングしたら次は$\alpha[t-k][m]$に比例する確率で$m$をサンプリングし、次は$\alpha[t-k-m][n]$から$n$をサンプリングし、・・・

のように後ろから順に単語長をサンプリングし文$s$を単語に分割します。

トライグラムの場合、$\alpha[N][k][j]$は最後の$k$文字とその前の$j$文字が単語である確率を表します。

よって$$p(<eos> \mid c^{N-k}_{N-k-j+1}c^N_{N-k+1})\cdot\alpha[N][k][j]$$に比例する確率で$k$と$j$を同時にサンプリングします。

つまりトライグラムの場合は後ろから2単語を同時に分割します。

### 可能な単語の最大長

文$s$が100文字からなり、$t=100$の場合、$\alpha[100][1]$から$\alpha[100][100]$を計算する必要があるのですが、実際の単語長はそこまで長くなく、おおよそ15～20文字が最大長であると考えられます。

そこでforward-filteringでは可能な単語の最大長を$L$とし、それ以上の長さの部分は計算を省略します。

つまり$\alpha[t][1]$から$\alpha[t][L]$までを計算し、$\alpha[t][L+1]$から$\alpha[t][t]$は計算しません。

## ポアソン補正

文字nグラムモデルによる単語nグラムの基底測度の計算では、式(10)をそのまま使うと長い単語の確率が小さくなりすぎます。

式(11)を見れば明らかですが、単純に長い文字列は積の回数が増えるため短い文字列に比べてどうしても確率が小さくなってしまいます。

そこでNPYLMでは以下のようにポアソン分布を用いた補正を行います。

$$
	\begin{align}
		p(c_1...c_k) &= p(c_1...c_k \mid k, \Theta_{vpylm})p(k \mid \Theta_{vpylm})\\
		&= p(c_1...c_k, k \mid \Theta_{vpylm})\\
		&= \frac{p(c_1...c_k, k \mid \Theta_{vpylm})}{p(k \mid \Theta_{vpylm})}Po(k \mid \lambda)\\
		&= p(c_1...c_k \mid k, \Theta_{vpylm})Po(k \mid \lambda)\\

	\end{align}\
$$

$p(c_1...c_k \mid k, \Theta_{vpylm})$は文字数$k$のあらゆる文字列の中で$c_1...c_k$が生成される確率を表します。

これはどういうことかというと、例えば10文字の単語が世界に以下の4つしか存在しない場合を考えます。

$$
	\begin{align}
		p(absolutely, k=10 \mid \Theta_{vpylm}) = 0.0000000000000001\nonumber\\
		p(accelerate, k=10 \mid \Theta_{vpylm}) = 0.0000000000000001\nonumber\\
		p(additional, k=10 \mid \Theta_{vpylm}) = 0.0000000000000001\nonumber\\
		p(aggressive, k=10 \mid \Theta_{vpylm}) = 0.0000000000000001\nonumber\\
	\end{align}\
$$

この時、$p(c_1...c_k \mid k, \Theta_{vpylm})$は上記の確率の総和で割ると求まります。

$$
	\begin{align}
		p(absolutely \mid k=10, \Theta_{vpylm}) = 0.25\nonumber\\
		p(accelerate \mid k=10, \Theta_{vpylm}) = 0.25\nonumber\\
		p(additional \mid k=10, \Theta_{vpylm}) = 0.25\nonumber\\
		p(aggressive \mid k=10, \Theta_{vpylm}) = 0.25\nonumber\\
	\end{align}\
$$

これにポアソン分布$Po(k=10 \mid \lambda)$を掛けて補正します。

たとえば$\lambda=4$とすると$Po(k=10 \mid \lambda=4) = 0.00529$ですので、補正後の確率は

$$
	\begin{align}
		p(absolutely \mid k=10, \Theta_{vpylm})Po(k=10 \mid \lambda=4) = 0.0013225\nonumber\\
		p(accelerate \mid k=10, \Theta_{vpylm})Po(k=10 \mid \lambda=4) = 0.0013225\nonumber\\
		p(additional \mid k=10, \Theta_{vpylm})Po(k=10 \mid \lambda=4) = 0.0013225\nonumber\\
		p(aggressive \mid k=10, \Theta_{vpylm})Po(k=10 \mid \lambda=4) = 0.0013225\nonumber\\
	\end{align}\
$$

となり、$p(c_1...c_k, k \mid \Theta_{vpylm})$に比べて自然な値になっています。

後述しますがこの補正が非常に重要であり、NPYLMの学習がうまくいくかどうかはこの補正を正しく行えるかに懸かっています。

## 実装

ここまでは論文に載っている部分の説明を行ないましたが、ここからは実際にコードを書く際に必要であり、かつ論文に載っていないことを説明していきます。