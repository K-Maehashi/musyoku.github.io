---
layout: post
title: ベイズ階層言語モデルによる教師なし形態素解析 [NPYLM]
category: 論文
tags:
- 自然言語処理
- VPYLM
- HPYLM
excerpt_separator: <!--more-->
---

## 概要

- [ベイズ階層言語モデルによる教師なし形態素解析](http://chasen.org/~daiti-m/paper/nl190segment.pdf)を読んだ
- NPYLMの実装方法の解説

## はじめに

NPYLMは文字nグラムモデルと単語nグラムモデルを教師なし学習によって同時に学習を行うモデルです。

文字nグラムモデルは文字列の並びに適切な確率を与えるモデルです。

例えばbrowに続いてnが出現する確率は文字nグラムモデルによって与えられます。

$$
	\begin{align}
		p(n \mid b, r, o, w)
	\end{align}\
$$

単語nグラムモデルは単語の並びに適切な確率を与えるモデルです。

the quick brownに続いてfoxが出現する確率は単語nグラムモデルによって与えられます。

$$
	\begin{align}
		p(fox \mid the, quick, brown)
	\end{align}\
$$

NPYLMではこれらのモデルを、分かち書きされていない文字列データから教師なしで学習します。

単語nグラムの学習の際に文字列を単語列に分かち書きする必要があり、その操作が「教師なし形態素解析」にあたります。

NPYLMは言語モデルであり、形態素解析手法ではありませんので注意が必要です。

## 参考文献

読んでおいたほうがいい文献です。

- [A Bayesian Interpretation of Interpolated Kneser-Ney](https://www.stats.ox.ac.uk/~teh/research/compling/hpylm.pdf)
- [Pitman-Yor過程に基づく可変長n-gram 言語モデル](http://chasen.org/~daiti-m/paper/nl178vpylm.pdf)
- [ベイズ階層言語モデルによる教師なし形態素解析](http://chasen.org/~daiti-m/paper/nl190segment-slides.pdf)
- [教師なし形態素解析とその周辺](http://chasen.org/~daiti-m/paper/Robotics-20130327.pdf)
- [理論はどうでもいいから作ってみたい人のためのNPYLM](http://www.slideshare.net/uchumik/dsirnlp06-nested-pitmanyor-language-model)

## おことわり

私は自然言語処理を勉強したことがないため、この分野の慣例などは全く知りません。

論文の理解が間違っている可能性があるためご了承下さい。

## ベイズ階層nグラム言語モデル

NPYLMを実装するにあたって、その根幹をなすベイズ階層言語モデルを理解する必要があります。

### nグラム分布の階層的な生成

ユニグラム（1グラム）分布$G_1 = p(\cdot)$があるとき、ある単語$w$を文脈とするバイグラム（2グラム）分布$G_2 = p(\cdot \mid w)$は、$G_1$とは異なるものの高頻度語などについて$G_1$を反映していると考えられます。

同様にしてトライグラム（3グラム）分布$G_3 = P(\cdot \mid w_1, w_2)$は$G_2$を反映し、4グラム分布$G_4 = P(\cdot \mid w_1, w_2, w_3)$は$G_3$を反映し、・・・

つまり、nグラム分布はn-1グラム分布を反映したものになっていると仮定します。

このような親の分布を反映した新しい分布を生成する仕組みとしてディリクレ過程やPitman-Yor過程があり、NPYLMではPitman-Yor過程に基づくnグラムモデルを用います。

（ディリクレ過程に基づくモデルも作ろうと思えば作れますが、自然言語がもつ"べき乗則"に従わないため、言語モデルにはPitman-Yor過程が使われます）

これらの確率過程については[続・わかりやすいパターン認識](http://amzn.asia/fauVKjZ)などの書籍に非常によくまとまっているため読むことをおすすめします。

### 中華料理店過程（CRP）

CRPはディリクレ過程の実現例の一つであり、クラスタリングの事前確率をモデル化する手法です。

これはどういうことかと言うと、まだデータを観測していない状態で、もしデータをクラスタリングするとしたらどのような分割が起こりうるか？ということをモデル化するものです。

自然言語処理においては、たとえばユニグラムの場合、ある1単語を観測する以前の状態のときに、これから観測する単語が何なのかをモデル化するということです。

CRPでは、テーブル、客、集中度パラメータ$\theta$を考えます。

これから観測するであろう単語を客と呼び、テーブル=単語、客数=その単語が生成された回数を表します。

$n$番目以降の客は、

- すでに$n_i$人着席しているテーブル$i$に確率$n_i/(n-1+\theta)$で着席する
- 新しいテーブルに確率$\theta/(n-1+\theta)$で着席する

のルールにもとづいてテーブルに着席します。

以下の図は7人目の客が座るテーブルを決定するそれぞれの確率を示しています。

![crp](/images/post/2016-12-11/crp.png)

この図には客は6人いますが、これは単語を観測する以前の状態で、単語を6個観測したと仮定すると、それら6個の単語はそれぞれ何なのかを表しています。

図より単語1を3回、単語2を2回、単語3を1回観測するであろうということがわかります。

ただし、あくまで分割の事前分布であるため、単語1が具体的にどの単語を表すかはまだ分かりません。

図では3種類の異なる単語が観測されるということが分かります。

次に、$n$番目までの客を観測した状態での$n+1$番目の客$x_{n+1}$の予測分布を考えます。

これは以下のように、

$$
	\begin{align}
		p(x_{n+1} \mid x_1,...,x_n) = \sum_{k=1}^{t_{\cdot}}\frac{c_k}{c_{\cdot}+\theta} + \frac{\theta}{c_{\cdot}+\theta}G_0
	\end{align}\
$$

各テーブルの客からなる経験分布と基底測度$G_0$の混合分布になります。

（$t_{\cdot}$は総テーブル数、$c_{\cdot}$は総客数、$c_k$は$k$番目のテーブルの客数）

ここで基底測度$G_0$が出てきましたが、これはテーブルを生成する親の連続分布です。一様分布を考えるとわかりやすいと思います。

この分布は、客$x_{n+1}$の座るテーブルが、$\frac{c_k}{c_{\cdot}+\theta}$の比で$k$番目のテーブルになり、$\frac{\theta}{c_{\cdot}+\theta}$の比で$G_0$から生成された新しいテーブルになることを表しています。

この分布から客$x_{n+1}$の座るテーブルが$w$になる確率を求めることができ、以下のようになります。

$$
	\begin{align}
		p(x_{n+1} = w \mid \Theta) = \frac{c_w}{c_{\cdot}+\theta} + \frac{\theta}{c_{\cdot}+\theta}G_0
	\end{align}\
$$

（$c_w$はテーブル$w$の客数、$\Theta$は$n$番目の客が着席した時点の客とテーブルの配置）

いま$G_0$が一様分布だと仮定します。

この時、$\theta$が大きければ$p(x_{n+1} = w \mid \Theta)$は$G_0$に近づくため、一様分布に近い分布になります。

逆に$\theta$が小さければ$p(x_{n+1} = w \mid \Theta)$は経験分布$\frac{c_w}{c_{\cdot}+\theta}$に近い分布になります。

このようにCRP（ディリクレ過程）は集中度パラメータ$\theta$と基底測度$G_0$から新しい分布$G$を生成することができ、$\theta$を調整することによって$G$を$G_0$に似せたり違う分布にしたりすることができます。

これは通常以下のように書きます。

$$
	\begin{align}
		G \sim DP(G_0, \theta)
	\end{align}\
$$

（$DP$はディリクレ過程を表します）

ちなみに$\theta \to \infty$では$G$は$G_0$に一致します。

ディリクレ過程では連続分布である基底測度と、生成済みのデータ点からなる経験分布との混合分布から次のデータを生成するのですが、CRPにおいて生成されるのはテーブル（単語）であり、客数はそのテーブル（単語）が生成された回数を表しています。

### Pitman-Yor過程

Pitman-Yor過程はCRPにディスカウント係数$d$を足したものになっており、基本はCRPと変わりません。

自然言語にはジップの法則と呼ばれる"べき乗則"が存在するのですが、Pitman-Yor過程はこのべき乗則に従う事前分布を作ることができるため、言語モデルの学習において良い事前分布となります。（詳しくは書籍をお読み下さい）

着席の際、$n$番目以降の客は、

- すでに$n_i$人着席しているテーブル$i$に確率$(n_i-d)/(n-1+\theta)$で着席する
- 新しいテーブルに確率$(\theta+dt_{\cdot})/(n-1+\theta)$で着席する

のルールにもとづいてテーブルに着席します。

ただし$t_{\cdot}$は$n-1$番目の客が着席した時点での総テーブル数を表します。

![pitman-yor](/images/post/2016-12-11/pitman-yor.png)

生成される分布$G$は

$$
	\begin{align}
		G \sim PY(G_0, \theta, d)
	\end{align}\
$$

のように書きます。

CRPと同様に予測分布は以下のようになります。

$$
	\begin{align}
		p(x_{n+1} \mid x_1,...,x_n) = \sum_{k=1}^{t_{\cdot}}\frac{c_k-d}{c_{\cdot}+\theta} + \frac{\theta+dt_{\cdot}}{c_{\cdot}+\theta}G_0\\
		p(x_{n+1} = w \mid \Theta) = \frac{c_w-dt_w}{c_{\cdot}+\theta} + \frac{\theta+dt_{\cdot}}{c_{\cdot}+\theta}G_0
	\end{align}\
$$

結局のところCRP（ディリクレ過程）やPitman-Yor過程がやっていることは、基底測度$G_0$に似た分布を作っているだけです。

そのことを理解しておけば問題はありません。

### 階層Pitman-Yor過程

Pitman-Yor過程の$G_0$をまた別のPitman-Yor過程にすることで階層Pitman-Yor過程を作ることができます。

先ほどnグラム分布はn-1グラム分布を反映していると仮定しましたが、階層Pitman-Yor仮定の枠組みでnグラムモデルを考えると以下の図のようになります。

![nested](/images/post/2016-12-11/nested.png)

ユニグラムの基底測度$G_0$は全語彙$V$の逆数の一様分布です。

ユニグラム分布は$G_0$から生成され、バイグラム分布はユニグラム分布から生成され、トライグラム分布はバイグラムから生成されると仮定します。

具体的には以下のように生成されます。

$$
	\begin{align}
		p(\cdot) &\sim PY(\frac{1}{V}, d_1, \theta_1)\nonumber\\
		p(\cdot \mid the) &\sim PY(p(\cdot), d_2, \theta_2)\nonumber\\
		p(\cdot \mid the, quick) &\sim PY(p(\cdot \mid the), d_3, \theta_3)\nonumber\\
		p(\cdot \mid the, quick, brown) &\sim PY(p(\cdot \mid the, quick), d_4, \theta_4)\nonumber\\
	\end{align}\
$$

## 文脈木

階層Pitman-Yor過程の実装には文脈木を用います。

1つのPitman-Yor過程をレストランと呼び、文脈ごとにレストランが作られます。

たとえば以下のバイグラム分布はそれぞれ別のレストランにより管理されます。

$$
	\begin{align}
		p(\cdot \mid the) &\sim PY(p(\cdot), d_2, \theta_2)\nonumber\\
		p(\cdot \mid a) &\sim PY(p(\cdot), d_2, \theta_2)\nonumber\\
		p(\cdot \mid when) &\sim PY(p(\cdot), d_2, \theta_2)\nonumber\\
		p(\cdot \mid he) &\sim PY(p(\cdot), d_2, \theta_2)\nonumber\\
		p(\cdot \mid she) &\sim PY(p(\cdot), d_2, \theta_2)\nonumber\\
	\end{align}\
$$

集中度パラメータやディスカウント係数はnグラムオーダーごとに共通のものを使います。

ここで単語列the quick brown foxを観測したときに、レストラン$p(\cdot \mid the, quick, brown)$に単語foxを追加することを考えます。

![suffix_tree_1](/images/post/2016-12-11/suffix_tree_1.png)

文脈the quick brownを右から左に辿っていき、theに相当するノード（レストラン）に単語foxを追加します。

追加する際は以下の確率に従って追加するテーブルを決定します。

- $max(0, c_{wk}-d)$に比例する確率で、単語$w$のテーブルの中から$k$番目のテーブルに客を追加
- $(\theta+dt_{\cdot})p(w \mid quick, brown)$に比例する確率で単語$w$のテーブルを新しく生成しそこに客を追加

ここではレストランにテーブルfoxが1つもない場合を考えます。

![suffix_tree_2](/images/post/2016-12-11/suffix_tree_2.png)

この場合は$G_0 = p(\cdot \mid the, quick)$からテーブルfoxが生成され、客はそこに追加されます。

$G_0$から単語が生成された場合、代理客をレストラン$G_0$（つまり$p(\cdot \mid the, quick)$）に追加します。

これはスムージングのためと説明されていますが、Pitman-Yor過程では分布$G$は自らが生成した全ての客の配置に基づく経験分布と$G_0$によって決まるため、$p(\cdot \mid the, quick)$が単語を生成したのなら$p(\cdot \mid the, quick)$に客を追加するのは当然のことです。

この代理客の追加は再帰的に行われます。

つまり$p(\cdot \mid the, quick)$において、その親$p(\cdot \mid the)$からテーブルが生成された場合、$p(\cdot \mid the)$にも代理客が追加されます。

その結果、ルートノードのレストラン（ユニグラム分布$p(\cdot)$）にはすべての単語のテーブルが存在します。

またレストランには同じ単語のテーブルが複数存在してもかまいません。

テーブルの数=親から生成された回数となります。

一般的に、文脈$h$に続く単語$w$の確率は以下のように計算します。

$$
	\begin{align}
		p(w \mid h) = \frac{c_{hw}-dt_{hw}}{\theta+c_{h\cdot}}+\frac{\theta+dt_{h\cdot}}{\theta+c_{h\cdot}}p(w|\pi(h))
	\end{align}\
$$

