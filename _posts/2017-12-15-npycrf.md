---
layout: post
title:  条件付確率場とベイズ階層言語モデルの統合による半教師あり形態素解析（NPYCRF）
category: 実装
tags:
- NPYCRF
excerpt_separator: <!--more-->
---

この記事は[自然言語処理 Advent Calendar 2017](https://qiita.com/advent-calendar/2017/nlp)の19日目の記事です。

<!--more-->

## はじめに

今回は半教師あり形態素解析の手法であるNPYCRFについて、その理論と実装方法をまとめました。

一般に形態素解析はCRFによる識別モデルが用いられ、CRFのパラメータは教師付きデータにより最適化されます。

しかし、古くはしょこたんブログに始まり、最近ではTwitterなどのソーシャルメディアで日々新しい単語が生み出されているため、こうしたテキストデータを適切に解析できるモデルが求められます。


（余談ですが最近は単語を省略するのが流行っているらしく、例えば「バイト」を「バ」と言うそうです。例）「バイト終わった」→「バおわ」）


NPYCRFは、教師なしデータの単語分割のための言語モデルであるNPYLMと、形態素解析のための識別モデルであるCRFを統合し、教師付きデータの分割基準を守りながら教師なしデータの単語分割を学習できるようになっています。

NPYLMに加えてCRFとL-BFGSを実装しなければならないため、実装難易度は当ブログ史上最高となっています。

最初から実装する場合は半年、NPYLMをベースに実装する場合は2ヶ月程度かかると思います。

また、通常こういった論文はページ数の制約があるため、本当にすべきことの1割程度しか論文に書かれません。

この記事では私の解釈に基づいて残りの9割の実装方法を説明するため、誤りを含む可能性があります。ご了承ください。

## 参考文献

実装するにあたり必要な文献です。

- [半教師あり形態素解析NPYCRFの修正](http://www.anlp.jp/proceedings/annual_meeting/2016/pdf_dir/D6-3.pdf)
	- オリジナル版のNPYCRFにはモデル統合部分に誤りがあります。
- [Nonparametric Bayesian Semi-supervised Word Segmentation](http://chasen.org/~daiti-m/paper/tacl2016semiseg.pdf)
	- 修正版の国際学会版です。より詳しくなっています。
- [条件付き確率場の理論と実践](http://www.ism.ac.jp/editsec/toukei/pdf/64-2-179.pdf)
	- CRFの理論や勾配計算について非常に詳しくまとまっています。
- [ベイズ階層言語モデルによる教師なし形態素解析](http://chasen.org/~daiti-m/paper/nl190segment.pdf)
	- NPYLMの論文です。
	- 去年[解説記事](/2016/12/14/%E3%83%99%E3%82%A4%E3%82%BA%E9%9A%8E%E5%B1%A4%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E3%82%88%E3%82%8B%E6%95%99%E5%B8%AB%E3%81%AA%E3%81%97%E5%BD%A2%E6%85%8B%E7%B4%A0%E8%A7%A3%E6%9E%90/)を書きました。

## 実装

コア実装をC++14で行い、BoostでPythonから利用できるようにしています。

[https://github.com/musyoku/python-npycrf](https://github.com/musyoku/python-npycrf)

まだ細かい部分が未完成ですが学習と単語分割はできます。

またNPYLMは特許が取られているため、この実装は研究以外の用途に使用しないでください。

<blockquote class="twitter-tweet" data-lang="ja"><p lang="ja" dir="ltr">正しい連絡先が分からないのでここで：「ご注文は機械学習ですか?」で特許のため公開できないとされているNPYLMのGitHub上の実装は、NTT知財に確認してもらったところ「試験または研究のための使用」であれば公開してもよいそうです。他の方の実装も、基本的に同じだと思います。</p>&mdash; Daichi Mochihashi (@daiti_m) <a href="https://twitter.com/daiti_m/status/851810748263157760?ref_src=twsrc%5Etfw">2017年4月11日</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


## NPYLMとCRF

NPYLMはセミマルコフモデル、CRFはマルコフモデルになっており、単語分割は以下のように表されます。

NPYLMは各ノードが単語を表しています。

CRFは1（＝語頭）と0（＝それ以外）の2つラベルを持ちます。

## 識別-生成統合モデル

NPYCRFではJESS-CM法を用いて、入力文$\boldsymbol x = x_1x_2 \cdots x_n$に対するラベル列$\boldsymbol y$の確率を以下のように表現します。

$$
	\begin{align}
		P_{\text{CONC}}(\boldsymbol y \mid \boldsymbol x) \propto P_{\text{CRF}}(\boldsymbol y \mid \boldsymbol x;\boldsymbol \Lambda)P_{\text{NPYLM}}(\boldsymbol y, \boldsymbol x;\boldsymbol \Theta)^{\lambda_0}
	\end{align}\
$$

$P_{\text{CRF}}$はCRF（識別モデル）、$P_{\text{NPYLM}}$はNPYLM（生成モデル)であり、$\boldsymbol \Lambda=(\lambda_1, \lambda_2, ..., \lambda_K)$はCRFのパラメータ（重み）、$\boldsymbol \Theta$はNPYLMのパラメータ（中華料理店過程の客の配置）を表しています。

$\propto$は比例を意味しているので式(1)は確率ではありませんが、$\boldsymbol x$に対する可能な全ての分割$\boldsymbol y$で式(1)を計算した総和$Z(x)^*$で割れば確率になるため、式(1)を等号で表すことができます。

ここで$\text{log}P_{\text{NPYLM}}(\boldsymbol y, \boldsymbol x;\boldsymbol \Theta)$を連続値を返す素性関数とみなせば、式(1)はパラメータ$\boldsymbol \Lambda^*=\lambda_0, \lambda_1, ..., \lambda_K$を持つ対数線形モデルの形で書くことができます。

$$
	\begin{align}
		P_{\text{CONC}}(\boldsymbol y \mid \boldsymbol x) &\propto \text{exp}
			\left(
					\lambda_0 \text{log}P_{\text{NPYLM}}(\boldsymbol y, \boldsymbol x;\boldsymbol \Theta)
					+
					\sum_{k=1}^K \lambda_k f_k(\boldsymbol y, \boldsymbol x)
			\right)\\
		&= \text{exp}(\boldsymbol \Lambda^* F^*(\boldsymbol y, \boldsymbol x))
	\end{align}\
$$

$f_k$はCRFの$k$番目の素性関数です。

## モデル変換

JESS-CMによるモデル統合では、識別モデルと生成モデルが同じ構造をしている必要があります。

しかしNPYCRFでは識別モデルがマルコフモデルであるのに対し、生成モデルがセミマルコフモデルになっているため、各モデルの情報を直接統合することができないので、何らかの変換処理を行う必要があります。

この変換処理はオリジナル版のNPYCRFに誤りがあり、修正版で正しい統合方法が提案されています。

## オリジナル版のモデル変換

ここからはNPYLMが単語bigramモデルであるとします。

文$\boldsymbol x$の$s$番目の文字$x_s$から$t$番目の文字$x_t$までの部分文字列を$c_s^t$で表します。

この時、マルコフモデル上の$c_s^t$に対応する区間$[s, t)$のポテンシャルを$\gamma(s, t)$とおくと、これは状態1で始まり状態1で終わるV字型の区間になります。

セミマルコフモデルの各ノードの遷移確率$P(c_t^{u-1} \mid c_s^{t-1})$に対応するポテンシャルとして、オリジナル版のNPYCRFでは

$$
	\begin{align}
		P(c_t^{u-1} \mid c_s^{t-1}) &\propto \text{exp}\left(\gamma(s, t) + \gamma(t, u)\right)
	\end{align}\
$$

を用いてNPYLMの前向き確率を

$$
	\begin{align}
		\alpha[t][k] = \sum_{j=1}^{t-k} \text{exp} 
			\left\{ 
				\lambda_0 \text{log}\left( P(c_{t-k+1}^t \mid c_{t-k-j+1}^{t-k}) \right) 
				+ \gamma(t-k-j+1, t-k+1) + \gamma(t-k+1, t+1)
			\right\} \cdot \alpha[t-k][j]
	\end{align}\
$$

としています。

$\alpha[t][k]$は、位置$t$から左の最も近い単語境界までの距離が$k$文字である確率を表しており、それ以前の分割全てについて周辺化された確率になっています。

マルコフモデルの前向き確率のようなものだと考えてください。

式(5)の$\text{exp}(\cdot)$が、CRFの情報を統合したNPYLMのセミマルコフモデル上での遷移確率になります。

オリジナル版のNPYCRFの誤りを分かりやすくするため、ここでは可能な単語の最大長を1として、長さ3文字の文$\boldsymbol x = x_1x_2x_3$の前向き確率を求めてみます。

（文頭に\<bos\>、文末に\<eos\>を挿入しています）

単語の最大長が1なので周辺化の処理がいらなくなり、以下のように前向き確率を求められます。

$$
	\begin{align}
		\alpha[1][1] &= \text{exp} \left\{ \lambda_0 \text{log}\left( P(c_1^1 \mid \text{<bos>}) \right) 
						+ \gamma(0, 1) + \gamma(1, 2) \right\}  \nonumber \\
		\alpha[2][1] &= \text{exp} \left\{ \lambda_0 \text{log}\left( P(c_2^2 \mid c_1^1) \right) 
						+ \gamma(1, 2) + \gamma(2, 3) \right\} \cdot  \alpha[1][1] \nonumber \\
		\alpha[3][1] &= \text{exp} \left\{ \lambda_0 \text{log}\left( P(c_3^3 \mid c_2^2) \right) 
						+ \gamma(2, 3) + \gamma(3, 4) \right\} \cdot  \alpha[2][1] \nonumber \\
		\alpha[4][1] &= \text{exp} \left\{ \lambda_0 \text{log}\left( P(<eos> \mid c_3^3) \right) 
						+ \gamma(3, 4) \right\} \cdot  \alpha[3][1]\\
	\end{align}\
$$

前向き確率の一番最後の値$\alpha[4][1]$は規格化定数$Z(x)^*$になります。

またこのケースでは可能な分割が1通りしか存在しないため、

$$
	\begin{align}
		P_{\text{CONC}}(\boldsymbol y \mid \boldsymbol x) &= P_{\text{CRF}}(\boldsymbol y \mid \boldsymbol x;\boldsymbol \Lambda)P_{\text{NPYLM}}(\boldsymbol y, \boldsymbol x;\boldsymbol \Theta)^{\lambda_0} \nonumber \\
		&= Z(x)^* \nonumber \\
		&= \alpha[4][1]
	\end{align}\
$$

となります。

式(6)と式(7)から以下のようになります。

$$
	\begin{align}
		P_{\text{CONC}}(\boldsymbol y \mid \boldsymbol x) &= \alpha[4][1] \nonumber \\
		&= \text{exp} \left\{ \lambda_0 \text{log}\left( P(<eos> \mid c_3^3) \right) 
						+ \gamma(3, 4) \right\} \cdot  \alpha[3][1] \nonumber \\
		&= \text{exp} \left\{ \lambda_0 \text{log}\left( P(<eos> \mid c_3^3) \right) 
						+ \gamma(3, 4) + \lambda_0 \text{log}\left( P(c_3^3 \mid c_2^2) \right) 
						+ \gamma(2, 3) + \gamma(3, 4) \right\} \cdot  \alpha[2][1] \nonumber \\
		&= \text{exp} \left\{ \lambda_0 \text{log}\left( P(<eos> \mid c_3^3) \right) 
						+ \gamma(3, 4) + \lambda_0 \text{log}\left( P(c_3^3 \mid c_2^2) \right) 
						+ \gamma(2, 3) + \gamma(3, 4) + \lambda_0 \text{log}\left( P(c_2^2 \mid c_1^1) \right) 
						+ \gamma(1, 2) + \gamma(2, 1) + \lambda_0 \text{log}\left( P(c_1^1 \mid \text{<bos>}) \right) 
						+ \gamma(0, 1) + \gamma(1, 2) \right\} \nonumber \\
		&= P(<eos> \mid c_3^3)^{\lambda_0}P(c_3^3 \mid c_2^2)^{\lambda_0}P(c_2^2 \mid c_1^1)^{\lambda_0}P(c_1^1 \mid \text{<bos>})^{\lambda_0} \nonumber \\ 
		&+ \text{exp}(2\gamma(3, 4))
		+ \text{exp}(2\gamma(2, 3))
		+ \text{exp}(2\gamma(1, 2))
		+ \text{exp}(\gamma(0, 1))
	\end{align}\
$$

一方、このケースで式(1)を展開すると以下のようになります。

$$
	\begin{align}
		P_{\text{CONC}}(\boldsymbol y \mid \boldsymbol x) &＝ P_{\text{CRF}}(\boldsymbol y \mid \boldsymbol x;\boldsymbol \Lambda)P_{\text{NPYLM}}(\boldsymbol y, \boldsymbol x;\boldsymbol \Theta)^{\lambda_0} \nonumber \\
		&＝ P(<eos> \mid c_3^3)^{\lambda_0}P(c_3^3 \mid c_2^2)^{\lambda_0}P(c_2^2 \mid c_1^1)^{\lambda_0}P(c_1^1 \mid \text{<bos>})^{\lambda_0} \nonumber\\ 
		&+ \text{exp}(\gamma(3, 4))
		+ \text{exp}(\gamma(2, 3))
		+ \text{exp}(\gamma(1, 2))
		+ \text{exp}(\gamma(0, 1))
	\end{align}\
$$

式(8)と式(9)をよく見比べると、式(8)では$\gamma$が2回出現しているため、前向き確率の計算時にモデルの統合が正しく行われていないことがわかります。

論文の以下の文はこのことを言っています。

>Markovモデルおよびsemi-Markovモデルそれぞれで特定のパス上のポテンシャルをそれぞれ一度ずつ足しあわせた形になっているはずであるが，(5) 式のように足しあわせて行くと，それぞれ γ(s, t) が二回ずつ現れることが分かる．この分解を見てもモデルの統合が正しくないことが直感的に理解できる．

このようにモデル統合に誤りがあるため、このままでは学習を正しく行えません。

## 正しいモデル変換

修正版のNPYCRFでは、任意の先行単語から単語$c_s^{t-1}$への遷移確率が$\gamma(s, t)$に対応していると考えます。

$$
	\begin{align}
		P(c_s^{t-1} \mid \cdot) = \frac{\text{exp}(\gamma(s, t))}{Z(x)}
	\end{align}\
$$

$Z(x)$はCRFの規格化定数（全経路のコストの総和）です。

さらに、前向き確率を以下のように表します。

$$
	\begin{align}
		\alpha[t][k] = \sum_{j=1}^{t-k} \text{exp} 
			\left\{ 
				\lambda_0 \text{log}\left( P(c_{t-k+1}^t \mid c_{t-k-j+1}^{t-k}) \right) 
				+ \gamma(t-k+1, t+1)
			\right\} \cdot \alpha[t-k][j]
	\end{align}\
$$

こうすることで前向き確率を終端まで計算した時に$P(c_{t-k+1}^t \mid c_{t-k-j+1}^{t-k})$と$\gamma(t-k+1, t+1)$が1回ずつ出現するため、モデル統合として辻褄が合っていることになります。

## CRFの素性

CRFの$k$番目の素性関数を$\pi_k(\boldsymbol x, y_{t-1}, y_t)$で表します。

素性関数は文字列$\boldsymbol x$と位置$t, t-1$のCRFの2値ラベル$y_t, y_{t-1}$が特定の組み合わせの時に1を返し、そうでない場合に0を返す関数になっています。

NPYLMでは以下の6種類の素性を考えます。

- $t-2, t-1, t, t+1, t+2$の位置の文字unigram
- $t-2, t-1, t, t+1$の位置の文字bigram
- $t-2, t-1, t, t+1$の位置について、$x_t = x_{t+1}$かどうか
- $t-3, t-2, t-1, t, t+1$の位置について、$x_t = x_{t+2}$かどうか
- $t$の位置の文字種のunigram
- $t$の位置の文字種のbigram

たとえば文字unigram素性として以下のようなものが考えられます。

$$
	\begin{align}
		\pi_k(\boldsymbol x, y_{t-1}, y_t) = 
			\begin{cases}
				1 & x_t\text{が"あ"} \land y_tが1 \land y_{t-1}が0 \\
				0 & それ以外
			\end{cases} \nonumber \\
	\end{align}\
$$

また、位置$t$のラベル$y_t$にのみ依存する素性関数$\pi_k(\boldsymbol x, y_t)$も用意します。

用いる素性の例をいくつか挙げておきます。（素性番号は適当です）

$$
	\begin{align}
		\pi_{33487}(\boldsymbol x, y_{t-1}, y_t) &= 
			\begin{cases}
				1 & x_{t-1}\text{が"機"} \land x_t\text{が"械"} \land y_tが1 \land y_{t-1}が0 \\
				0 & それ以外
			\end{cases} \nonumber \\

		\pi_{98}(\boldsymbol x, y_{t-1}, y_t) &= 
			\begin{cases}
				1 & x_{t-1} = x_t \\
				0 & それ以外
			\end{cases} \nonumber \\

		\pi_{128734}(\boldsymbol x, y_t) &= 
			\begin{cases}
				1 & \text{type}(x_t) = \text{"ひらがな"} \\
				0 & それ以外
			\end{cases} \nonumber \\
	\end{align}\
$$

学習時にはデータセットの各文から上記6通りの素性を展開し、それぞれの素性関数について重みを一つ用意するため、CRFのパラメータ数は数億を超えます。

分かりやすさのため$\pi_k(\boldsymbol x, y_{t-1}, y_t)$と表記しましたが、実際はラベルの値$i,j$と位置$t$を分けた$\pi_k(\boldsymbol x, i, j, t)$の形で素性関数を考えます。

$\boldsymbol x$の各位置$t$について素性関数を適用する必要があるからです。

私はCRFを実装するのは初めてなのでノウハウなどがよくわからないのですが、以下のように素性IDを返す素性関数を実装しました。

```cpp
int index_w_label_u(int y_i){
	int index = y_i;
	return index + _offset_w_label_u;
}
int index_w_label_b(int y_i_1, int y_i){
	int index =  y_i_1 * 2 + y_i;
	return index + _offset_w_label_b;
}
int index_w_unigram_u(int y_i, int i, int x_i){
	int index = x_i * _x_range_unigram * 2 + (i - 1) * 2 + y_i;
	return index + _offset_w_unigram_u;
}
int index_w_unigram_b(int y_i_1, int y_i, int i, int x_i){
	int index = x_i * _x_range_unigram * 2 * 2 + (i - 1) * 2 * 2 + y_i * 2 + y_i_1;
	return index + _offset_w_unigram_b;
}
int index_w_bigram_u(int y_i, int i, int x_i_1, int x_i){
	int index = x_i * _num_character_ids * _x_range_bigram * 2 + x_i_1 * _x_range_bigram * 2 + (i - 1) * 2 + y_i;
	return index + _offset_w_bigram_u;
}
int index_w_bigram_b(int y_i_1, int y_i, int i, int x_i_1, int x_i){
	int index = x_i * _num_character_ids * _x_range_bigram * 2 * 2 + x_i_1 * _x_range_bigram * 2 * 2 + (i - 1) * 2 * 2 + y_i * 2 + y_i_1;
	return index + _offset_w_bigram_b;
}
int index_w_identical_1_u(int y_i, int i){
	int index = (i - 1) * 2 + y_i;
	return index + _offset_w_identical_1_u;
}
int index_w_identical_1_b(int y_i_1, int y_i, int i){
	int index = (i - 1) * 2 * 2 + y_i * 2 + y_i_1;
	return index + _offset_w_identical_1_b;
}
int index_w_identical_2_u(int y_i, int i){
	int index = (i - 1) * 2 + y_i;
	return index + _offset_w_identical_2_u;
}
int index_w_identical_2_b(int y_i_1, int y_i, int i){
	int index = (i - 1) * 2 * 2 + y_i * 2 + y_i_1;
	return index + _offset_w_identical_2_b;
}
int index_w_unigram_type_u(int y_i, int type_i){
	int index = type_i * 2 + y_i;
	return index + _offset_w_unigram_type_u;
}
int index_w_unigram_type_b(int y_i_1, int y_i, int type_i){
	int index = type_i * 2 * 2 + y_i * 2 + y_i_1;
	return index + _offset_w_unigram_type_b;
}
int index_w_bigram_type_u(int y_i, int type_i_1, int type_i){
	int index = type_i * _num_character_types * 2 + type_i_1 * 2 + y_i;
	return index + _offset_w_bigram_type_u;
}
int index_w_bigram_type_b(int y_i_1, int y_i, int type_i_1, int type_i){
	int index = type_i * _num_character_types * 2 * 2 + type_i_1 * 2 * 2 + y_i * 2 + y_i_1;
	return index + _offset_w_bigram_type_b;
}
```

同じIDを違う素性関数が返さないようにオフセットを与えています。

こうすることで$\boldsymbol \Lambda$を1つの巨大なベクトルとして実装でき、素性ID$k$から$\lambda_k$を引けるようになっています。

実際、学習時に$\boldsymbol \Lambda$は数億次元の巨大なベクトルになるため、数GBのメモリを消費します。

学習完了後の推論時には不要な重みを枝刈りし圧縮することで数十MBまで小さくなります。

## 周辺確率の計算

オリジナル版のNPYCRFではセミマルコフモデルのノード間の遷移確率をCRFのパスのコストに変換してCRFのパラメータを更新しています。

しかし、CRFのパラメータの勾配計算には、各素性の文字列上の各位置での発火の期待値さえ求めれば充分であるため、NPYLMの情報をマルコフモデルに変換せずに直接CRFの勾配を計算することができます。

素性関数$\pi_k$に対応する重み$\lambda_k$の、対数尤度$\cal{L}(\boldsymbol x, \boldsymbol y)$に対する勾配は以下のように求められます。

$$
	\begin{align}
		\frac{\partial \cal{L}(\boldsymbol x, \boldsymbol y)}{\partial \lambda_k} = 
			\sum_{t=1}^N \left\{ 
					\pi_k(\boldsymbol x, y_t, y_{t+1}, t + 1) - \sum_{i \in \{0,1\}}\sum_{j \in \{0,1\}}P(y_t=i, y_{t+1}=j \mid \boldsymbol x) \pi_k(\boldsymbol x, i, j, t + 1)
				\right\}
	\end{align}\
$$

$P(y_t=i, y_{t+1}=j \mid \boldsymbol x)$は、与えられた$\boldsymbol x$に対して現在のモデルでラベルを予測した時、位置$t$のラベルが$i$で位置$t+1$のラベルが$j$になる確率（周辺確率）を表しています。

そのため、各$m, i, j$について$P(y_t=i, y_{t+1}=j \mid \boldsymbol x)$が計算できれば$\lambda_0$の勾配を計算することができます。

### 単語の周辺確率

$P(y_t=i, y_{t+1}=j \mid \boldsymbol x)$を求めるには、まず単語の周辺確率を求める必要があります。

まず、$c_{t-k-j+1}^{t-k}$から$c_{t-k+1}^t$への遷移を考えると、この周辺確率は以下のように表されます。

$$
	\begin{align}
		P(c_{t-k-j+1}^{t-k},  c_{t-k+1}^t \mid \boldsymbol x) = 
		\frac {
			\alpha[t-k][j] \cdot \beta[t][k] \cdot \text{exp}\left[ \lambda_0 \text{log}\left(P(c_{t-k-j+1}^{t-k} \mid c_{t-k+1}^t)\right) + \gamma(t-k+1, t+1)\right]
		}{
			Z(x)^*
		}
	\end{align}\
$$

$\beta[t][k]$は以下で表される後向き確率です。

$$
	\begin{align}
		\beta[t][k] = \sum_{j=1}^{N-t} \text{exp}\left[
				\lambda_0 \text{log}\left(P(c_{t+1}^{t+j} \mid c_{t-k+1}^t)\right) + \gamma(t+1, t+j+1)
			\right] \cdot \beta[t+j][j]
	\end{align}\
$$

式(13)から単語の周辺確率を求めます。

$$
	\begin{align}
		P_{\text{CONC}}(c_{t-k+1}^t \mid \boldsymbol x) &= \sum_{j} P(c_{t-k-j+1}^{t-k},  c_{t-k+1}^t \mid \boldsymbol x)  \nonumber \\
		&= \sum_{j} \frac {
				\alpha[t-k][j] \cdot \beta[t][k] \cdot \text{exp}\left[ \lambda_0 \text{log}\left(P(c_{t-k-j+1}^{t-k} \mid c_{t-k+1}^t)\right) + \gamma(t-k+1, t+1)\right]
			}{
				Z(x)^*
			} \nonumber \\
		&= \frac{\beta[t][k]}{Z(x)^*} \sum_{j} \alpha[t-k][j] \cdot \beta[t][k] \cdot \text{exp}\left[ \lambda_0 \text{log}\left(P(c_{t-k-j+1}^{t-k} \mid c_{t-k+1}^t)\right) + \gamma(t-k+1, t+1)\right] \nonumber \\
		&= \frac{\alpha[t][k] \cdot \beta[t][k]}{Z(x)^*} 
	\end{align}\
$$

この式(15)を用いて$P(y_t=i, y_{t+1}=j \mid \boldsymbol x)$の各ラベル$i,j$の組み合わせ（`1-1`、`1-0`、`0-1`、`0-0`の4通り）について周辺確率を計算します。

### `1-1`の場合

$y_t=1, y_{t+1}=1$となるケースは$c_t^t$が単語になるため、周辺確率は以下のように求めます。

$$
	\begin{align}
		P(y_t=1, y_{t+1}=1 \mid \boldsymbol x) = P_{\text{CONC}}(c_t^t \mid \boldsymbol x)
	\end{align}\
$$

### `1-0`の場合

$y_{t}=1, y_{t+1}=0$となるケースは、位置$t$から任意の長さの単語が始まることを意味しているため、周辺確率は以下のように求めます。

$$
	\begin{align}
		P(y_t=1, y_{t+1}=0 \mid \boldsymbol x) 
		&= \sum_{j=2} P(y_t=1, y_{t+1}=0, ..., y_{t+j}=1 \mid \boldsymbol x) \nonumber \\
		&= \sum_{j=1} P_{\text{CONC}}(c_t^{t+j} \mid \boldsymbol x)
	\end{align}\
$$

ただし$...$の範囲のラベルの値はすべて$0$です。

### `0-1`の場合

$y_{t}=0, y_{t+1}=1$となるケースは、任意の長さの単語が位置$t$で終了し、 位置$t+1$から新しい単語が始まることを意味しているため、周辺確率は以下のように求めます。

$$
	\begin{align}
		P(y_t=0, y_{t+1}=1 \mid \boldsymbol x) 
		&= \sum_{j=1} P(y_{t-j}=1, ...,  y_{t}=0, y_{t+1}=1 \mid \boldsymbol x) \nonumber \\
		&= \sum_{j=1} P_{\text{CONC}}(c_{t-j}^{t} \mid \boldsymbol x)
	\end{align}\
$$

### `0-0`の場合

$y_{t}=0, y_{t+1}=0$となるケースは、位置$t$と位置$t+1$がともに単語の途中であることを意味しているため、周辺確率は以下のように求めます。

$$
	\begin{align}
		P(y_t=0, y_{t+1}=0 \mid \boldsymbol x) 
		&= \sum_{j=1}\sum_{k=2} P(y_{t-j}=1, ...,  y_{t}=0, y_{t+1}=0, ..., y_{t+k} \mid \boldsymbol x) \nonumber \\
		&= \sum_{j=1}\sum_{k=1} P_{\text{CONC}}(c_{t-j}^{t+k} \mid \boldsymbol x)
	\end{align}\
$$

ただし、$P(y_t=i, y_{t+1}=j \mid \boldsymbol x)$の各ラベル$i,j$の組み合わせは4通りしか存在しないため、`1-1`、`1-0`、`0-1`のケースを求めれば`0-0`のケースは以下のように求めることができます。

$$
	\begin{align}
		P(y_t=0, y_{t+1}=0 \mid \boldsymbol x) = 1 - P(y_t=1, y_{t+1}=1 \mid \boldsymbol x) - P(y_t=1, y_{t+1}=0 \mid \boldsymbol x) - P(y_t=0, y_{t+1}=1 \mid \boldsymbol x)
	\end{align}\
$$

### trigramの場合

ここまではセミマルコフモデルのノードの遷移確率を単語bigram確率で考えてきましたが、実際は精度面からNPYLMをtrigramにすることが多いです。



### 実装

```cpp
void enumerate_marginal_p_path_given_sentence_using_p_substring(py_x, sentence_length, pc_x){
	py_x(0, 0, 0) = 0;
	py_x(0, 0, 1) = 0;
	py_x(0, 1, 0) = 0;
	py_x(0, 1, 1) = 1;
	for(t = 1;t <= sentence_length;t++){
		py_x(t, 1, 1) = compute_p_z_case_1_1(sentence_length, t, pc_x);
		py_x(t, 1, 0) = compute_p_z_case_1_0(sentence_length, t, pc_x);
		py_x(t, 0, 1) = compute_p_z_case_0_1(sentence_length, t, pc_x);
		py_x(t, 0, 0) = std::max(0.0, 1.0 - py_x(t, 1, 1) - py_x(t, 1, 0) - py_x(t, 0, 1));
	}
}
double compute_p_z_case_1_1(sentence_length, t, pc_x){
	return pc_x(t, 1);
}
double compute_p_z_case_1_0(sentence_length, t, pc_x){
	if(t == sentence_length){
		return 0;
	}
	double p_1_0 = 0;
	for(int j = 2;j <= std::min(sentence_length - t + 1, _max_word_length);j++){
		p_1_0 += pc_x(t + j - 1, j);
	}
	return p_1_0;
}
double compute_p_z_case_0_1(sentence_length, t, pc_x){
	double p_0_1 = 0;
	for(int j = 2;j <= std::min(t, _max_word_length);j++){
		p_0_1 += pc_x(t, j);
	}
	return p_0_1;
}
double compute_p_z_case_0_0(sentence_length, t, pc_x){
	if(t == 1){
		return 0;
	}
	double p_0_0 = 0;
	for(int k = 1;k <= std::min(sentence_length - t, _max_word_length - 2);k++){
		for(int j = k + 2;j <= std::min(t + k, _max_word_length);j++){
			p_0_0 += pc_x(t + k, j);
		}
	}
	return p_0_0;
}
```

実装時には`0-0`のケースも式(19)に従って計算し、式(20)の結果と一致するかどうかを確かめましょう。

## $\lambda_0$の勾配計算

$(\lambda_1,...,\lambda_K)$はCRFの素性に対する重みなので上述の方法で勾配を計算できますが、$\lambda_0$は単語bigram（またはtrigram）に対する素性であるため、勾配の計算方法が特殊です。

基本的には式(12)と同様、素性関数の値から発火の期待値を引くと勾配を計算できるため、以下のように求めます。

$$
	\begin{align}
		\frac{\partial \cal{L}(\boldsymbol x, \boldsymbol y)}{\partial \lambda_0} = 
			\sum_{\{s, t, u\} \in \boldsymbol y} \left\{ 
					\text{log}\left( P(c_t^{u-1} \mid c_s^{t-1}) \right)
				\right\}
			- \sum_{t=1}^N \sum_{k=1}^{t} \sum_{j=1}^{t-k} P_{\text{CONC}}(c_{t-k+1}^{t}, c_{t-k-j+1}^{t-k} \mid \boldsymbol x)
				\text{log}\left( P(c_{t-k+1}^{t} \mid c_{t-k-j+1}^{t-k}) \right)
	\end{align}\
$$

コードは以下のようになります。

```cpp
void backward_lambda_0(p_conc, pw_h, max_word_length){
	// 素性の発火
	for(int n = 1;n <= num_words;n++){
		int t, k, j = ...
		_grad_lambda_0 += log(pw_h(t, k, j));
	}

	// 発火の期待値を引く
	for(int t = 1;t <= num_characters;t++){
		for(int k = 1;k <= min(t, max_word_length);k++){
			for(int j = (t - k == 0) ? 0 : 1;j <= min(t - k, max_word_length);j++){
				_grad_lambda_0 -= p_conc(t, k, j) * log(pw_h(t, k, j));
			}
		}
	}
}
```

trigramの場合は単純に発火の期待値の部分のループがもう一つ増えるだけです。

## スケーリング係数

前向き確率と後ろ向き確率は、実際には長い系列でアンダーフローを起こす問題があるため、式のまま計算することはありません。

（私の経験上、$\boldsymbol x$が100文字を超えると危なく、200文字を超えると確実にアンダーフローを起こします）

一般にマルコフモデルの場合はスケーリング係数を用いて各時刻の前向き確率・後ろ向き確率をスケーリングすることでアンダーフローに対処します。

今回はセミマルコフモデルなのですが、マルコフモデルと同様にスケーリング係数を求めることができます。

これについては以前に[解説記事](/2017/12/10/semi-markov-model/)を書いているので詳細をここでは書きませんが、スケーリング係数を以下のように定義します。

$$
	\begin{align}
		c[t] = P(x_t \mid x_1x_2 \cdots x_{t-1})
	\end{align}\
$$

スケーリングされた前向き確率$\bar{\alpha}[t-k][j]$から$\hat{\alpha}[t][k]$を計算し、

$$
	\begin{align}
		\hat{\alpha}[t][k] = \sum_{j=1}^{t-k} \pi(q_t = k \mid q_{t-k}=j)\bar{\alpha}[t-k][j]\frac {1} {\prod_{m=t-k+1}^{t-1} c[m]}
	\end{align}\
$$

位置$t$のスケーリング係数を求め、

$$
	\begin{align}
		c[t] = \sum_k \hat{\alpha}[t][k]
	\end{align}\
$$

位置$t$の前向き確率をスケーリングします。

$$
	\begin{align}
		\bar{\alpha}[t][k] = \frac {\hat{\alpha}[t][k]} {c[t]}
	\end{align}\
$$

後向き確率は前向き確率の計算時に求めたスケーリング係数をそのまま用いて以下のように計算することができます。

$$
	\begin{align}
		\bar{\beta}[t][k] = \sum_{j=1}^{n-t} \pi(q_{t+j} = j \mid q_t=k) \cdot \bar{\beta}[t+j][j] \cdot 
			\frac {1} {\prod_{m=t+1}^{t+j} c[m]}
	\end{align}\
$$

さらに、スケーリング係数は単語分割で周辺化されているため、規格化定数を計算することができます。

$$
	\begin{align}
		Z(\boldsymbol x)^* = \prod_{t=1}^N c[t]
	\end{align}\
$$

このスケーリング係数を用いると、CRFの周辺確率の計算は以下のように書き直すことができます。

## おわりに

時間が十分に取れなかったのでモデルの振る舞いの解析などがまだできていません。

NPYLMでは分割できる未知語がNPYCRFでは分割できないこともあり、そもそも統合モデル上のNPYLMが学習に成功しているのかどうかなどを今後調べていきたいと思います。

<blockquote class="twitter-tweet" data-lang="ja"><p lang="ja" dir="ltr">NPYCRFを実装しようと思って持橋先生のスライドを読んでいたら、本来難しい「リリカルなのは」の形態素解析も教師なしで適切な単語分割が学習できるとのことなので、実装したらこのツイートの単語分割をやってみたい。<a href="https://t.co/tVTEnCHGbt">https://t.co/tVTEnCHGbt</a> <a href="https://t.co/wJFlqSkAp3">https://t.co/wJFlqSkAp3</a></p>&mdash; 💥 (@musyokudon) <a href="https://twitter.com/musyokudon/status/867416782436315136?ref_src=twsrc%5Etfw">2017年5月24日</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet" data-lang="ja"><p lang="ja" dir="ltr">形態素解析が難しいのにはなのはのほかにはがないとかがをられとかがありこれらははにはにやけよりなからの伝統だそうです。</p>&mdash; asaokitan (@asaokitan) <a href="https://twitter.com/asaokitan/status/502100435184787458?ref_src=twsrc%5Etfw">2014年8月20日</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet" data-conversation="none" data-lang="ja"><p lang="ja" dir="ltr">たしかに教師なし（NPYLM）だとうまくいった。教師付きデータ（wikipediaコーパス）も使うとそれに引っ張られるのかうまくいかない（NPYCRF）。このあたりのバランスはチューニングでどうにかなる問題なのだろうか。 <a href="https://t.co/isYwPDKvtB">pic.twitter.com/isYwPDKvtB</a></p>&mdash; 💥 (@musyokudon) <a href="https://twitter.com/musyokudon/status/940171775261908992?ref_src=twsrc%5Etfw">2017年12月11日</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet" data-lang="ja"><p lang="ja" dir="ltr">NPYCRFでは学習した補完重みが一定なので、必ずしも上手く行かない場合もあるはずです。この場合何をすればよいかはNPYCRFの論文の最後に書いてある通りで明確なのですが、私は他にやることが多いので、誰かできる人を待っている感じでしょうか。</p>&mdash; Daichi Mochihashi (@daiti_m) <a href="https://twitter.com/daiti_m/status/940656603216347137?ref_src=twsrc%5Etfw">2017年12月12日</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


余談ですがMeCab開発者の工藤氏による「形態素解析・構文解析の理論と実装」という本が発売されるそうです。

楽しみですね。

<blockquote class="twitter-tweet" data-lang="ja"><p lang="und" dir="ltr"><a href="https://t.co/DxvPbp8LhA">pic.twitter.com/DxvPbp8LhA</a></p>&mdash; Manabu Sassano (@sassano) <a href="https://twitter.com/sassano/status/940586222166011904?ref_src=twsrc%5Etfw">2017年12月12日</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet" data-lang="ja"><p lang="ja" dir="ltr">ようやく一冊まるまる形態素解析という狂気じみた本の初稿ができた。これからレビューです。読んでフィードバックくださる方DMください。ご飯おごります。</p>&mdash; Taku Kudo (@taku910) <a href="https://twitter.com/taku910/status/940116830957346816?ref_src=twsrc%5Etfw">2017年12月11日</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
