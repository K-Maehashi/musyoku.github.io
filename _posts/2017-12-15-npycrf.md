---
layout: post
title:  条件付確率場とベイズ階層言語モデルの統合による半教師あり形態素解析（NPYCRF）
category: 実装
tags:
- NPYCRF
excerpt_separator: <!--more-->
---

この記事は[自然言語処理 Advent Calendar 2017](https://qiita.com/advent-calendar/2017/nlp)の19日目の記事です。

<!--more-->

## はじめに

今回は半教師あり形態素解析の手法であるNPYCRFについて、その理論と実装方法をまとめました。

一般に形態素解析はCRFによる識別モデルが用いられ、CRFのパラメータは教師付きデータにより最適化されます。

しかし、古くはしょこたんブログに始まり、最近ではTwitterなどのソーシャルメディアで日々新しい単語が生み出されているため、こうしたテキストデータを適切に解析できるモデルが求められます。


（余談ですが最近は単語を省略するのが流行っているらしく、例えば「バイト」を「バ」と言います。例）「バイト終わった」→「バおわ」）


NPYCRFは、教師なしデータの単語分割のための言語モデルであるNPYLMと、形態素解析のための識別モデルであるCRFを統合し、教師付きデータの分割基準を守りながら教師なしデータの単語分割を学習できるようになっています。

NPYLMに加えてCRFとL-BFGSを実装しなければならないため、実装難易度は当ブログ史上最高となっています。

最初から実装する場合は半年、NPYLMをベースに実装する場合は2ヶ月程度かかると思います。

また、通常こういった論文はページ数の制約があるため、本当にすべきことの1割程度しか論文に書かれません。

この記事では私の解釈に基づいて残りの9割の実装方法を説明するため、誤りを含む可能性があります。ご了承ください。

## 参考文献

実装するにあたり必要な文献です。

- [半教師あり形態素解析NPYCRFの修正](http://www.anlp.jp/proceedings/annual_meeting/2016/pdf_dir/D6-3.pdf)
	- オリジナル版のNPYCRFにはモデル統合部分に誤りがあります。
- [Nonparametric Bayesian Semi-supervised Word Segmentation](http://chasen.org/~daiti-m/paper/tacl2016semiseg.pdf)
	- 修正版の国際学会版です。より詳しくなっています。
- [条件付き確率場の理論と実践](http://www.ism.ac.jp/editsec/toukei/pdf/64-2-179.pdf)
	- CRFの理論や勾配計算について非常に詳しくまとまっています。
- [ベイズ階層言語モデルによる教師なし形態素解析](http://chasen.org/~daiti-m/paper/nl190segment.pdf)
	- NPYLMの論文です。
	- 去年[解説記事](/2016/12/14/%E3%83%99%E3%82%A4%E3%82%BA%E9%9A%8E%E5%B1%A4%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E3%82%88%E3%82%8B%E6%95%99%E5%B8%AB%E3%81%AA%E3%81%97%E5%BD%A2%E6%85%8B%E7%B4%A0%E8%A7%A3%E6%9E%90/)を書きました。

## 実装

コア実装をC++14で行い、BoostでPythonから利用できるようにしています。

[https://github.com/musyoku/python-npycrf](https://github.com/musyoku/python-npycrf)

まだ細かい部分が未完成ですが学習と単語分割はできます。

またNPYLMは特許が取られているため、この実装は研究以外の用途に使用しないでください。

<blockquote class="twitter-tweet" data-lang="ja"><p lang="ja" dir="ltr">正しい連絡先が分からないのでここで：「ご注文は機械学習ですか?」で特許のため公開できないとされているNPYLMのGitHub上の実装は、NTT知財に確認してもらったところ「試験または研究のための使用」であれば公開してもよいそうです。他の方の実装も、基本的に同じだと思います。</p>&mdash; Daichi Mochihashi (@daiti_m) <a href="https://twitter.com/daiti_m/status/851810748263157760?ref_src=twsrc%5Etfw">2017年4月11日</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


## NPYLMとCRF

NPYLMはセミマルコフモデル、CRFはマルコフモデルになっており、単語分割は以下のように表されます。

NPYLMは各ノードが単語を表しています。

CRFは1（＝語頭）と0（＝それ以外）の2つラベルを持ちます。

## 識別-生成統合モデル

NPYCRFではJESS-CM法を用いて、入力文$\boldsymbol x = x_1x_2 \cdots x_n$に対するラベル列$\boldsymbol y$の確率を以下のように表現します。

$$
	\begin{align}
		P_{\text{CONC}}(\boldsymbol y \mid \boldsymbol x) \propto P_{\text{CRF}}(\boldsymbol y \mid \boldsymbol x;\boldsymbol \Lambda)P_{\text{NPYLM}}(\boldsymbol y, \boldsymbol x;\boldsymbol \Theta)^{\lambda_0}
	\end{align}\
$$

$P_{\text{CRF}}$はCRF（識別モデル）、$P_{\text{NPYLM}}$はNPYLM（生成モデル)であり、$\boldsymbol \Lambda=(\lambda_1, \lambda_2, ..., \lambda_K)$はCRFのパラメータ（重み）、$\boldsymbol \Theta$はNPYLMのパラメータ（中華料理店過程の客の配置）を表しています。

$\propto$は比例を意味しているので式(1)は確率ではありませんが、$\boldsymbol x$に対する可能な全ての分割$\boldsymbol y$で式(1)を計算した総和$Z(x)^*$で割れば確率になるため、式(1)を等号で表すことができます。

ここで$\text{log}P_{\text{NPYLM}}(\boldsymbol y, \boldsymbol x;\boldsymbol \Theta)$を連続値を返す素性関数とみなせば、式(1)はパラメータ$\boldsymbol \Lambda^*=\lambda_0, \lambda_1, ..., \lambda_K$を持つ対数線形モデルの形で書くことができます。

$$
	\begin{align}
		P_{\text{CONC}}(\boldsymbol y \mid \boldsymbol x) &\propto \text{exp}
			\left(
					\lambda_0 \text{log}P_{\text{NPYLM}}(\boldsymbol y, \boldsymbol x;\boldsymbol \Theta)
					+
					\sum_{k=1}^K \lambda_k f_k(\boldsymbol y, \boldsymbol x)
			\right)\\
		&= \text{exp}(\boldsymbol \Lambda^* F^*(\boldsymbol y, \boldsymbol x))
	\end{align}\
$$

$f_k$はCRFの$k$番目の素性関数です。

## モデル変換

JESS-CMによるモデル統合では、識別モデルと生成モデルが同じ構造をしている必要があります。

しかしNPYCRFでは識別モデルがマルコフモデルであるのに対し、生成モデルがセミマルコフモデルになっているため、各モデルの情報を直接統合することができないので、何らかの変換処理を行う必要があります。

この変換処理はオリジナル版のNPYCRFに誤りがあり、修正版で正しい統合方法が提案されています。

## オリジナル版のモデル変換

ここからはNPYLMが単語bigramモデルであるとします。

文$\boldsymbol x$の$s$番目の文字$x_s$から$t$番目の文字$x_t$までの部分文字列を$c_s^t$で表します。

この時、マルコフモデル上の$c_s^t$に対応する区間$[s, t)$のポテンシャルを$\gamma(s, t)$とおくと、これは状態1で始まり状態1で終わるV字型の区間になります。

セミマルコフモデルの各ノードの遷移確率$P(c_t^{u-1} \mid c_s^{t-1})$に対応するポテンシャルとして、オリジナル版のNPYCRFでは

$$
	\begin{align}
		P(c_t^{u-1} \mid c_s^{t-1}) &\propto \text{exp}\left(\gamma(s, t) + \gamma(t, u)\right)
	\end{align}\
$$

を用いてNPYLMの前向き確率を

$$
	\begin{align}
		\alpha[t][k] = \sum_{j=1}^{t-k} \text{exp} 
			\left\{ 
				\lambda_0 \text{log}\left( P(c_{t-k+1}^t \mid c_{t-k-j+1}^{t-k}) \right) 
				+ \gamma(t-k-j+1, t-k+1) + \gamma(t-k+1, t+1)
			\right\} \cdot \alpha[t-k][j]
	\end{align}\
$$

としています。

$\alpha[t][k]$は、位置$t$から左の最も近い単語境界までの距離が$k$文字である確率を表しており、それ以前の分割全てについて周辺化された確率になっています。

マルコフモデルの前向き確率のようなものだと考えてください。

式(5)の$\text{exp}(\cdot)$が、CRFの情報を統合したNPYLMのセミマルコフモデル上での遷移確率になります。

オリジナル版のNPYCRFの誤りを分かりやすくするため、ここでは可能な単語の最大長を1として、長さ3文字の文$\boldsymbol x = x_1x_2x_3$の前向き確率を求めてみます。

（文頭に\<bos\>、文末に\<eos\>を挿入しています）

単語の最大長が1なので周辺化の処理がいらなくなり、以下のように前向き確率を求められます。

$$
	\begin{align}
		\alpha[1][1] &= \text{exp} \left\{ \lambda_0 \text{log}\left( P(c_1^1 \mid \text{<bos>}) \right) 
						+ \gamma(0, 1) + \gamma(1, 2) \right\}  \nonumber \\
		\alpha[2][1] &= \text{exp} \left\{ \lambda_0 \text{log}\left( P(c_2^2 \mid c_1^1) \right) 
						+ \gamma(1, 2) + \gamma(2, 3) \right\} \cdot  \alpha[1][1] \nonumber \\
		\alpha[3][1] &= \text{exp} \left\{ \lambda_0 \text{log}\left( P(c_3^3 \mid c_2^2) \right) 
						+ \gamma(2, 3) + \gamma(3, 4) \right\} \cdot  \alpha[2][1] \nonumber \\
		\alpha[4][1] &= \text{exp} \left\{ \lambda_0 \text{log}\left( P(<eos> \mid c_3^3) \right) 
						+ \gamma(3, 4) \right\} \cdot  \alpha[3][1]\\
	\end{align}\
$$

前向き確率の一番最後の値$\alpha[4][1]$は規格化定数$Z(x)^*$になります。

またこのケースでは可能な分割が1通りしか存在しないため、

$$
	\begin{align}
		P_{\text{CONC}}(\boldsymbol y \mid \boldsymbol x) &= P_{\text{CRF}}(\boldsymbol y \mid \boldsymbol x;\boldsymbol \Lambda)P_{\text{NPYLM}}(\boldsymbol y, \boldsymbol x;\boldsymbol \Theta)^{\lambda_0} \nonumber \\
		&= Z(x)^* \nonumber \\
		&= \alpha[4][1]
	\end{align}\
$$

となります。

式(6)と式(7)から以下のようになります。

$$
	\begin{align}
		P_{\text{CONC}}(\boldsymbol y \mid \boldsymbol x) &= \alpha[4][1] \nonumber \\
		&= \text{exp} \left\{ \lambda_0 \text{log}\left( P(<eos> \mid c_3^3) \right) 
						+ \gamma(3, 4) \right\} \cdot  \alpha[3][1] \nonumber \\
		&= \text{exp} \left\{ \lambda_0 \text{log}\left( P(<eos> \mid c_3^3) \right) 
						+ \gamma(3, 4) + \lambda_0 \text{log}\left( P(c_3^3 \mid c_2^2) \right) 
						+ \gamma(2, 3) + \gamma(3, 4) \right\} \cdot  \alpha[2][1] \nonumber \\
		&= \text{exp} \left\{ \lambda_0 \text{log}\left( P(<eos> \mid c_3^3) \right) 
						+ \gamma(3, 4) + \lambda_0 \text{log}\left( P(c_3^3 \mid c_2^2) \right) 
						+ \gamma(2, 3) + \gamma(3, 4) + \lambda_0 \text{log}\left( P(c_2^2 \mid c_1^1) \right) 
						+ \gamma(1, 2) + \gamma(2, 1) + \lambda_0 \text{log}\left( P(c_1^1 \mid \text{<bos>}) \right) 
						+ \gamma(0, 1) + \gamma(1, 2) \right\} \nonumber \\
		&= P(<eos> \mid c_3^3)^{\lambda_0}P(c_3^3 \mid c_2^2)^{\lambda_0}P(c_2^2 \mid c_1^1)^{\lambda_0}P(c_1^1 \mid \text{<bos>})^{\lambda_0} \nonumber \\ 
		&+ \text{exp}(2\gamma(3, 4))
		+ \text{exp}(2\gamma(2, 3))
		+ \text{exp}(2\gamma(1, 2))
		+ \text{exp}(\gamma(0, 1))
	\end{align}\
$$

一方、このケースで式(1)を展開すると以下のようになります。

$$
	\begin{align}
		P_{\text{CONC}}(\boldsymbol y \mid \boldsymbol x) &＝ P_{\text{CRF}}(\boldsymbol y \mid \boldsymbol x;\boldsymbol \Lambda)P_{\text{NPYLM}}(\boldsymbol y, \boldsymbol x;\boldsymbol \Theta)^{\lambda_0} \nonumber \\
		&＝ P(<eos> \mid c_3^3)^{\lambda_0}P(c_3^3 \mid c_2^2)^{\lambda_0}P(c_2^2 \mid c_1^1)^{\lambda_0}P(c_1^1 \mid \text{<bos>})^{\lambda_0} \nonumber\\ 
		&+ \text{exp}(\gamma(3, 4))
		+ \text{exp}(\gamma(2, 3))
		+ \text{exp}(\gamma(1, 2))
		+ \text{exp}(\gamma(0, 1))
	\end{align}\
$$

式(8)と式(9)をよく見比べると、式(8)では$\gamma$が2回出現しているため、前向き確率の計算時にモデルの統合が正しく行われていないことがわかります。

論文の以下の文はこのことを言っています。

>Markovモデルおよびsemi-Markovモデルそれぞれで特定のパス上のポテンシャルをそれぞれ一度ずつ足しあわせた形になっているはずであるが，(5) 式のように足しあわせて行くと，それぞれ γ(s, t) が二回ずつ現れることが分かる．この分解を見てもモデルの統合が正しくないことが直感的に理解できる．
