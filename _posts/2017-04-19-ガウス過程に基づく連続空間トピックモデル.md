---
layout: post
title:  ガウス過程に基づく連続空間トピックモデル
category: 実装
tags:
- 自然言語処理
excerpt_separator: <!--more-->
---

## 概要

- [ガウス過程に基づく連続空間トピックモデル](http://chasen.org/~daiti-m/paper/nl213cstm.pdf)を読んだ
- C++で実装した
- Doc2Vecとの比較など

## はじめに

ガウス過程に基づく連続空間トピックモデル（以下CSTM）は、ガウス過程を用いて単語や文書の潜在座標（分散表現）を学習するトピックモデルです。

実装は[https://github.com/musyoku/cstm](https://github.com/musyoku/cstm)です。

gensimのDoc2Vecと比較し、その性質を調べました。

ちなみにガウス過程を知らなくても大丈夫です。

私はガウス過程やカーネル法、トピックモデルは初見なのですが普通に実装できました。

実装難易度は低めです。

## CSTMの考え方

CSTMでは、各単語$w$が$d$次元の潜在座標$\phi(w) \sim {\cal N}(\boldsymbol 0, I_d)$を持っていると仮定します。（$I_d$は単位行列）

つまり各単語は$d$次元のベクトルで表され、そのベクトルのそれぞれの要素は平均0、分散1の正規分布に従います。

次に、カーネル行列$K$、平均$0$のガウス過程$f$を文書ごとに考え、

$$
  \begin{align}
    f_d = {\rm GP}(0, K)
  \end{align}\
$$

文書$d$における単語の確率を以下のようにモデル化します。

$$
  \begin{align}
    p_d(w) \propto e^{f_d(w_k)}G_0(w_k)
  \end{align}\
$$

この$f$は実際には語彙次元のガウス分布なので、単語$w$ごとに$f(w)$は違う値を返しますが、その値の平均は0になります。

論文に書かれていますが、$f(w)$はおおよそ$-9 < f(w) < 9$の値をとるそうです。

このような性質の$f(w)$を用いて文書ごとに$w$の確率をモデル化したものが式(2)になっています。

$f(w)$は$-9 < f(w) < 9$の値をとるため、$e^{f(w)}$は$e^{-9} < e^{f(w)} < e^9$の範囲の値になります。

数値で書くとこれはだいたい$0.0001234 < e^{f(w)} < 8103$くらいになります。

この$e^{f_d(w_k)}$を文書ごとの倍率と考え、これをデフォルト確率$G_0(w_k)$に掛けることで文書ごとの$w$の確率が決まります。

$G_0(w_k)$は文書全体での$w$の出現確率の最尤推定値です。

このようにモデル化することで、文書全体ではほとんど出現しないが、特定の文書にだけ高頻度で出現するような単語であっても、式(2)を用いることで文書ごとに確率を変動させることで適切な確率を与えることができます。

またカーネル行列として線形カーネルを用います。

$$
  \begin{align}
    K(w_i, w_j) = \phi(w_i)^T\phi(w_j)
  \end{align}\
$$

ただしここまで説明したことは実際のCSTMでは用いません。

あくまで基本となる考え方です。

## CSTM

このような$f$を直接求めるのは難しいらしく、CSTMでは補助変数を導入した手法を用います。

まず文書$d$の潜在座標を${\boldsymbol u_d} \sim {\cal N}(\boldsymbol 0, I_d)$とし、全ての単語の$\phi(w)$をまとめて$\Phi = (\phi(w_1), \phi(w_2), ..., \phi(w_V))^T$とおきます。

$f_d=\Phi {\boldsymbol u_d}$として、この$f_d$の分布がどうなっているかを考えるのですが、$u$を積分消去すると

$$
  \begin{align}
      f_d \sim N(0, \Phi^T\Phi) = N(0, K)
  \end{align}\
$$

となるため、この$f$は式(1)と同じガウス過程に従います。

（式(3)より$K=\Phi^T\Phi$です。式(4)の導出のやり方がわからないので論文の式をそのまま載せています）

このように補助変数（文書ベクトル）を用いることで、本来考えていた式(1)と同じガウス過程に従う$f_d$が作れるようになります。

次に単語の確率ですが、式(2)を直接用いると文書のバースト性をうまくモデル化できないため、CSTMではDirichlet Compound Multinomial（DCM）を用います。

DCMはパラメータ$\boldsymbol \alpha_d$、文書$d$での単語の出現頻度$\boldsymbol n_d$のもとでの語彙$\boldsymbol w$の確率を以下のように表します。

$$
  \begin{align}
      p_d(\boldsymbol w \mid \boldsymbol \alpha_d, \boldsymbol n_d) = 
      \frac{\Gamma(\sum_k \alpha_{d, k})}{\Gamma(\sum_k \alpha_{d, k} + n_{d, k})}
      \prod_k \frac{\Gamma(\alpha_{d, k} + n_{d, k})}{\Gamma(\alpha_{d, k})}
  \end{align}\
$$

語彙数を$V$とすると、$\boldsymbol \alpha_d = (\alpha_{d,1}, \alpha_{d,2}, ... \alpha_{d, V})$であり、$\alpha_{d, k}$はおそらく0以上の実数です。

$\boldsymbol n_d = (n_{d,1}, n_{d,2}, ... n_{d, V})$は文書$d$での各単語の出現頻度です。

$\boldsymbol w = (w_1, w_2, ... w_V)$は語彙集合です。

式(5)は文書$d$における語彙全体の確率を表しているため、その文書に含まれていない単語の確率も考えていることに注意が必要です。

式(5)に含まれる単語の出現頻度の値については、文書ごとの出現頻度である$\boldsymbol n_d$を使うべきだと思うのですが、すべての文書を合わせた値$\boldsymbol n$を使うべきなのかどうかがよくわからないです。

次に$f_d(w_k)$を用いて$\alpha_{d,k}$を以下のように表します。

$$
  \begin{align}
    \alpha_{d, k} = \alpha_0G_0(w_k)e^{f_d(w_k)} = \alpha_0G_0(w_k)e^{\phi(w_k)^T{\boldsymbol u_d}}
  \end{align}\
$$

式(2)が単語の個別の確率を$f_d(w_k)$で文書ごとに変更しているのに対し、式(6)はDCMのパラメータ$\boldsymbol \alpha_d$を文書ごとに変更しています。

このようにCSTMでは語彙全体の同時確率をモデル化し、その確率を$\boldsymbol \alpha$を通じて文書ごとに違う値に変えるような動作になっています。

$\alpha_0$は学習すべきパラメータです。

## 学習

CSTMにおける学習は、式(5)が正しい値になるように${\boldsymbol u_d}$と$\Phi$を更新することです。

式(5)を微分して更新量を計算できそうですが、論文によるとランダムウォークによるメトロポリス・ヘイスティングス法（MH法）の方が優れているそうです。


MH法では更新したい変数について提案分布から候補となる値を生成し、採択確率に従ってその値で更新するアルゴリズムです。

MH法については[マルコフ連鎖モンテカルロ法入門](http://ebsa.ism.ac.jp/ebooks/sites/default/files/ebook/1881/pdf/vol3_ch10.pdf)が詳しいです。

文書ベクトル$u_d$の提案分布は${\cal N}(u_d, \sigma^2_{(u)})$、単語ベクトル$\phi(w_k)$の提案分布は${\cal N}(w_k, \sigma^2_{(\phi)})$、$\alpha_0$の提案分布は${\cal N}(w_k, \sigma^2_{(\alpha)})$を使います。

論文によると$\sigma^2_{(u)} = 0.01,\sigma^2_{(\phi)} = 0.02, \sigma^2_{(\alpha)} = 0.2$です。

採択確率は論文に載っていませんが、「パラメータの事前分布および式 (6), (9) から得られる尤度を用いて受理を判定する」とあるのでおそらく以下の形ではないかと思います。


$$
  \begin{align}
    {\cal A}(\boldsymbol u'_d) = \min\left\{
      1,
      \frac{
        p_d(\boldsymbol w \mid \boldsymbol \alpha'_d, \boldsymbol n_d)p(\boldsymbol u'_i \mid \boldsymbol 0, I_d)
      }{
        p_d(\boldsymbol w \mid \boldsymbol \alpha_d, \boldsymbol n_d)p(\boldsymbol u_d \mid \boldsymbol 0, I_d)
      }
    \right\}
  \end{align}\
$$

${\cal A}(\boldsymbol u'_i)$が採択確率です。

$\boldsymbol \alpha'_d$は$\boldsymbol u'_i$を用いて式(6)を全ての語彙について計算したものです。

$p(\boldsymbol u'_i \mid \boldsymbol 0, I_d)$と$p(\boldsymbol u_d \mid \boldsymbol 0, I_d)$は尤度関数ですが、これは単純に標準多変量正規分布の密度関数にそのまま値を入れて計算すればOKです。

これについては[多変量正規分布](http://www012.upp.so-net.ne.jp/doi/math/anova/m_normal.pdf)が詳しいです。

また求まる値は確率ではなく尤度なので1を超えることもあります。

$p_d(\boldsymbol w \mid \boldsymbol \alpha'_d, \boldsymbol n_d)$と$p_d(\boldsymbol w \mid \boldsymbol \alpha_d, \boldsymbol n_d)$も尤度関数とみなしますが、それぞれ$\boldsymbol \alpha'_d$と$\boldsymbol \alpha_d$以外の値を固定してから式(5)をそのまま計算すればOKです。

MH法では採択確率に遷移確率の比$\frac{p(\boldsymbol u_i \mid \boldsymbol u'_i)}{p(\boldsymbol u'_i \mid \boldsymbol u_i)}$が必要ですが、今回のランダムウォークでは$p(\boldsymbol u_i \mid \boldsymbol u'_i) = p(\boldsymbol u'_i \mid \boldsymbol u_i)$なので不要です。