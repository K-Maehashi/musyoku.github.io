---
layout: post
title: WaveNet - A Generative Model for Raw Audio [arXiv:1609.03499]
category: 論文
tags:
- Chainer
- WaveNet
excerpt_separator: <!--more-->
---

## 概要

- [WaveNet: A Generative Model for Raw Audio](http://arxiv.org/abs/1609.03499) を読んだ
- Chainer 1.12で実装した

<!--more-->

## はじめに

Google DeepMindが音声生成の新たな手法を開発し[発表](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)しましたが、これが従来手法を大きく超える高い品質の音声を生成できると話題になりました。

発表から数日でGitHubには様々な実装が公開されましたが、私もChainerで実装してみました。（→[GitHub](https://github.com/musyoku/wavenet))

このWaveNetを実装するにあたり、

- 実装の詳細が論文に書いていない
- 1秒の音声を生成するのに90分かかる
- 学習コストが大きい

といった点に注意が必要です。

特に音声生成はリアルタイムで行えるような速度が出ません。

DeepMindの中の人の[ツイート](https://twitter.com/hardmaru/status/773968758519902208)によると1秒の音声を90分かけて生成したそうですが、音楽CDの音質と同じサンプリング周波数44,100Hzで生成するとそのくらいかかります。

電話並みの8,000Hzに落としても1秒あたり数分～十数分かかります。

また実装に関しても例によって詳細は全く書かれていないため、私の実装が本当に正しいのかどうか分かりません。

## 音声データの取り扱い

一般的なwavファイル（16bit PCM）は各時刻において-32768～32767の65535通りの値を持ちます。

WaveNetは次の時刻における値をソフトマックス層から出力するのですが、65535個もユニットを作るのは無駄が多いため、[μ-lawアルゴリズム](https://ja.wikipedia.org/wiki/%CE%9C-law%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0)によって256段階に圧縮します。

その後one-hotなベクトル（255個の要素が0で1つだけ1の要素を持ったベクトル）に変換しWaveNetへの入力とします。

これは論文に書かれていないため推測ですが、私の実装では各時刻のデータを1ピクセル$\times$1ピクセルの画像にし、チャンネル数を256にしてここをone-hotベクトルとみなすことにしました。

図で表すとこんな感じです。

![input](/images/post/2016-09-17/input.png)

またサンプリング周波数は学習速度とのトレードオフになります。

実験する場合は16,000Hzか8,000Hzに落として行いました。

DeepMindが発表したような品質で学習させると数週間くらいかかると思います。

## Dilated Causal Convolution

WaveNetでは過去のある時刻までの範囲の全データ点を入力として取り、次の時刻の音を予測します。

言ってみればRNNと同じようなものです。

この「過去のデータをどの程度見るか」という範囲のことを受容野と論文では呼んでいます。

この受容野に対しWaveNetはRNNではなく畳み込みニューラルネットを使って時系列を考慮した特徴量を取り出します。

この時、受容野の広さが100ミリ秒だとすると、音声データのサンプリング周波数が44,100Hzだった場合、受容野には$44100 \times 0.1 = 4410$個ものデータ点が存在し、これら全てをWaveNetの畳み込み層は同時に見なければなりません。

論文中の図を借りて説明しますが、仮に畳み込みフィルタの幅が2だった場合、4410個のデータ点を畳み込んでいくと、畳み込み層は4409層必要になります。

![input](/images/post/2016-09-17/naive_conv.png)

（極端に言うとフィルタサイズが4410なら1層で済みます）

この問題を解決するために使うのがDilated Convolutionです。（atrous convolutionとかconvolution with holesと呼ばれたりします）

すべての層でフィルタサイズは2とします。

![input](/images/post/2016-09-17/dilated_conv.png)

dilationが1なら通常の畳込み、2なら1つ飛ばし、4なら7つ飛ばしで畳み込んでいきます。

こうすることでたった4層の畳み込み層の受容野が16になります。

$n$層なら受容野の幅は$2^n$になりますので、通常の畳込み層を重ねるよりもはるかに効率よく広い受容野を作ることができます。

（先ほどの4410の幅をたった13層でカバーできてしまいます）

Dilated Convolutionをアニメーションにすると以下のようになります。

![gif](/images/post/2016-09-17/dilated_conv.gif)

左側の何も写っていない場所はパディングがあります。

このDilated ConvolutionはChainerの次期バージョンで利用できるようになりますが、今回は自作しました。

## Residual Block

上記のDilated Convolutionは、例えば最大dilationが4だとするとdilation=1,2,4の3層の畳み込み層になりますが、これを1つの大きな畳み込み層とみなします。

つまり、あたかもフィルタサイズ16の1層の畳み込み層であるかのように考えます。

このみなし畳み込み層を何層にも積み重ねて深いネットワークを作ります。

ただ深くするだけでは学習が進まないため、[Residual Network](https://arxiv.org/abs/1512.03385)と呼ばれる構造を取り入れます。

ResNet（Residual Network）は画像認識の分野で成功しており、100層～1000層の畳込みニューラルネットの学習を可能にします。

WaveNetでは以下のように畳み込み層を積み重ねていきます。

![residual](/images/post/2016-09-17/arch.png)

入力が畳み込み層をスキップし出力と合流しているのがResNetの特徴です。

この合流に関しても、毎レイヤーごとに合流させるのか、ある程度の層を一気に飛ばして合流させるのかなどの派生も考えられますが、どの構造が一番精度が出るかは実際に自分で実験してみないと分かりません。




