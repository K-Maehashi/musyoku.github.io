---
layout: post
title: The Infinite Hidden Markov Model
category: 実装
tags:
- 自然言語処理
excerpt_separator: <!--more-->
---

## 概要

- [The Infinite Hidden Markov Model](http://mlg.eng.cam.ac.uk/zoubin/papers/ihmm.pdf)を読んだ
- C++で実装した

<!--more-->

## はじめに

Infinite Hidden Markov Model（以下IHMM）は、隠れマルコフモデル（HMM）の状態数をデータから決定することができるモデルです。

HMMにおける状態遷移確率をディリクレ過程によって生成することで無限の状態数を扱います。

それぞれの状態からの遷移確率に共通の（離散的な）基底分布を与えることで現在の状態数を共有し、さらにこの基底分布の基底分布として連続分布を与えることで新しい状態を無限に生成することができます。

## 参考文献

- [最近のベイズ理論の進展と応用 (III) ノンパラメトリックベイズ](http://chasen.org/~daiti-m/paper/ieice10npbayes.pdf)
	- IHMMによる「不思議の国のアリス」の学習例が載っています
- [続・わかりやすいパターン認識―教師なし学習入門―](https://www.amazon.co.jp/dp/427421530X)
	- ディリクレ過程や隠れマルコフモデルの分かりやすい解説が載っています

## HMM

まずマルコフモデルについて簡単に説明します。

マルコフモデルは複数の**状態**を持ち、ある状態から別の状態へ一定の確率で遷移します。

この確率を**遷移確率**と呼び、状態の遷移後、その状態に依存した一定の確率で**出力記号**を出力します。

この確率を**出力確率**と呼びます。

図で表すと以下のような動作になります。

![image](/images/post/2017-02-27/markov_model.png)

まず初期状態を表す特殊な状態から$s_{t-1}$に遷移します。

遷移したら$p(y\mid s_{t-1})$から出力記号を生成し出力します。

次に$p(s_t\mid s_{t-1})$に従って遷移先を決定します。

遷移したら$p(y\mid s_{t})$から出力記号を生成し出力します。

マルコフモデルでは状態を$s$で表し、出力記号を$y$で表すのが慣例のようです。

今回は品詞推定を扱うので、状態が品詞に対応し、出力記号が単語に対応します。

ただし、通常のテキストデータでは単語列のみ入手可能で品詞は分かりません。

このように出力系列のみ観測可能で、状態系列が分からないモデルのことを隠れマルコフモデルといいます。

## IHMM

HMMでは遷移確率$A$と出力確率$B$は以下のような行列で表します。

$$
	\begin{align}
	A = \begin{pmatrix}
			0.1 &0.7 &0.2\\
			0.2 &0.1 &0.7\\
			0.7 &0.2 &0.1\\
		\end{pmatrix}\
	B = \begin{pmatrix}
			0.9 &0.1\\
			0.6 &0.4\\
			0.1 &0.9\\
		\end{pmatrix}\
	\end{align}\
$$

ここでは状態数を3、出力記号数を2としています。

（遷移確率があらかじめ与えられているのはHMMではなくただのマルコフモデルですので、ここでは推定した遷移確率と考えてください）

この行列は行が品詞を表しており、列が遷移先の品詞と出力記号に対応しています。

![image](/images/post/2017-02-27/markov_transition.png)

分かりやすくするため時刻$t$の添字を付けてありますが、実際は時刻に関係なく同じ行列を使います。

この図の枠内の確率をよく見ると総和が全て1になっていますが、マルコフモデルではこのような多項分布を用いて遷移確率や出力確率を表します。

IHMMではこの多項分布を無限次元に拡張することで無限の状態を扱います。

この無限次元の多項分布を作るために用いられるのがディリクレ過程と呼ばれる確率過程です。

詳しくは触れませんが、ディリクレ過程は元となる分布（基底測度、基底分布）に似た分布を生成することができます。

IHMMではそれぞれの遷移確率（上の行列での各行の多項分布）や出力確率が、共通の基底分布$G_0$を持つディリクレ過程に従っていると考えます。

図で表すと以下のようになります。

![image](/images/post/2017-02-27/hmm_dirichlet.png)

この$G_0$は論文では$\rm oracle$と呼ばれています。

さらにこの$G_0$は一様分布$H$を基底分布に持つディリクレ過程に従っています。

![image](/images/post/2017-02-27/ihmm_dirichlet.png)

出力確率は省略しましたが遷移確率と同様の生成過程を持っています。

次に、品詞$s_1\ldots s_t$を観測した状態で、$s_{t+1}$の事後予測確率を論文の記号を用いて表すと以下のようになります。

$$
	\begin{align}
		p(s_{t+1} = j \mid s_t=i, \beta, \gamma) &= \frac{n_{ij}}{\sum_{j'=1}^{K}n_{ij'} + \beta}
					+\frac{\beta}{\sum_{j'=1}^{K}n_{ij'} + \beta}G_0(s_{t+1}=j \mid \gamma)\\
		G_0(s_{t+1} = j \mid \gamma) &= \frac{n_{j}^o}{\sum_{j'=1}^{K}n_{j'}^o + \gamma}
					+\frac{\gamma}{\sum_{j'=1}^{K}n_{j'}^o + \gamma}H(s_{t+1}=j)\\
		H(s_{t+1}=j) &= \frac{1}{K+1}\\
	\end{align}\
$$

$n_{ij}$は品詞$i$に続いて品詞$j$が出現した回数、$\sum_{j'=1}^{K}n_{ij'}$は品詞$i$に続いて出現した全ての品詞の回数です。

$n_{j}^o$は品詞$j$が出現した回数、$\sum_{j'=1}^{K}n_{j'}^o$は全ての品詞の出現回数の総和です。

$K$は現在の総品詞数です。

式(2)と(3)の事後予測確率は経験分布と基底分布との混合分布の形になっています。

![image](/images/post/2017-02-27/ihmm_posterior.png)

そのため$n_{ij}=0$の場合でも第二項の基底分布を用いて補完することで0ではない値を返すことができます。

さらに、今まで観測したことがない新しい品詞$j=K+1$の場合、$n_{ij}=0$かつ$n_j^o=0$ですが、式(4)の一様分布があるため値を計算することできます。

その場合は補完の補完を経由しているため非常に小さな値になりますが、$j=K+1$に限らず$j>K$について、式(2)を用いることで何らかの確率を割り当てることができます。

このようにしてIHMMは無限の状態を扱っており、基底分布との混合分布があるおかげで無限次元の遷移確率と出力確率を計算することができるようになっています。

## 二段階の生成過程について

$G_0$を経由せずいきなり$H$から$p(s_{t+1}\mid s_t)$を生成すれば良いのでは？という疑問があると思います。

式で書くと以下のようになります。

$$
	\begin{align}
		p(s_{t+1} = j \mid s_t=i, \beta, \gamma) &= \frac{n_{ij}}{\sum_{j'=1}^{K}n_{ij'} + \beta}
					+\frac{\beta}{\sum_{j'=1}^{K}n_{ij'} + \beta}H(s_{t+1}=j)\\
	\end{align}\
$$

これでも全ての状態$j$への遷移確率が計算できてしまうため問題はありませんが、不自然なモデルになってしまいます。

この式では$n_{ij}$が0だった場合常に一様分布を用いて補完することになりますが、一様分布による補完をするということは、品詞$i$の後ろには全ての品詞が平等に出現するという仮定を置くことに相当します。

しかし、品詞の出現頻度は均等ではなく、高い頻度で出現するものもあればほとんど出現しないものもあるのが一般的ですので、そういった性質を無視して一様分布で補完するのは不自然です。

そのため、この品詞の出現のしやすさを事前分布としてモデルに組み込むために$G_0$（oracle）が必要になってきます。

再掲しますが、IHMMではある品詞の出現回数を全ての品詞の出現回数の総和で割ることで、各品詞の事前分布を作っています。

$$
	\begin{align}
		G_0(s_{t+1} = j \mid \gamma) &= \frac{n_{j}^o}{\sum_{j'=1}^{K}n_{j'}^o + \gamma}
					+\frac{\gamma}{\sum_{j'=1}^{K}n_{j'}^o + \gamma}H(s_{t+1}=j)\nonumber\\
	\end{align}\
$$

また$G_0$を経由するもう一つの理由は、現在の品詞数$K$をすべての状態間で共有させるためです。

後述しますが、実際に品詞推定を行う場合は、現在存在しない新しい品詞を無限に考えるのではなく、今ある品詞$j \leq K$に加えて$j=K+1$の一つだけ考えます。

$G_0$があれば$n_j$の種類の数がそのまま$K$になります。

## 出力確率

状態$s_1 \ldots s_t$と出力$y_1\ldots y_t$が与えられた状態での$y_{t+1}$の予測確率は以下のようになります。

$$
	\begin{align}
		p(y_{t+1} = w \mid s_{t+1}=i, \beta^e, \gamma^e) &= \frac{m_{iq}}{\sum_{q'=1}^{K}m_{iq'} + \beta^e}
					+\frac{\beta^e}{\sum_{q'=1}^{K}m_{iq'} + \beta^e}G_0^e(y_{t+1}=w \mid \gamma^e)\\
		G_0(s_{t+1} = j \mid \gamma) &= \frac{n_{j}^o}{\sum_{j'=1}^{K}n_{j'}^o + \gamma}
					+\frac{\gamma^e}{\sum_{q'=1}^{K}m_{q'}^o + \gamma^e}H^e(y_{t+1}=w)\\
		H^e(y_{t+1}=w) &= \frac{1}{\mid W \mid}\\
	\end{align}\
$$

## 実験