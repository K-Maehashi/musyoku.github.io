---
layout: post
title: 言語モデルのためのディリクレ過程とピットマン・ヨー過程
category: 自然言語処理
tags:
- HPYLM
excerpt_separator: <!--more-->
---

## 概要

- 階層ピットマン・ヨー言語モデルのベースとなるディリクレ過程（Dirichlet Process）とピットマン・ヨー過程（Pitman-Yor Proccess）について
- スムージングに用いられるInterpolated Kneser-Neyと階層ピットマン・ヨー言語モデルの関連について

## 資料

- [A Bayesian Interpretation of Interpolated Kneser-Ney](http://www.stats.ox.ac.uk/~teh/research/compling/hpylm.pdf)
	- Pitman-Yor言語モデルについて
- [続・わかりやすいパターン認識 ―教師なし学習入門―](https://www.amazon.co.jp/dp/427421530X)
	- 非常にわかりやすいのでおすすめです
	- EMアルゴリズム・マルコフモデル・隠れマルコフモデル・ノンパラメトリックベイズ

## はじめに

以前に実装した[階層ピットマン・ヨー言語モデル（HPYLM）](http://musyoku.github.io/2016/07/26/A_Hierarchical_Bayesian_Language_Model_based_on_Pitman-Yor_Processes/)や[可変長n-gram言語モデル](http://musyoku.github.io/2016/07/28/Pitman-Yor%E9%81%8E%E7%A8%8B%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%8F%E5%8F%AF%E5%A4%89%E9%95%B7n-gram%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB/)のベースとなるディリクレ過程やピットマン・ヨー過程についてまとめました。

基本的には[論文](http://www.stats.ox.ac.uk/~teh/research/compling/hpylm.pdf)の第3章（Pitman-Yor Processes）を読むために必要なディリクレ過程の知識を[書籍](https://www.amazon.co.jp/dp/427421530X)をもとにまとめています。


## ディリクレ分布とディリクレ過程

ディリクレ分布やディリクレ過程の考え方を理解する上で、サイコロ生成の例えが役立ちます。

通常の確率分布（たとえば正規分布）では、そこからサンプリングすると何らかの値が得られます。

しかしディリクレ分布は**分布の分布**なので、サンプリングして得られるのは何らかの確率分布です。

サイコロで例えると、$c$次元ディリクレ分布は$c$面のサイコロそのものを生成する分布です。

生成されるサイコロの各目の出る確率は$\pi_1,\pi_2,...,\pi_c$ですが、この値は一定ではなく、サイコロを生成するたび違う確率を持ったサイコロが得られます。

ディリクレ過程はそれを更に発展させ、無限個の面を持ちうるサイコロを生成できるようになっています。

面の数は事前に決定するわけではなく、確率的に変動するという点でディリクレ分布よりも柔軟です。

ただし注意すべきなのはディリクレ過程はあくまで事前分布であるということです。

つまり、面数が分からず、各目の出る確率も分からないサイコロを持っているとして、それを降る前に、もし振ったとしたらどのような目の出方をするか？ということを示すものになっています。

## ディリクレ過程の数学

ディリクレ過程を理解するには[書籍](https://www.amazon.co.jp/dp/427421530X)を読むのが一番早いです。

ぜひ購入して読みましょう。

ここからは厳密な定義ではなく、書籍と同じ例題でディリクレ過程についてまとめます。

まず基底分布を$G_0$、集中度パラメータを$\alpha$とし、以下に示す一次元の$\boldsymbol \theta$上の確率分布（$G_0$に相当）を考えます。

![G_0](/images/post/2016-09-25/dirichlet.png)

また$G_0(\boldsymbol \theta)$から生成された$\boldsymbol \theta$が区間$A_i$に所属する確率を$G_0(A_i)$で表します。

$G_0$は確率密度関数としての条件を満たすために図の三角形の面積が$1$になるように設定されています。

（$\theta$の値域は$[0,2]$であり、$\theta = 4/3$で最大値$1$をとるようになっています）

このとき、$r$次元確率ベクトル$\boldsymbol g=(G(A_0), G(A_1), ..., G(A_r))$が

$$
	\begin{align}
		p(\boldsymbol g) &= {\rm Dir}(\alpha_1, ..., \alpha_r)\\
		\alpha_i &= \alpha G_0(A_i)\hspace{30pt}(i = 1,...,r)
	\end{align}\
$$

を満たすとき、このような確率分布$G(\boldsymbol \theta)$を生成する確率過程をディリクレ過程といい、

$$
	\begin{align}
		G(\boldsymbol \theta) \sim {\rm DP}(\alpha, G_0(\boldsymbol \theta))
	\end{align}\
$$

と記します。

$G(A_i)$は$G_0(A_i)$と同様、$G(\boldsymbol \theta)$から生成された$\boldsymbol \theta$が区間$A_i$に所属する確率を表します。

また$G(A_i)$の期待値と分散は

$$
	\begin{align}
		{\rm E}(G(A_i)) &= G_0(A_i)\\
		{\rm V}(G(A_i)) &= G_0(A_i)(1 - G_0(A_i))/(\alpha + 1)\\
	\end{align}\
$$

となります。

これはつまり、${\rm DP}(\alpha, G_0(\boldsymbol \theta))$から$G(\boldsymbol \theta)$を生成し、その$G(\boldsymbol \theta)$から生成した$\boldsymbol \theta$が区間$A_i$内にある確率は、平均して$G_0(A_i)$となります。

また集中度$\alpha$が大きくなるほど生成される$G(\boldsymbol \theta)$の分散が小さくなります。